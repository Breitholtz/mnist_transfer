{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function \n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import gc, re, copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.keras.layers import deserialize, serialize\n",
    "from tensorflow.python.keras.saving import saving_utils\n",
    "\n",
    "# Project imports \n",
    "from data import mnist_m as mnistm\n",
    "from data import mnist\n",
    "from data.label_shift import label_shift_linear, plot_labeldist, plot_splitbars\n",
    "from experiments.training import *\n",
    "from experiments.SL_bound import *\n",
    "from util.kl import *\n",
    "from util.misc import *\n",
    "from results.plotting import *\n",
    "\n",
    "# Hyper-parameters\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 10\n",
    "make_plots = False\n",
    "\n",
    "delta=0.05 ## what would this be?   \n",
    "\n",
    "alphas=[]\n",
    "length=10\n",
    "for i in range(length-1):\n",
    "    alphas.append((i+1)/length)\n",
    "\n",
    "epsilons=[0.03,0.01,0.001]\n",
    "\n",
    "sigmas=[]\n",
    "for i in range(2,9):  \n",
    "    sigmas.append([3,i])#3*10**(-i))\n",
    "    if(i==8):\n",
    "        break\n",
    "    sigmas.append([1,i])#10**(-i))\n",
    "\n",
    "img_rows, img_cols = 32, 32\n",
    "project_folder = \"/cephyr/users/frejohk/Alvis/projects/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean, variance 0.36348352 70.18035\n",
      "---------------Load MNIST----------------\n",
      "Training set (60000, 32, 32, 3) (60000, 10)\n",
      "Test set (10000, 32, 32, 3) (10000, 10)\n",
      "\n",
      "\n",
      "mean, variance 1.1809415 74.36859\n",
      "---------------Load MNIST-M----------------\n",
      "Training set (60000, 32, 32, 3) (60000, 10)\n",
      "Test set (10000, 32, 32, 3) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = mnist.load_mnist()\n",
    "x_train_m, y_train_m, x_test_m, y_test_m = mnistm.load_mnistm(y_train,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create label shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Add train and test together and shift the distributions to create source and target distributions\n",
    "### MNIST all data\n",
    "x_full=np.append(x_train,x_test, axis=0)\n",
    "y_full=np.append(y_train,y_test, axis=0)\n",
    "### MNIST-M all data\n",
    "x_full_m=np.append(x_train_m,x_test_m, axis=0)\n",
    "y_full_m=np.append(y_train_m,y_test_m, axis=0)\n",
    "#x_shift,y_shift,x_shift_target,y_shift_target =label_shift(x_train,y_train,1/2,7)\n",
    "x_shift, y_shift, x_shift_target, y_shift_target = label_shift_linear(x_full,y_full,1/12,[0,1,2,3,4,5,6,7,8,9])\n",
    "x_shift_m, y_shift_m,x_shift_target_m, y_shift_target_m = label_shift_linear(x_full_m,y_full_m,1/12,[0,1,2,3,4,5,6,7,8,9],decreasing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if make_plots:\n",
    "    plot_labeldist([0,1,2,3,4,5,6,7,8,9],y_shift_target,\"shifted, target\")\n",
    "    plot_labeldist([0,1,2,3,4,5,6,7,8,9],y_shift,\"shifted, source\")\n",
    "    plot_splitbars([0,1,2,3,4,5,6,7,8,9],y_shift,y_shift_m,\"MNIST, source\",\"MNIST-M, source\")\n",
    "    plot_splitbars([0,1,2,3,4,5,6,7,8,9],y_shift_target,y_shift_target_m,\"MNIST, target\",\"MNIST-M, target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10.986111, 5.0, 2.999428, 1.99958, 1.399789, 1.0, 0.71435696, 0.5, 0.33346358, 0.20003448], [0.09088036, 0.19987813, 0.33326975, 0.5, 0.7143216, 1.0, 1.4001397, 2.0, 3.0, 5.0025883]]\n"
     ]
    }
   ],
   "source": [
    "##### Add the label shifted datasets to each other creating the source and target domain for task 2\n",
    "\n",
    "##### calculate the label densities here\n",
    "densities=[]\n",
    "densities.append(np.sum(y_shift,axis=0))\n",
    "densities.append(np.sum(y_shift_m,axis=0))\n",
    "densities.append(np.sum(y_shift_target,axis=0))\n",
    "densities.append(np.sum(y_shift_target_m,axis=0))\n",
    "# mnist source, mnist-m source, mnist target,  mnist-m target\n",
    "#print(densities)\n",
    "TASK=2\n",
    "if TASK==1:\n",
    "    ###### label density shifted mnist\n",
    "    x_source=x_shift\n",
    "    y_source=y_shift\n",
    "    x_target=x_shift_target\n",
    "    y_target=y_shift_target\n",
    "elif TASK==2:\n",
    "    #### MIXED MNIST and MNIST-m\n",
    "    L=len(densities[0])\n",
    "    interdomain_densities = [[] for x in range(2)]\n",
    "    for i in range(L):\n",
    "        ## all densities are # in mnist over # in mnist-m\n",
    "        interdomain_densities[0].append(densities[0][i]/densities[1][i])\n",
    "        interdomain_densities[1].append(densities[2][i]/densities[3][i])\n",
    "    print(interdomain_densities)\n",
    "    x_source=np.append(x_shift,x_shift_m, axis=0)\n",
    "    y_source=np.append(y_shift,y_shift_m, axis=0)\n",
    "    x_target=np.append(x_shift_target,x_shift_target_m, axis=0)\n",
    "    y_target=np.append(y_shift_target,y_shift_target_m, axis=0)\n",
    "elif TASK==3:\n",
    "    #### MNIST -> MNIST-m\n",
    "    x_source=x_full\n",
    "    y_source=y_full\n",
    "    x_target=x_full_m\n",
    "    y_target=y_full_m\n",
    "elif TASK==4:\n",
    "    #### MNIST->USPS\n",
    "    x_source=x_full\n",
    "    y_source=y_full\n",
    "    x_target=x_usps\n",
    "    y_target=y_usps\n",
    "elif TASK==5:\n",
    "    #### MNIST -> SVHN\n",
    "    x_source=x_full\n",
    "    y_source=y_full\n",
    "    x_target=x_svhn\n",
    "    y_target=y_svhn\n",
    "elif TASK==6:\n",
    "    x_source=x_chexpert\n",
    "    y_source=y_chexpert\n",
    "    x_source=x_chest14\n",
    "    y_source=y_chest14\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack(model, training_config, weights):\n",
    "    restored_model = deserialize(model)\n",
    "    if training_config is not None:\n",
    "        restored_model.compile(\n",
    "            **saving_utils.compile_args_from_training_config(\n",
    "                training_config\n",
    "            )\n",
    "        )\n",
    "    restored_model.set_weights(weights)\n",
    "    return restored_model\n",
    "\n",
    "# Hotfix function\n",
    "def make_keras_picklable():\n",
    "    def __reduce__(self):\n",
    "        model_metadata = saving_utils.model_metadata(self)\n",
    "        training_config = model_metadata.get(\"training_config\", None)\n",
    "        model = serialize(self)\n",
    "        weights = self.get_weights()\n",
    "        return (unpack, (model, training_config, weights))\n",
    "\n",
    "    cls = Model\n",
    "    cls.__reduce__ = __reduce__\n",
    "\n",
    "def read_weights(model,w_a,x_bound,y_bound,x_target,y_target,sigma,epsilon,alpha,Binary=False,Task=TASK):\n",
    "    batch_size=128\n",
    "    batches_per_epoch=np.ceil(len(y_target)/batch_size) ## should be 547\n",
    "    epoch=1\n",
    "    \n",
    "    # Run the function to fix pickling issue\n",
    "    make_keras_picklable()\n",
    "    \n",
    "    \n",
    "    sigma=sigma[0]*10**(-1*sigma[1])    \n",
    "    \n",
    "    ### Here we do something more intelligent to not have to hardcode the epoch amounts. \n",
    "    ### we parse the filenames and sort them in numerical order and then load the weights\n",
    "    if Binary:\n",
    "        path=\"posteriors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))\n",
    "    else:\n",
    "        path=\"posteriors/\"+\"task\"+str(TASK)+\"/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))\n",
    "        #epochs_trained\n",
    "    #epochs = [] #list of \n",
    "    list1=[]\n",
    "    list2=[]\n",
    "    dirFiles = os.listdir(path) #list of directory files\n",
    "    ## remove the ckpt.index and sort so that we get the epochs that are in the directory\n",
    "    for files in dirFiles: #filter out all non checkpoints\n",
    "        if '.ckpt.index' in files:\n",
    "            name = re.sub('\\.ckpt.index$', '', files)\n",
    "            ### if it has a one it goes in one list and if it starts with a two it goes in the other\n",
    "            if (name[0]==\"1\"):\n",
    "                list1.append(name)\n",
    "            elif (name[0]==\"2\"):\n",
    "                list2.append(name)\n",
    "            #epochs.append(name)\n",
    "    #epochs.sort(key=lambda f: int(re.sub('\\D', '', f)))\n",
    "    list1.sort(key=lambda f: int(re.sub('\\D', '', f)))\n",
    "    num_batchweights=len(list1)\n",
    "    list2.sort(key=lambda f: int(re.sub('\\D', '', f)))\n",
    "    list1.extend(list2)\n",
    "    Ws=list1 ## vector of checkpoint filenames\n",
    "    \n",
    "    weight_updates=[]\n",
    "    for i in Ws:\n",
    "        #print(i)\n",
    "        if i[0]==\"1\":\n",
    "            if i[1]==\"_\":\n",
    "                weight_updates.append(int(i[2:]))\n",
    "    for i in list2:\n",
    "        weight_updates.append((int(i[2:])+1)*batches_per_epoch)\n",
    "   \n",
    "    ### load the model and the weights\n",
    "    N_checkpoints=len(Ws)\n",
    "    KLs=np.zeros(N_checkpoints)\n",
    "    errors=np.zeros(N_checkpoints)\n",
    "    targeterrors=np.zeros(N_checkpoints)\n",
    "    epochs=np.zeros(N_checkpoints)\n",
    "#     for checkpoint in Ws:\n",
    "    \n",
    "    #### here we should pass all the checkpoints to different processes and evaluate on the dataset\n",
    "    args=[]\n",
    "    #for i in range(N_checkpoints):\n",
    "    #for i in range(2):\n",
    "        #args.append(nnp.array(i,Ws[i],path,KLs, errors,targeterrors,epochs,model,w_a,copy.deepcopy(x_bound),copy.deepcopy(y_bound),copy.deepcopy(x_target),copy.deepcopy(y_target),sigma,epsilon,alpha,Binary,TASK))\n",
    "        #args.append(np.array([i,Ws[i],path,KLs, errors,targeterrors,epochs,model,w_a,x_bound,y_bound,x_target,y_target,sigma,epsilon,alpha,Binary,TASK]))\n",
    "    args.append([Ws[0],x_bound,y_bound])   \n",
    "    args.append([Ws[1],copy.deepcopy(x_bound),copy.deepcopy(y_bound)])\n",
    "    args.append([Ws[2],copy.deepcopy(x_bound),copy.deepcopy(y_bound)])   \n",
    "    args.append([Ws[3],copy.deepcopy(x_bound),copy.deepcopy(y_bound)])\n",
    "    p = Pool(processes = 3)\n",
    "    print(\"could deep copy\")\n",
    "    #for arg in args:\n",
    "    if __name__ == '__main__':\n",
    "        p.imap(dumb_func,args)\n",
    "        p.close()\n",
    "        p.join()\n",
    "\n",
    "    #print(\"we made it here!!!!\")\n",
    "    print(KLs)\n",
    "    print(errors)\n",
    "    print(targeterrors)\n",
    "    sys.exit(-1)\n",
    "    \n",
    "    return KLs,errors,targeterrors,Ws,Xvector\n",
    "\n",
    "def dumb_func(args):\n",
    "    #print(args)\n",
    "    init_tf()\n",
    "    \n",
    "    \n",
    "    print(\"!!!!\")\n",
    "    model=init_MNIST_model_binary()\n",
    "    model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.003, momentum=0.95),metrics=['accuracy'])\n",
    "    print(\"Model compiled\")\n",
    "    path=\"posteriors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))\n",
    "    model.load_weights(path+\"/\"+str(checkpoint_name)+\".ckpt\").expect_partial()\n",
    "    print(w_s=model.get_weights())\n",
    "    #model.evaluate(args[1],args[2])\n",
    "    sum=0\n",
    "    for i in range(1000000):\n",
    "        sum+=i\n",
    "    print(sum)\n",
    "    print(len(args[0]))\n",
    "    \n",
    "def init_tf():\n",
    "    ### making sure that we have the GPU to work on\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    #logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    if gpus:\n",
    "      # Restrict TensorFlow to only allocate 4GB of memory on the first GPU\n",
    "      # I do not know why I have to do this but gpu does not work otherwise.\n",
    "        try:\n",
    "            tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)])\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        except RuntimeError as e:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "            print(e)\n",
    "\n",
    "    \n",
    "## custom callback to terminate training at some specific value of a metric\n",
    "    \n",
    "class stop_callback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, monitor='accuracy', value=0.001, verbose=0):\n",
    "        super(tf.keras.callbacks.Callback, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.value = value\n",
    "        self.verbose = verbose\n",
    "    def on_epoch_end(self, epoch, logs={}): \n",
    "        if(logs.get('accuracy')> self.value): # select the accuracy\n",
    "            print(\"\\n !!! training error threshold reached, no further training !!!\")\n",
    "            self.model.stop_training = True\n",
    "            \n",
    "class fast_checkpoints(tf.keras.callbacks.Callback):\n",
    "    def __init__(self,checkpoint_path,save_freq):\n",
    "        super(tf.keras.callbacks.Callback, self).__init__()\n",
    "        self.save_freq=save_freq\n",
    "        self.filepath=checkpoint_path\n",
    "        self.verbose=1\n",
    "        self.save_best_only=False\n",
    "        self.save_weights_only=True\n",
    "    def on_train_batch_begin(self, batch, epoch, logs=None):\n",
    "         if batch%self.save_freq==0:\n",
    "\n",
    "            print(\"\\n Saved weights at the start of batch\"+str(batch)+\"\\n\")\n",
    "\n",
    "            ## Create folder\n",
    "            weight_path = self.filepath+\"/1_\"+str(batch)+\".ckpt\"\n",
    "            os.makedirs(os.path.dirname(weight_path), exist_ok=True)\n",
    "            \n",
    "            self.model.save_weights(weight_path)\n",
    "            \n",
    "def train_posterior(alpha,x_train,y_train,prior_weights=None,x_test=[],y_test=[],save=True,epsilon=0.01,Task=2,Binary=False):\n",
    "        \n",
    "        TASK=Task\n",
    "        batch_size=128\n",
    "        \n",
    "        ### x_test should be the whole of S for early stopping purposes\n",
    "    \n",
    "        checkpoint_path = \"posteriors/\"+\"task\"+str(TASK)+\"/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))\n",
    "        if Binary:\n",
    "            checkpoint_path = \"posteriors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))\n",
    "        \n",
    "        \n",
    "        # Create a callback that saves the model's weights every epoch\n",
    "        cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "            save_freq=547,   ### 547 = ceiling(70000/128) i.e training set for MNIST/MNIST-M,\n",
    "            filepath=checkpoint_path+\"/2_{epoch:0d}.ckpt\", \n",
    "            verbose=1,\n",
    "            save_best_only=False,\n",
    "            save_weights_only=True,\n",
    "                ## tune when to save as needed for plots\n",
    "        )\n",
    "        fast_cp_callback =fast_checkpoints(checkpoint_path,45)\n",
    "        stopping_callback=stop_callback(monitor='val_acc',value=1-epsilon)\n",
    "    \n",
    "        if Binary:\n",
    "            M=init_MNIST_model_binary()\n",
    "        else:\n",
    "            M=init_MNIST_model()\n",
    "\n",
    "        \n",
    "            \n",
    "        ## choose loss function, optimiser etc. and train\n",
    "        \n",
    "        M.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "               optimizer=tf.keras.optimizers.SGD(learning_rate=0.003, momentum=0.95),\n",
    "                      metrics=['accuracy'],)\n",
    "        ### load the prior weights\n",
    "        if prior_weights is not None:\n",
    "            M.set_weights(prior_weights)\n",
    "        elif(alpha==0):\n",
    "            ### save the rand. init as the prior\n",
    "            prior_path=\"priors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(100*alpha))+\"/prior.ckpt\"\n",
    "            \n",
    "            ## Create the folder\n",
    "            os.makedirs(os.path.dirname(prior_path), exist_ok=True)\n",
    "            \n",
    "            ## Save the weights\n",
    "            M.save_weights(prior_path)\n",
    "        else:\n",
    "            if Binary:\n",
    "                prior_path=\"priors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(100*alpha))+\"/prior.ckpt\"\n",
    "                M.load_weights(prior_path)#.expect_partial()\n",
    "            else:\n",
    "                prior_path=\"priors/\"+\"task\"+str(TASK)+\"/\"+str(int(100*alpha))+\"/prior.ckpt\"\n",
    "                M.load_weights(prior_path)#.expect_partial()\n",
    "        \n",
    "    \n",
    "        if save:\n",
    "            CALLBACK=[fast_cp_callback,stopping_callback]\n",
    "        else:\n",
    "            CALLBACK=[stopping_callback]\n",
    "        ### train for one epoch with more checkpoints to be able to plot more there\n",
    "        fit_info = M.fit(x_train, y_train,\n",
    "           batch_size=batch_size,\n",
    "           epochs=1, \n",
    "           callbacks=CALLBACK,\n",
    "           validation_data=(x_test, y_test),\n",
    "           verbose=1,\n",
    "                        )\n",
    "        \n",
    "        \n",
    "        if save:\n",
    "            CALLBACK=[cp_callback,stopping_callback]\n",
    "        else:\n",
    "            CALLBACK=[stopping_callback]\n",
    "            \n",
    "        fit_info = M.fit(x_train, y_train,\n",
    "           batch_size=batch_size,\n",
    "           epochs=2000, # we should have done early stopping before this completes\n",
    "           callbacks=CALLBACK,\n",
    "           validation_data=(x_test, y_test),\n",
    "           verbose=1,\n",
    "                        )\n",
    "        \n",
    "        \n",
    "         #### save the last posterior weights to disk\n",
    "        epochs_trained=len(fit_info.history['loss'])\n",
    "        if save:\n",
    "            ## Create the folder\n",
    "            os.makedirs(os.path.dirname(checkpoint_path+\"/2_\"+str(epochs_trained)), exist_ok=True)\n",
    "            \n",
    "            M.save_weights(checkpoint_path+\"/2_\"+str(epochs_trained)) ###### check if we need this; TODO!!!!!!\n",
    "            \n",
    "        #### save textfile with parameters, i.e. alpha ,epochs trained and epsilon\n",
    "        if Binary:\n",
    "            with open('posteriors/'+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+'/params.txt', 'w') as f:\n",
    "                f.write('\\n'.join([str(alpha), str(epsilon), str(epochs_trained)]))     \n",
    "            f.close()\n",
    "        else:\n",
    "            with open('posteriors/'+\"task\"+str(TASK)+\"/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+'/params.txt', 'w') as f:\n",
    "                f.write('\\n'.join([str(alpha), str(epsilon), str(epochs_trained)]))     \n",
    "            f.close()\n",
    "        W=M.get_weights()\n",
    "        return W\n",
    "    \n",
    "def train_prior(alpha,total_epochs,x_train=[],y_train=[],x_target=[],y_target=[],save=True,Task=2,Binary=False):\n",
    "    TASK=Task\n",
    "    checkpoint_path = \"priors/\"+\"task\"+str(TASK)+\"/\"+str(int(100*alpha))\n",
    "    \n",
    "    if Binary:\n",
    "        M=init_MNIST_model_binary()\n",
    "        checkpoint_path = \"priors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(100*alpha))#+\"/prior.ckpt\"\n",
    "    else:\n",
    "        M=init_MNIST_model()\n",
    "        \n",
    "    fast_cp_callback =fast_checkpoints(checkpoint_path,10)\n",
    "    if save:\n",
    "            CALLBACK=[fast_cp_callback]\n",
    "    else:\n",
    "            CALLBACK=[]\n",
    "            \n",
    "    ## choose loss function, optimiser etc. and train\n",
    "    M.compile(loss=keras.losses.categorical_crossentropy,\n",
    "               optimizer=keras.optimizers.SGD(learning_rate=0.003, momentum=0.95),\n",
    "                      metrics=['accuracy'],)\n",
    "    fit_info = M.fit(x_train, y_train,\n",
    "           batch_size=batch_size,\n",
    "           callbacks=CALLBACK,\n",
    "           epochs=total_epochs,\n",
    "           verbose=1,\n",
    "                        )\n",
    "    #### save the final prior weights to disk\n",
    "    if save:\n",
    "        os.makedirs(checkpoint_path, exist_ok=True)\n",
    "        M.save_weights(checkpoint_path+\"/prior.ckpt\")\n",
    "     \n",
    "    \n",
    "    list1=[]\n",
    "    \n",
    "    dirFiles = os.listdir(checkpoint_path) #list of directory files\n",
    "    \n",
    "    ## remove the ckpt.index and sort so that we get the epochs that are in the directory\n",
    "    for files in dirFiles: #filter out all non weights\n",
    "        if '.ckpt.index' in files:\n",
    "            name = re.sub('\\.ckpt.index$', '', files)\n",
    "            if (name[0]==\"1\"):\n",
    "                list1.append(name)\n",
    "        \n",
    "    list1.sort(key=lambda f: int(re.sub('\\D', '', f)))\n",
    "    list1.append(\"prior\")    ## add the final weights which has no number\n",
    "    \n",
    "    Ws=list1\n",
    "    weight_updates=[]\n",
    "    for i in Ws:\n",
    "        if i[0]==\"1\":\n",
    "            if i[1]==\"_\":\n",
    "                weight_updates.append(int(i[2:]))\n",
    "    weight_updates.append(int(np.ceil(len(y_train)/batch_size)))\n",
    "    \n",
    "    error=[]\n",
    "    target_error=[]\n",
    "    for checkpoint in Ws:\n",
    "        if Binary:\n",
    "            model=init_MNIST_model_binary()\n",
    "            model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                   optimizer=keras.optimizers.SGD(learning_rate=0.003, momentum=0.95),\n",
    "                      metrics=['accuracy'],)\n",
    "        else:\n",
    "            model=init_MNIST_model()\n",
    "            model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                   optimizer=keras.optimizers.SGD(learning_rate=0.003, momentum=0.95),\n",
    "                      metrics=['accuracy'],)\n",
    "            \n",
    "        model.load_weights(checkpoint_path+\"/\"+str(checkpoint)+\".ckpt\")#.expect_partial()\n",
    "        target_error.append(1-model.evaluate(x_target,y_target,verbose=0)[1])\n",
    "        error.append(1-model.evaluate(x_train,y_train,verbose=0)[1])\n",
    "    \n",
    "    if save:\n",
    "        results=pd.DataFrame({'Weightupdates': weight_updates,\n",
    "            'Trainerror': error,\n",
    "            'targeterror':target_error,\n",
    "            })\n",
    "        with open(path_to_root_file+'mnist_transfer/'+checkpoint_path+\"/results.pkl\",'wb') as f:\n",
    "            pickle.dump(results,f)\n",
    "        f.close()\n",
    "    \n",
    "    return model.get_weights()\n",
    "    \n",
    "def read_and_prepare_results(alpha,x_bound,y_bound,x_target,y_target,sigma,delta,N,epsilon,Binary=False,Task=TASK):\n",
    "    \n",
    "    sigma_tmp=sigma\n",
    "    sigma=sigma[0]*10**(-1*sigma[1])\n",
    "    \n",
    "    ## read params.txt for the desired alpha and get the parameters\n",
    "    if Binary:\n",
    "        with open('posteriors/'+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+'/params.txt', 'rb+') as f:\n",
    "            params=f.readlines()\n",
    "        f.close()\n",
    "        prior_path=\"priors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(100*alpha))+\"/prior.ckpt\"\n",
    "        result_path=\"results/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"\n",
    "    else:\n",
    "        with open('posteriors/'+\"task\"+str(TASK)+\"/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+'/params.txt', 'rb+') as f:\n",
    "            params=f.readlines()\n",
    "        f.close()\n",
    "        prior_path=\"priors/\"+\"task\"+str(TASK)+\"/\"+str(int(100*alpha))+\"/prior.ckpt\"\n",
    "        result_path=\"results/\"+\"task\"+str(TASK)+\"/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"\n",
    "        \n",
    "    epsilon=float(params[1])\n",
    "    epochs_trained=int(params[2])\n",
    "    \n",
    "    # initialise model\n",
    "    if Binary:\n",
    "        M=init_MNIST_model_binary()\n",
    "    else:\n",
    "        M=init_MNIST_model()\n",
    "    M.compile(loss=keras.losses.categorical_crossentropy,\n",
    "               optimizer=keras.optimizers.SGD(learning_rate=0.003, momentum=0.95),\n",
    "                      metrics=['accuracy'],)\n",
    "    ### load the prior weights if there are any\n",
    "    if alpha==0:\n",
    "        ### do nothing, i.e take the random init\n",
    "        w_a=M.get_weights()\n",
    "    else:\n",
    "        M.load_weights(prior_path).expect_partial()\n",
    "        w_a=M.get_weights()\n",
    "    print(Binary)\n",
    "    # read the weights and calculate what is needed for the bound\n",
    "    [KLs,errors,targeterrors,Ws,weight_updates]=read_weights(M,w_a,x_bound,y_bound,x_target,y_target,sigma_tmp,epsilon,alpha,Binary=Binary,Task=TASK)    \n",
    "    \n",
    "    #print(KLs)\n",
    "    #print(errors)\n",
    "    #print(targeterrors)\n",
    "    #print(Ws)\n",
    "   \n",
    "    bound=[]\n",
    "    ### calculate the bound\n",
    "    for i in range(len(weight_updates)):\n",
    "        bound.append(calculate_bound(KLs[i],alpha,delta,N,errors[i]))\n",
    "    \n",
    "    \n",
    "    #save the results to a pickled dataframe in results\n",
    "    results=pd.DataFrame({'Weightupdates': weight_updates,\n",
    "        'Trainerror': errors,\n",
    "        'targeterror':targeterrors,\n",
    "        'KL': KLs,\n",
    "        'Bound': bound})\n",
    "    with open(path_to_root_file+'mnist_transfer/'+result_path+str(sigma_tmp[0])+str(sigma_tmp[1])+\"_results.pkl\",'wb') as f:#int(sigma*10**8)\n",
    "        pickle.dump(results,f)\n",
    "    f.close()\n",
    "    return results\n",
    "\n",
    "def plot_result_file(epsilon,alpha,sigma,Binary=False,Task=TASK):\n",
    "    sigma_tmp=sigma\n",
    "    sigma=sigma[0]*10**(-1*sigma[1])\n",
    "    \n",
    "    if Binary:\n",
    "        result_path=\"results/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"+str(sigma_tmp[0])+str(sigma_tmp[1])\n",
    "        plt.title(\"Binary: \"+r\"$\\alpha$=\"+str(alpha)+r\" $\\epsilon$=\"+str(epsilon)+r\" $\\sigma$=\"+str(sigma))\n",
    "    else:\n",
    "        result_path=\"results/\"+\"task\"+str(TASK)+\"/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"+str(sigma_tmp[0])+str(sigma_tmp[1])\n",
    "        plt.title(r\"$\\alpha$=\"+str(alpha)+r\" $\\epsilon$=\"+str(epsilon)+r\" $\\sigma$=\"+str(sigma))\n",
    "    results=pd.read_pickle(result_path+\"_results.pkl\")\n",
    "    \n",
    "    ### do the plots\n",
    "    plt.plot(results[\"Weightupdates\"],results[\"Bound\"],'r*-')\n",
    "    plt.plot(results[\"Weightupdates\"],results[\"Trainerror\"],'m^-')\n",
    "    \n",
    "    plt.xlabel(\"Weight updates\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    \n",
    "    plt.legend([\"Bound\",\"Empirical error\"])\n",
    "    plt.show()\n",
    "\n",
    "def find_optimal_sigma(sigmas,epsilon, alpha,Binary=False,Task=TASK):\n",
    "    #### to find the optimal sigma just do a search through all the results \n",
    "    #### and save the one for each parameter which has the minimal bound\n",
    "    #### Do we do this per epoch or for some other value? The sigma which yields the lowest bound overall for some epoch?\n",
    "    optimal=[0,1]\n",
    "    # search through all epochs and pick the sigma which yields the smallest bound during the whole training process\n",
    "    for sigma in sigmas:\n",
    "        sigma_tmp=sigma\n",
    "        sigma=sigma[0]*10**(-1*sigma[1])\n",
    "        if Binary:\n",
    "            result_path=\"results/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"+str(sigma_tmp[0])+str(sigma_tmp[1])\n",
    "        else:\n",
    "            result_path=\"results/\"+\"task\"+str(TASK)+\"/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"+str(sigma_tmp[0])+str(sigma_tmp[1])\n",
    "        results=pd.read_pickle(result_path+\"_results.pkl\")\n",
    "        MIN=np.min(results[\"Bound\"])\n",
    "        if (MIN<optimal[1]):\n",
    "            optimal[1]=MIN\n",
    "            optimal[0]=sigma\n",
    "    print(\"The optimal sigma is {} with bound value {}\".format(optimal[0],optimal[1]))\n",
    "\n",
    "   \n",
    "#### find the optimal sigma for every combination of parameters\n",
    "\n",
    "#### use the optimal sigmas to calculate the bound 50 times(with different data orders and initialisation)\n",
    "#### (Note: also delta=13*delta_0) for every combination and save the mean and std for plotting into a result file\n",
    "#for i in range(50):\n",
    "    ## take in the data and split with a new seed\n",
    " #   x_bound, x_prior, y_bound , y_prior = train_test_split(x_source,y_source,test_size=alpha,random_state=(69105+i))\n",
    "#### \n",
    "def read_prior(alpha,TASK=2,Binary=True):\n",
    "    checkpoint_path = \"priors/\"+\"task\"+str(TASK)+\"/\"+str(int(100*alpha))\n",
    "    if Binary:\n",
    "        checkpoint_path = \"priors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(100*alpha))\n",
    "    result_path=path_to_root_file+'mnist_transfer/'+checkpoint_path+\"/results.pkl\"\n",
    "    results=pd.read_pickle(result_path)\n",
    "    plt.title(r\"$\\alpha$=\"+str(alpha))\n",
    "    plt.plot(results[\"Weightupdates\"],results[\"Trainerror\"],'m^-')\n",
    "    plt.plot(results[\"Weightupdates\"],results[\"targeterror\"],'k^-')\n",
    "    plt.legend([\"Training error\",\"Target error\"])\n",
    "    \n",
    "def plot_prior_and_posterior(alpha,epsilon,sigma,TASK=2,Binary=True):\n",
    "    ### load in the prior data\n",
    "    sigma_tmp=sigma\n",
    "    sigma=sigma[0]*10**(-1*sigma[1])\n",
    "    checkpoint_path = \"priors/\"+\"task\"+str(TASK)+\"/\"+str(int(100*alpha))\n",
    "    if Binary:\n",
    "        checkpoint_path = \"priors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(100*alpha))\n",
    "    result_path=path_to_root_file+'mnist_transfer/'+checkpoint_path+\"/results.pkl\"\n",
    "    results=pd.read_pickle(result_path)\n",
    "    result_path_post=\"results/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"+str(sigma_tmp[0])+str(sigma_tmp[1])\n",
    "    results2=pd.read_pickle(result_path_post+\"_results.pkl\")\n",
    "    \n",
    "    \n",
    "    ### remove/ignore the last entry of the prior data \n",
    "    ### as it should be a duplication of the first one from the posterior results\n",
    "    \n",
    "    ### training error\n",
    "    A=list(results2[\"Weightupdates\"]+list(results[\"Weightupdates\"])[-1])\n",
    "    B=list(results[\"Weightupdates\"])[:-1]\n",
    "    B.extend(A)\n",
    "    C=list(results[\"Trainerror\"])[:-1]\n",
    "    C.extend(list(results2[\"train_germain\"]))\n",
    "    plt.plot(B,C,'-m^')\n",
    "    \n",
    "    ## target error\n",
    "    D=list(results[\"targeterror\"])[:-1]\n",
    "    D.extend(list(results2[\"target_germain\"]))\n",
    "    plt.plot(B,D,'-k*')\n",
    "    \n",
    "    ### bound\n",
    "    E=results2[\"germain_bound\"]\n",
    "    plt.plot(A,E,'-D')\n",
    "    F=results2['boundpart3_germain']\n",
    "    plt.plot(A,F,'-o')\n",
    "    print(results2[\"target_germain\"])\n",
    "    print(results2[\"germain_bound\"])\n",
    "    ### lines for uninformative region and worse than random guessing; also for end of prior training\n",
    "    plt.axvline(A[0],color=\"grey\")\n",
    "    plt.axhline(y=0.5, color=\"black\", linestyle=\"--\")\n",
    "    plt.axhline(y=1, color=\"red\", linestyle=\"--\")\n",
    "    plt.legend([\"Training error\",\"Target error\",\"Bound\",\"KL-part\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha is:0.1\n",
      "Train on 70010 samples, validate on 70010 samples\n",
      "\n",
      " Saved weights at the start of batch0\n",
      "\n",
      " 4224/70010 [>.............................] - ETA: 27s - loss: 0.1632 - accuracy: 0.9370 \n",
      " Saved weights at the start of batch45\n",
      "\n",
      "10240/70010 [===>..........................] - ETA: 11s - loss: 0.1630 - accuracy: 0.9339\n",
      " Saved weights at the start of batch90\n",
      "\n",
      "16000/70010 [=====>........................] - ETA: 7s - loss: 0.1610 - accuracy: 0.9348\n",
      " Saved weights at the start of batch135\n",
      "\n",
      "21760/70010 [========>.....................] - ETA: 5s - loss: 0.1579 - accuracy: 0.9367\n",
      " Saved weights at the start of batch180\n",
      "\n",
      "27520/70010 [==========>...................] - ETA: 4s - loss: 0.1550 - accuracy: 0.9385\n",
      " Saved weights at the start of batch225\n",
      "\n",
      "33280/70010 [=============>................] - ETA: 3s - loss: 0.1529 - accuracy: 0.9395\n",
      " Saved weights at the start of batch270\n",
      "\n",
      "39040/70010 [===============>..............] - ETA: 2s - loss: 0.1512 - accuracy: 0.9403\n",
      " Saved weights at the start of batch315\n",
      "\n",
      "44800/70010 [==================>...........] - ETA: 1s - loss: 0.1483 - accuracy: 0.9419\n",
      " Saved weights at the start of batch360\n",
      "\n",
      "50560/70010 [====================>.........] - ETA: 1s - loss: 0.1464 - accuracy: 0.9425\n",
      " Saved weights at the start of batch405\n",
      "\n",
      "56320/70010 [=======================>......] - ETA: 0s - loss: 0.1437 - accuracy: 0.9436\n",
      " Saved weights at the start of batch450\n",
      "\n",
      "62080/70010 [=========================>....] - ETA: 0s - loss: 0.1424 - accuracy: 0.9444\n",
      " Saved weights at the start of batch495\n",
      "\n",
      "67840/70010 [============================>.] - ETA: 0s - loss: 0.1408 - accuracy: 0.9451\n",
      " Saved weights at the start of batch540\n",
      "\n",
      "70010/70010 [==============================] - 6s 80us/sample - loss: 0.1402 - accuracy: 0.9455 - val_loss: 0.1139 - val_accuracy: 0.9557\n",
      "Train on 70010 samples, validate on 70010 samples\n",
      "Epoch 1/2000\n",
      "  128/70010 [..............................] - ETA: 4s - loss: 0.1133 - accuracy: 0.9531\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      " 1280/70010 [..............................] - ETA: 23s - loss: 0.1416 - accuracy: 0.9453\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      " 2176/70010 [..............................] - ETA: 15s - loss: 0.1398 - accuracy: 0.9439\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      " 2560/70010 [>.............................] - ETA: 16s - loss: 0.1381 - accuracy: 0.9434\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      " 3200/70010 [>.............................] - ETA: 16s - loss: 0.1381 - accuracy: 0.9453\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      " 4352/70010 [>.............................] - ETA: 12s - loss: 0.1352 - accuracy: 0.9476\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      " 4480/70010 [>.............................] - ETA: 13s - loss: 0.1339 - accuracy: 0.9482\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      " 5760/70010 [=>............................] - ETA: 11s - loss: 0.1331 - accuracy: 0.9472\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      " 6400/70010 [=>............................] - ETA: 12s - loss: 0.1319 - accuracy: 0.9480\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      " 7552/70010 [==>...........................] - ETA: 11s - loss: 0.1304 - accuracy: 0.9493\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      " 8064/70010 [==>...........................] - ETA: 10s - loss: 0.1303 - accuracy: 0.9495\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      " 8960/70010 [==>...........................] - ETA: 10s - loss: 0.1306 - accuracy: 0.9488\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "10112/70010 [===>..........................] - ETA: 9s - loss: 0.1276 - accuracy: 0.9505 \n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "10880/70010 [===>..........................] - ETA: 9s - loss: 0.1255 - accuracy: 0.9505\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "12032/70010 [====>.........................] - ETA: 8s - loss: 0.1247 - accuracy: 0.9515\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "12160/70010 [====>.........................] - ETA: 8s - loss: 0.1241 - accuracy: 0.9518\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "13312/70010 [====>.........................] - ETA: 8s - loss: 0.1238 - accuracy: 0.9518\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "13440/70010 [====>.........................] - ETA: 8s - loss: 0.1239 - accuracy: 0.9519\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "14080/70010 [=====>........................] - ETA: 8s - loss: 0.1224 - accuracy: 0.9526\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "15104/70010 [=====>........................] - ETA: 8s - loss: 0.1223 - accuracy: 0.9525\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "15360/70010 [=====>........................] - ETA: 8s - loss: 0.1221 - accuracy: 0.9523\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "16000/70010 [=====>........................] - ETA: 8s - loss: 0.1216 - accuracy: 0.9524\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "16640/70010 [======>.......................] - ETA: 8s - loss: 0.1213 - accuracy: 0.9526\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "17280/70010 [======>.......................] - ETA: 8s - loss: 0.1205 - accuracy: 0.9534\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "17920/70010 [======>.......................] - ETA: 8s - loss: 0.1200 - accuracy: 0.9538\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "18560/70010 [======>.......................] - ETA: 8s - loss: 0.1193 - accuracy: 0.9539\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "19840/70010 [=======>......................] - ETA: 7s - loss: 0.1186 - accuracy: 0.9543\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "20480/70010 [=======>......................] - ETA: 7s - loss: 0.1183 - accuracy: 0.9545\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "21760/70010 [========>.....................] - ETA: 7s - loss: 0.1181 - accuracy: 0.9545\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "22912/70010 [========>.....................] - ETA: 6s - loss: 0.1188 - accuracy: 0.9540\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "23040/70010 [========>.....................] - ETA: 6s - loss: 0.1186 - accuracy: 0.9541\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "23680/70010 [=========>....................] - ETA: 6s - loss: 0.1187 - accuracy: 0.9540\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "24960/70010 [=========>....................] - ETA: 6s - loss: 0.1180 - accuracy: 0.9544\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "25600/70010 [=========>....................] - ETA: 6s - loss: 0.1176 - accuracy: 0.9547\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "26752/70010 [==========>...................] - ETA: 5s - loss: 0.1167 - accuracy: 0.9552\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "27520/70010 [==========>...................] - ETA: 5s - loss: 0.1160 - accuracy: 0.9555\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28800/70010 [===========>..................] - ETA: 5s - loss: 0.1166 - accuracy: 0.9551\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "30080/70010 [===========>..................] - ETA: 5s - loss: 0.1166 - accuracy: 0.9551\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "31232/70010 [============>.................] - ETA: 4s - loss: 0.1155 - accuracy: 0.9556\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "32000/70010 [============>.................] - ETA: 4s - loss: 0.1153 - accuracy: 0.9556\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "32640/70010 [============>.................] - ETA: 4s - loss: 0.1151 - accuracy: 0.9559\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "33920/70010 [=============>................] - ETA: 4s - loss: 0.1152 - accuracy: 0.9557\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "35200/70010 [==============>...............] - ETA: 4s - loss: 0.1145 - accuracy: 0.9559\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "36480/70010 [==============>...............] - ETA: 4s - loss: 0.1144 - accuracy: 0.9559\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "37760/70010 [===============>..............] - ETA: 4s - loss: 0.1143 - accuracy: 0.9560\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "38400/70010 [===============>..............] - ETA: 3s - loss: 0.1144 - accuracy: 0.9560\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "39680/70010 [================>.............] - ETA: 3s - loss: 0.1141 - accuracy: 0.9561\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "40960/70010 [================>.............] - ETA: 3s - loss: 0.1147 - accuracy: 0.9558\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "42112/70010 [=================>............] - ETA: 3s - loss: 0.1151 - accuracy: 0.9557\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "42880/70010 [=================>............] - ETA: 3s - loss: 0.1149 - accuracy: 0.9558\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "43520/70010 [=================>............] - ETA: 3s - loss: 0.1145 - accuracy: 0.9560\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "44672/70010 [==================>...........] - ETA: 3s - loss: 0.1141 - accuracy: 0.9559\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "45440/70010 [==================>...........] - ETA: 2s - loss: 0.1137 - accuracy: 0.9562\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "46592/70010 [==================>...........] - ETA: 2s - loss: 0.1134 - accuracy: 0.9563\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "47360/70010 [===================>..........] - ETA: 2s - loss: 0.1134 - accuracy: 0.9563\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "48000/70010 [===================>..........] - ETA: 2s - loss: 0.1132 - accuracy: 0.9563\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "49152/70010 [====================>.........] - ETA: 2s - loss: 0.1128 - accuracy: 0.9564\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "49920/70010 [====================>.........] - ETA: 2s - loss: 0.1127 - accuracy: 0.9564\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "50560/70010 [====================>.........] - ETA: 2s - loss: 0.1122 - accuracy: 0.9566\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "51200/70010 [====================>.........] - ETA: 2s - loss: 0.1121 - accuracy: 0.9567\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "52480/70010 [=====================>........] - ETA: 2s - loss: 0.1118 - accuracy: 0.9567\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "53120/70010 [=====================>........] - ETA: 2s - loss: 0.1118 - accuracy: 0.9568\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "53760/70010 [======================>.......] - ETA: 2s - loss: 0.1120 - accuracy: 0.9567\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "54400/70010 [======================>.......] - ETA: 2s - loss: 0.1117 - accuracy: 0.9570\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "55680/70010 [======================>.......] - ETA: 1s - loss: 0.1116 - accuracy: 0.9570\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "56320/70010 [=======================>......] - ETA: 1s - loss: 0.1114 - accuracy: 0.9570\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "56960/70010 [=======================>......] - ETA: 1s - loss: 0.1111 - accuracy: 0.9572\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "57600/70010 [=======================>......] - ETA: 1s - loss: 0.1114 - accuracy: 0.9572\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "58880/70010 [========================>.....] - ETA: 1s - loss: 0.1114 - accuracy: 0.9572\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "60160/70010 [========================>.....] - ETA: 1s - loss: 0.1108 - accuracy: 0.9573\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "60800/70010 [=========================>....] - ETA: 1s - loss: 0.1106 - accuracy: 0.9574\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "62080/70010 [=========================>....] - ETA: 1s - loss: 0.1107 - accuracy: 0.9572\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "62720/70010 [=========================>....] - ETA: 0s - loss: 0.1107 - accuracy: 0.9572\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "63360/70010 [==========================>...] - ETA: 0s - loss: 0.1106 - accuracy: 0.9573\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "64000/70010 [==========================>...] - ETA: 0s - loss: 0.1104 - accuracy: 0.9574\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "64640/70010 [==========================>...] - ETA: 0s - loss: 0.1105 - accuracy: 0.9573\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "65280/70010 [==========================>...] - ETA: 0s - loss: 0.1105 - accuracy: 0.9572\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "66560/70010 [===========================>..] - ETA: 0s - loss: 0.1099 - accuracy: 0.9574\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "67840/70010 [============================>.] - ETA: 0s - loss: 0.1092 - accuracy: 0.9578\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "68480/70010 [============================>.] - ETA: 0s - loss: 0.1089 - accuracy: 0.9578\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69120/70010 [============================>.] - ETA: 0s - loss: 0.1089 - accuracy: 0.9579\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "70010/70010 [==============================] - 11s 160us/sample - loss: 0.1087 - accuracy: 0.9580 - val_loss: 0.0903 - val_accuracy: 0.9657\n",
      "Epoch 2/2000\n",
      "  128/70010 [..............................] - ETA: 2s - loss: 0.1065 - accuracy: 0.9531\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "  384/70010 [..............................] - ETA: 12s - loss: 0.1223 - accuracy: 0.9505\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      " 1536/70010 [..............................] - ETA: 5s - loss: 0.0851 - accuracy: 0.9668 \n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      " 1664/70010 [..............................] - ETA: 10s - loss: 0.0829 - accuracy: 0.9675\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      " 2304/70010 [..............................] - ETA: 13s - loss: 0.0825 - accuracy: 0.9688\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      " 2944/70010 [>.............................] - ETA: 12s - loss: 0.0834 - accuracy: 0.9684\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      " 3584/70010 [>.............................] - ETA: 11s - loss: 0.0878 - accuracy: 0.9665\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      " 4224/70010 [>.............................] - ETA: 13s - loss: 0.0949 - accuracy: 0.9650\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      " 5504/70010 [=>............................] - ETA: 14s - loss: 0.0925 - accuracy: 0.9651\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      " 6144/70010 [=>............................] - ETA: 13s - loss: 0.0911 - accuracy: 0.9666\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      " 6784/70010 [=>............................] - ETA: 14s - loss: 0.0914 - accuracy: 0.9667\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      " 7424/70010 [==>...........................] - ETA: 15s - loss: 0.0904 - accuracy: 0.9669\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      " 8064/70010 [==>...........................] - ETA: 14s - loss: 0.0909 - accuracy: 0.9665\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      " 8704/70010 [==>...........................] - ETA: 13s - loss: 0.0925 - accuracy: 0.9662\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      " 9344/70010 [===>..........................] - ETA: 13s - loss: 0.0925 - accuracy: 0.9660\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      " 9984/70010 [===>..........................] - ETA: 13s - loss: 0.0932 - accuracy: 0.9652\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "10624/70010 [===>..........................] - ETA: 13s - loss: 0.0931 - accuracy: 0.9653\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "11264/70010 [===>..........................] - ETA: 13s - loss: 0.0922 - accuracy: 0.9658\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "11904/70010 [====>.........................] - ETA: 13s - loss: 0.0929 - accuracy: 0.9652\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "13056/70010 [====>.........................] - ETA: 12s - loss: 0.0917 - accuracy: 0.9654\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "13824/70010 [====>.........................] - ETA: 12s - loss: 0.0904 - accuracy: 0.9656\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "14464/70010 [=====>........................] - ETA: 12s - loss: 0.0907 - accuracy: 0.9657\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "15104/70010 [=====>........................] - ETA: 12s - loss: 0.0915 - accuracy: 0.9653\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "15744/70010 [=====>........................] - ETA: 11s - loss: 0.0932 - accuracy: 0.9648\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "16384/70010 [======>.......................] - ETA: 12s - loss: 0.0931 - accuracy: 0.9650\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "17024/70010 [======>.......................] - ETA: 11s - loss: 0.0927 - accuracy: 0.9653\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "17664/70010 [======>.......................] - ETA: 12s - loss: 0.0921 - accuracy: 0.9653\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "18304/70010 [======>.......................] - ETA: 11s - loss: 0.0920 - accuracy: 0.9654\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "18944/70010 [=======>......................] - ETA: 12s - loss: 0.0919 - accuracy: 0.9654\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "20096/70010 [=======>......................] - ETA: 11s - loss: 0.0910 - accuracy: 0.9657\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "20224/70010 [=======>......................] - ETA: 11s - loss: 0.0907 - accuracy: 0.9658\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "21504/70010 [========>.....................] - ETA: 10s - loss: 0.0902 - accuracy: 0.9657\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "22144/70010 [========>.....................] - ETA: 10s - loss: 0.0897 - accuracy: 0.9660\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "22784/70010 [========>.....................] - ETA: 10s - loss: 0.0898 - accuracy: 0.9659\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "23424/70010 [=========>....................] - ETA: 10s - loss: 0.0898 - accuracy: 0.9660\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "24064/70010 [=========>....................] - ETA: 10s - loss: 0.0890 - accuracy: 0.9665\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "25216/70010 [=========>....................] - ETA: 9s - loss: 0.0892 - accuracy: 0.9665 \n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "25344/70010 [=========>....................] - ETA: 9s - loss: 0.0890 - accuracy: 0.9665\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "25984/70010 [==========>...................] - ETA: 9s - loss: 0.0888 - accuracy: 0.9666\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "26624/70010 [==========>...................] - ETA: 9s - loss: 0.0899 - accuracy: 0.9660\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "27776/70010 [==========>...................] - ETA: 8s - loss: 0.0888 - accuracy: 0.9665\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "27904/70010 [==========>...................] - ETA: 8s - loss: 0.0887 - accuracy: 0.9665\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "28544/70010 [===========>..................] - ETA: 8s - loss: 0.0881 - accuracy: 0.9666\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "29184/70010 [===========>..................] - ETA: 8s - loss: 0.0883 - accuracy: 0.9666\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "29824/70010 [===========>..................] - ETA: 8s - loss: 0.0888 - accuracy: 0.9663\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "30464/70010 [============>.................] - ETA: 8s - loss: 0.0887 - accuracy: 0.9664\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "31104/70010 [============>.................] - ETA: 8s - loss: 0.0885 - accuracy: 0.9664\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "31744/70010 [============>.................] - ETA: 8s - loss: 0.0882 - accuracy: 0.9666\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "32384/70010 [============>.................] - ETA: 8s - loss: 0.0877 - accuracy: 0.9667\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33536/70010 [=============>................] - ETA: 8s - loss: 0.0877 - accuracy: 0.9668\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "33920/70010 [=============>................] - ETA: 8s - loss: 0.0880 - accuracy: 0.9667\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "34304/70010 [=============>................] - ETA: 7s - loss: 0.0880 - accuracy: 0.9666\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "34944/70010 [=============>................] - ETA: 8s - loss: 0.0883 - accuracy: 0.9665\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "35584/70010 [==============>...............] - ETA: 7s - loss: 0.0883 - accuracy: 0.9665\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "36224/70010 [==============>...............] - ETA: 7s - loss: 0.0885 - accuracy: 0.9664\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "36864/70010 [==============>...............] - ETA: 7s - loss: 0.0884 - accuracy: 0.9665\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "38016/70010 [===============>..............] - ETA: 7s - loss: 0.0884 - accuracy: 0.9664\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "38144/70010 [===============>..............] - ETA: 7s - loss: 0.0885 - accuracy: 0.9664\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "38784/70010 [===============>..............] - ETA: 6s - loss: 0.0885 - accuracy: 0.9663\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "39424/70010 [===============>..............] - ETA: 6s - loss: 0.0884 - accuracy: 0.9663\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "40064/70010 [================>.............] - ETA: 6s - loss: 0.0881 - accuracy: 0.9664\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "41088/70010 [================>.............] - ETA: 6s - loss: 0.0884 - accuracy: 0.9662\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "41344/70010 [================>.............] - ETA: 6s - loss: 0.0884 - accuracy: 0.9662\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "41984/70010 [================>.............] - ETA: 6s - loss: 0.0881 - accuracy: 0.9664\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "42880/70010 [=================>............] - ETA: 5s - loss: 0.0878 - accuracy: 0.9665\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "43264/70010 [=================>............] - ETA: 5s - loss: 0.0875 - accuracy: 0.9667\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "44416/70010 [==================>...........] - ETA: 5s - loss: 0.0872 - accuracy: 0.9669\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "44544/70010 [==================>...........] - ETA: 5s - loss: 0.0870 - accuracy: 0.9669\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "45824/70010 [==================>...........] - ETA: 5s - loss: 0.0868 - accuracy: 0.9670\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "46464/70010 [==================>...........] - ETA: 5s - loss: 0.0867 - accuracy: 0.9670\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "47104/70010 [===================>..........] - ETA: 4s - loss: 0.0868 - accuracy: 0.9670\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "48256/70010 [===================>..........] - ETA: 4s - loss: 0.0871 - accuracy: 0.9668\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "48384/70010 [===================>..........] - ETA: 4s - loss: 0.0871 - accuracy: 0.9668\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "49536/70010 [====================>.........] - ETA: 4s - loss: 0.0874 - accuracy: 0.9666\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "50304/70010 [====================>.........] - ETA: 4s - loss: 0.0873 - accuracy: 0.9666\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "50944/70010 [====================>.........] - ETA: 4s - loss: 0.0871 - accuracy: 0.9667\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "51584/70010 [=====================>........] - ETA: 3s - loss: 0.0874 - accuracy: 0.9665\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "52224/70010 [=====================>........] - ETA: 3s - loss: 0.0874 - accuracy: 0.9666\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "53504/70010 [=====================>........] - ETA: 3s - loss: 0.0871 - accuracy: 0.9666\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "54144/70010 [======================>.......] - ETA: 3s - loss: 0.0870 - accuracy: 0.9666\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "54912/70010 [======================>.......] - ETA: 3s - loss: 0.0869 - accuracy: 0.9667\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "55424/70010 [======================>.......] - ETA: 3s - loss: 0.0869 - accuracy: 0.9666\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "56064/70010 [=======================>......] - ETA: 2s - loss: 0.0870 - accuracy: 0.9666\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "56704/70010 [=======================>......] - ETA: 2s - loss: 0.0870 - accuracy: 0.9666\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "57344/70010 [=======================>......] - ETA: 2s - loss: 0.0870 - accuracy: 0.9665\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "57984/70010 [=======================>......] - ETA: 2s - loss: 0.0870 - accuracy: 0.9666\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "58624/70010 [========================>.....] - ETA: 2s - loss: 0.0871 - accuracy: 0.9664\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "59264/70010 [========================>.....] - ETA: 2s - loss: 0.0871 - accuracy: 0.9664\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "60416/70010 [========================>.....] - ETA: 1s - loss: 0.0870 - accuracy: 0.9665\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "60544/70010 [========================>.....] - ETA: 1s - loss: 0.0870 - accuracy: 0.9665\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "61184/70010 [=========================>....] - ETA: 1s - loss: 0.0869 - accuracy: 0.9666\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "62336/70010 [=========================>....] - ETA: 1s - loss: 0.0868 - accuracy: 0.9667\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "62464/70010 [=========================>....] - ETA: 1s - loss: 0.0868 - accuracy: 0.9667\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "63232/70010 [==========================>...] - ETA: 1s - loss: 0.0865 - accuracy: 0.9668\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "63744/70010 [==========================>...] - ETA: 1s - loss: 0.0865 - accuracy: 0.9667\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "64384/70010 [==========================>...] - ETA: 1s - loss: 0.0863 - accuracy: 0.9667\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "65024/70010 [==========================>...] - ETA: 1s - loss: 0.0865 - accuracy: 0.9667\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "65664/70010 [===========================>..] - ETA: 0s - loss: 0.0863 - accuracy: 0.9668\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "66304/70010 [===========================>..] - ETA: 0s - loss: 0.0865 - accuracy: 0.9667\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "66944/70010 [===========================>..] - ETA: 0s - loss: 0.0866 - accuracy: 0.9667\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67584/70010 [===========================>..] - ETA: 0s - loss: 0.0867 - accuracy: 0.9666\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "68864/70010 [============================>.] - ETA: 0s - loss: 0.0866 - accuracy: 0.9667\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "70010/70010 [==============================] - 16s 228us/sample - loss: 0.0868 - accuracy: 0.9667 - val_loss: 0.0780 - val_accuracy: 0.9718\n",
      "Epoch 3/2000\n",
      "\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      "  128/70010 [..............................] - ETA: 12s - loss: 0.0717 - accuracy: 0.9844\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      "  768/70010 [..............................] - ETA: 26s - loss: 0.0839 - accuracy: 0.9727\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      " 1920/70010 [..............................] - ETA: 12s - loss: 0.0773 - accuracy: 0.9729\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      "\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      " 2688/70010 [>.............................] - ETA: 10s - loss: 0.0786 - accuracy: 0.9717\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      " 3328/70010 [>.............................] - ETA: 11s - loss: 0.0775 - accuracy: 0.9709\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      " 3968/70010 [>.............................] - ETA: 10s - loss: 0.0742 - accuracy: 0.9733\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      " 4608/70010 [>.............................] - ETA: 10s - loss: 0.0774 - accuracy: 0.9720\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      " 5248/70010 [=>............................] - ETA: 10s - loss: 0.0745 - accuracy: 0.9726\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      " 5888/70010 [=>............................] - ETA: 10s - loss: 0.0731 - accuracy: 0.9730\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      " 6528/70010 [=>............................] - ETA: 11s - loss: 0.0730 - accuracy: 0.9733\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      " 7168/70010 [==>...........................] - ETA: 10s - loss: 0.0756 - accuracy: 0.9718\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      " 7808/70010 [==>...........................] - ETA: 10s - loss: 0.0748 - accuracy: 0.9730\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      " 8448/70010 [==>...........................] - ETA: 10s - loss: 0.0768 - accuracy: 0.9722\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      " 9216/70010 [==>...........................] - ETA: 9s - loss: 0.0768 - accuracy: 0.9727 \n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      " 9728/70010 [===>..........................] - ETA: 9s - loss: 0.0776 - accuracy: 0.9725\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      "10368/70010 [===>..........................] - ETA: 9s - loss: 0.0788 - accuracy: 0.9719\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      "11008/70010 [===>..........................] - ETA: 10s - loss: 0.0791 - accuracy: 0.9719\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      "\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      "12288/70010 [====>.........................] - ETA: 9s - loss: 0.0799 - accuracy: 0.9708 \n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      "12928/70010 [====>.........................] - ETA: 9s - loss: 0.0792 - accuracy: 0.9709\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      "13568/70010 [====>.........................] - ETA: 9s - loss: 0.0789 - accuracy: 0.9710\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      "14208/70010 [=====>........................] - ETA: 9s - loss: 0.0789 - accuracy: 0.9709\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      "14848/70010 [=====>........................] - ETA: 9s - loss: 0.0790 - accuracy: 0.9706\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      "15488/70010 [=====>........................] - ETA: 9s - loss: 0.0788 - accuracy: 0.9708\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      "16128/70010 [=====>........................] - ETA: 9s - loss: 0.0787 - accuracy: 0.9705\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      "16768/70010 [======>.......................] - ETA: 10s - loss: 0.0779 - accuracy: 0.9705"
     ]
    }
   ],
   "source": [
    "alphas=[0.1]\n",
    "y_source_bin=np.array(make_mnist_binary(y_source))\n",
    "y_target_bin=np.array(make_mnist_binary(y_target))\n",
    "for alpha in alphas:\n",
    "    print(\"Alpha is:\"+str(alpha))\n",
    "    x_bound, x_prior, y_bound , y_prior = train_test_split(x_source,y_source_bin,test_size=alpha,random_state=69105)\n",
    "    #w_a=train_prior(alpha,1,x_source,y_source_bin,x_target=x_target,y_target=y_target_bin,save=True,Task=2,Binary=True)\n",
    "    \n",
    "    for epsilon in epsilons:\n",
    "        w_s=train_posterior(alpha,x_source,y_source_bin,None,x_test=x_source,y_test=y_source_bin,epsilon=epsilon,Task=TASK,Binary=True)\n",
    "        #for sigma in sigmas:\n",
    "            #res=read_and_prepare_results(alpha,x_source,y_source_bin,x_target,y_target_bin,sigma,delta,len(x_source),epsilon,Binary=True)#x_bound,y_bound,x_target,y_target_bin,sigma,delta,len(x_bound),epsilon,Binary=True)\n",
    "            #plot_result_file(epsilon,alpha,sigma,TASK,Binary=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
