{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function \n",
    "\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import gc, re, copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.keras.layers import deserialize, serialize\n",
    "from tensorflow.python.keras.saving import saving_utils\n",
    "\n",
    "# Project imports \n",
    "from data import mnist,mnist_m\n",
    "from data.tasks import *\n",
    "from data.label_shift import label_shift_linear, plot_labeldist, plot_splitbars\n",
    "from experiments.training import *\n",
    "from experiments.SL_bound import *\n",
    "from util.kl import *\n",
    "from util.misc import *\n",
    "from results.plotting import *\n",
    "\n",
    "# Hyper-parameters\n",
    "#batch_size = 128\n",
    "num_classes = 10\n",
    "make_plots = False\n",
    "\n",
    "delta=0.05 ## what would this be?   \n",
    "\n",
    "alphas=[0]\n",
    "#length=10\n",
    "#for i in range(length-1):\n",
    "#    alphas.append((i+1)/length)\n",
    "\n",
    "\n",
    "\n",
    "sigmas=[]\n",
    "for i in range(2,9):  \n",
    "    sigmas.append([3,i])\n",
    "    if(i==8):\n",
    "        break\n",
    "    sigmas.append([1,i])\n",
    "\n",
    "img_rows, img_cols = 32, 32\n",
    "project_folder = \"/cephyr/users/adambre/Alvis/\"\n",
    "\n",
    "\n",
    "### TODO\n",
    "\n",
    "# # check output_dir, create it if not exists\n",
    "#     if not os.path.isdir(output_dir):\n",
    "#         os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/adam/Code/Datasets/chexpert/CheXpert-v1.0-small/train/patient00001/study1/view1_frontal.jpg', '/home/adam/Code/Datasets/chexpert/CheXpert-v1.0-small/train/patient00002/study2/view1_frontal.jpg', '/home/adam/Code/Datasets/chexpert/CheXpert-v1.0-small/train/patient00002/study1/view1_frontal.jpg', '/home/adam/Code/Datasets/chexpert/CheXpert-v1.0-small/train/patient00002/study1/view2_lateral.jpg', '/home/adam/Code/Datasets/chexpert/CheXpert-v1.0-small/train/patient00003/study1/view1_frontal.jpg', '/home/adam/Code/Datasets/chexpert/CheXpert-v1.0-small/train/patient00004/study1/view1_frontal.jpg', '/home/adam/Code/Datasets/chexpert/CheXpert-v1.0-small/train/patient00004/study1/view2_lateral.jpg', '/home/adam/Code/Datasets/chexpert/CheXpert-v1.0-small/train/patient00005/study1/view1_frontal.jpg', '/home/adam/Code/Datasets/chexpert/CheXpert-v1.0-small/train/patient00005/study1/view2_lateral.jpg', '/home/adam/Code/Datasets/chexpert/CheXpert-v1.0-small/train/patient00005/study2/view1_frontal.jpg']\n",
      "[[1, 0, 0, 0, 0, 0], [0, 1, 1, 1, 1, 1], [0, 0, 0, 1, 0, 0], [0, 0, 0, 1, 0, 0], [0, 0, 1, 0, 0, 0], [1, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]]\n",
      "['/home/adam/Code/Datasets/chexpert/CheXpert-v1.0-small/valid/patient64541/study1/view1_frontal.jpg', '/home/adam/Code/Datasets/chexpert/CheXpert-v1.0-small/valid/patient64542/study1/view1_frontal.jpg', '/home/adam/Code/Datasets/chexpert/CheXpert-v1.0-small/valid/patient64542/study1/view2_lateral.jpg', '/home/adam/Code/Datasets/chexpert/CheXpert-v1.0-small/valid/patient64543/study1/view1_frontal.jpg', '/home/adam/Code/Datasets/chexpert/CheXpert-v1.0-small/valid/patient64544/study1/view1_frontal.jpg', '/home/adam/Code/Datasets/chexpert/CheXpert-v1.0-small/valid/patient64545/study1/view1_frontal.jpg', '/home/adam/Code/Datasets/chexpert/CheXpert-v1.0-small/valid/patient64546/study1/view1_frontal.jpg', '/home/adam/Code/Datasets/chexpert/CheXpert-v1.0-small/valid/patient64547/study1/view1_frontal.jpg', '/home/adam/Code/Datasets/chexpert/CheXpert-v1.0-small/valid/patient64547/study1/view2_frontal.jpg', '/home/adam/Code/Datasets/chexpert/CheXpert-v1.0-small/valid/patient64547/study1/view3_lateral.jpg']\n",
      "[[0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0], [1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 1], [0, 1, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]]\n",
      "['/home/adam/Code/Datasets/chexpert/CheXpert-v1.0-small/train/patient00001/study1/view1_frontal.jpg', '/home/adam/Code/Datasets/chexpert/CheXpert-v1.0-small/train/patient00002/study2/view1_frontal.jpg', '/home/adam/Code/Datasets/chexpert/CheXpert-v1.0-small/train/patient00002/study1/view1_frontal.jpg', '/home/adam/Code/Datasets/chexpert/CheXpert-v1.0-small/train/patient00002/study1/view2_lateral.jpg', '/home/adam/Code/Datasets/chexpert/CheXpert-v1.0-small/train/patient00003/study1/view1_frontal.jpg', '/home/adam/Code/Datasets/chexpert/CheXpert-v1.0-small/train/patient00004/study1/view1_frontal.jpg', '/home/adam/Code/Datasets/chexpert/CheXpert-v1.0-small/train/patient00004/study1/view2_lateral.jpg', '/home/adam/Code/Datasets/chexpert/CheXpert-v1.0-small/train/patient00005/study1/view1_frontal.jpg', '/home/adam/Code/Datasets/chexpert/CheXpert-v1.0-small/train/patient00005/study1/view2_lateral.jpg', '/home/adam/Code/Datasets/chexpert/CheXpert-v1.0-small/train/patient00005/study2/view1_frontal.jpg']\n",
      "[[1, 0, 0, 0, 0, 0], [0, 1, 1, 1, 1, 1], [0, 0, 0, 1, 0, 0], [0, 0, 0, 1, 0, 0], [0, 0, 1, 0, 0, 0], [1, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/adam/Code/Datasets/chexpert/resized32chex//home/adam/Code/Datasets/chexpert/CheXpert-v1.0-small/train/patient00001/study1/view1_frontal.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-e1d2488201c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_xray14_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m \u001b[0mload_resize_and_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchexpert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m  \u001b[0;31m#####load chestXray14 from files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;31m# x_data=np.load(data_path+\"X_sample.npy\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-e1d2488201c2>\u001b[0m in \u001b[0;36mload_resize_and_save\u001b[0;34m(chexpert, img_size)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mimg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_read_and_resize_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/keras/preprocessing/image.py\u001b[0m in \u001b[0;36msave_img\u001b[0;34m(path, x, data_format, file_format, scale, **kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdata_format\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0mdata_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_data_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m   image.save_img(path,\n\u001b[0m\u001b[1;32m    259\u001b[0m                  \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m                  \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras_preprocessing/image/utils.py\u001b[0m in \u001b[0;36msave_img\u001b[0;34m(path, x, data_format, file_format, scale, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m                       'RGBA images, converting to RGB.')\n\u001b[1;32m     75\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.9/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2230\u001b[0m                 \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r+b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2231\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2232\u001b[0;31m                 \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w+b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2234\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/adam/Code/Datasets/chexpert/resized32chex//home/adam/Code/Datasets/chexpert/CheXpert-v1.0-small/train/patient00001/study1/view1_frontal.jpg'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow_datasets as tfds\n",
    "import collections\n",
    "import csv\n",
    "\n",
    "\n",
    "#data_path=\"/home/adam/Code/Datasets/chexpert/1.0.0/\"\n",
    "#data_path2=\"/home/adam/Code/Datasets/chexpert/CheXpert-v1.0-small\"\n",
    "def tf_read_and_resize_image(filename,img_size):\n",
    "    ### takes file path and image size and resizes the image\n",
    "    image_string = tf.io.read_file(filename)\n",
    "    image = tf.image.decode_jpeg(image_string, channels=3)\n",
    "    image=tf.cast(image, tf.float32)\n",
    "    image = tf.image.resize(image, (img_size, img_size))#, method=tf.image.ResizeMethod.BILINEAR)\n",
    "    return image\n",
    "\n",
    "def load_from_csv(imgs_path,csv_path):\n",
    "    _LABELS = collections.OrderedDict({\n",
    "            \"-1.0\": \"uncertain\",\n",
    "            \"1.0\": \"positive\",\n",
    "            \"0.0\": \"negative\",\n",
    "            \"\": \"unmentioned\",\n",
    "        })\n",
    "    labeldict={\"positive\":1,\"negative\":0, \"unmentioned\":0,\"uncertain\":1} ### sets all uncertain labels to 1\n",
    "    overlapping_labels=[0,2,5,6,8,10] ## according to pham et al. NF,CM,ED,CD,AC,PE\n",
    "    ##### loads chexpert filenames and labels from files\n",
    "    label_arr=[]\n",
    "    arr=[]\n",
    "    with tf.io.gfile.GFile(csv_path) as csv_f:\n",
    "        reader = csv.DictReader(csv_f)\n",
    "        # Get keys for each label from csv\n",
    "        label_keys = reader.fieldnames[5:]\n",
    "\n",
    "        for row in reader:\n",
    "            # Get image based on indicated path in csv\n",
    "            name = row[\"Path\"]\n",
    "            labels = [_LABELS[row[key]] for key in label_keys]\n",
    "            labels_overlap=[labeldict[labels[i]] for i in overlapping_labels]\n",
    "\n",
    "            ## save the image_name and the label array\n",
    "            label_arr.append(labels_overlap)\n",
    "            arr.append(os.path.join(imgs_path, name))\n",
    "        return arr,label_arr\n",
    "    \n",
    "def make_xray14_labels():\n",
    "    ##### load the labels and convert the labels to binary vectors which maps the occurrence of a label to a 1\n",
    "    data_path=\"/home/adam/Code/Datasets/chestXray14/\"\n",
    "    labeldict={\"No Finding\":0,\"Cardiomegaly\":1,\"Edema\":2,\"Consolidation\":3,\"Atelectasis\":4,\"Effusion\":5}\n",
    "    data = pd.read_csv(data_path+\"Data_Entry_2017_v2020.csv\")\n",
    "    sample = os.listdir(data_path+\"resized32/\")\n",
    "\n",
    "    sample = pd.DataFrame({'Image Index': sample})\n",
    "\n",
    "    sample = pd.merge(sample, data, how='left', on='Image Index')\n",
    "\n",
    "    sample.columns = ['Image_Index', 'Finding_Labels', 'Follow_Up_#', 'Patient_ID',\n",
    "                      'Patient_Age', 'Patient_Gender', 'View_Position',\n",
    "                      'Original_Image_Width', 'Original_Image_Height',\n",
    "                      'Original_Image_Pixel_Spacing_X',\n",
    "                      'Original_Image_Pixel_Spacing_Y']#, 'Unnamed']\n",
    "    def make_one_hot(label_string):\n",
    "        result=np.zeros(6)\n",
    "        labels=label_string.split('|')\n",
    "        for l in labels:\n",
    "            if l not in [\"No Finding\",\"Cardiomegaly\",\"Edema\",\"Consolidation\",\"Atelectasis\",\"Effusion\"]:\n",
    "                pass\n",
    "            else:\n",
    "                result[labeldict[l]]=1\n",
    "        return result.astype(int)\n",
    "\n",
    "    sample['Finding_Labels'] = sample['Finding_Labels'].apply(lambda x: make_one_hot(x))\n",
    "    print(sample['Finding_Labels'].shape)\n",
    "    y=sample['Finding_Labels']\n",
    "    return y\n",
    "\n",
    "def load_resize_and_save(chexpert=False,img_size=32):\n",
    "##### loads the xray datasets from the raw images and then resizes to desired size and saves them in a new directory\n",
    "    if chexpert:\n",
    "        data_path=\"/home/adam/Code/Datasets/chexpert/\"\n",
    "        _DATA_DIR = \"/home/adam/Code/Datasets/chexpert/CheXpert-v1.0-small\" ### where is the source data?\n",
    "        _TRAIN_DIR = os.path.join(_DATA_DIR, \"train\")\n",
    "        _VALIDATION_DIR = os.path.join(_DATA_DIR, \"valid\")\n",
    "        _TRAIN_LABELS_FNAME = os.path.join(_DATA_DIR, \"train.csv\")\n",
    "        _VALIDATION_LABELS_FNAME = os.path.join(_DATA_DIR, \"valid.csv\")\n",
    "        \n",
    "        filenames,labels=load_from_csv(data_path,_TRAIN_LABELS_FNAME)\n",
    "        \n",
    "        filenames_2,labels_2=load_from_csv(data_path,_VALIDATION_LABELS_FNAME)\n",
    "\n",
    "        filenames.extend(filenames_2)\n",
    "        labels.extend(labels_2)\n",
    "     \n",
    "        new_path=data_path+\"resized\"+str(img_size)+\"chex\"\n",
    "        if not os.path.exists(new_path):\n",
    "            os.makedirs(new_path)\n",
    "        ## load in, resize and save image in array file\n",
    "        for file in filenames:\n",
    "            img=np.array(tf_read_and_resize_image(file,img_size))\n",
    "            tf.keras.preprocessing.image.save_img(new_path,img)\n",
    "            sys.exit(-1)\n",
    "            \n",
    "        #np.save(\"/home/adam/Code/Datasets/chexpert/chexpert128.npy\",np.array([np.array(tf_read_and_resize_image(file)) for file in filenames]))\n",
    "        #np.save(\"/home/adam/Code/Datasets/chexpert/labels.npy\",np.array(labels)) # save labels to separate file\n",
    "    else:\n",
    "        data_path=\"/home/adam/Code/Datasets/chestXray14/\"\n",
    "        new_path=data_path+\"resized\"+str(img_size)\n",
    "        if not os.path.exists(new_path):\n",
    "            os.makedirs(new_path)\n",
    "\n",
    "        dirs = [l for l in os.listdir(data_path+\"images/\") if l != '.DS_Store']\n",
    "        for file in dirs:\n",
    "            img=np.array(tf_read_and_resize_image(data_path+\"images/\"+file,img_size))\n",
    "            print(img)\n",
    "            tf.keras.preprocessing.image.save_img(new_path+\"/\"+file,img)\n",
    "            sys.exit(-1)\n",
    "        y=make_xray14_labels()\n",
    "load_resize_and_save(chexpert=True,img_size=32)\n",
    " #####load chestXray14 from files\n",
    "# x_data=np.load(data_path+\"X_sample.npy\")\n",
    "#y=pd.read_csv(data_path+\"sample_labels.csv\")\n",
    "#print(y)\n",
    "# print(y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(223414, 32, 32, 3) (223414, 6)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "X=np.load(\"/home/adam/Code/Datasets/chexpert/chexpert32.npy\")\n",
    "y=np.load(\"/home/adam/Code/Datasets/chexpert/labels.npy\")\n",
    "print(X.shape, y.shape)\n",
    "#for file in filenames:\n",
    "    #img=tf_read_and_resize_image(file)\n",
    "    #\n",
    "    #plt.imshow(tf.cast(img, dtype=tf.uint8))\n",
    "    #img=(img-np.mean(img))/np.std(img)\n",
    "    #plt.imshow(img)\n",
    "    #sys.exit(-1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean, variance 0.36348352 70.18035\n",
      "---------------Load MNIST----------------\n",
      "Training set (60000, 32, 32, 3) (60000, 10)\n",
      "Test set (10000, 32, 32, 3) (10000, 10)\n",
      "\n",
      "\n",
      "mean, variance 1.1809415 74.36859\n",
      "---------------Load MNIST-M----------------\n",
      "Training set (60000, 32, 32, 3) (60000, 10)\n",
      "Test set (10000, 32, 32, 3) (10000, 10)\n",
      "[[10.986111, 5.0, 2.999428, 1.99958, 1.399789, 1.0, 0.71435696, 0.5, 0.33346358, 0.20003448], [0.09088036, 0.19987813, 0.33326975, 0.5, 0.7143216, 1.0, 1.4001397, 2.0, 3.0, 5.0025883]]\n",
      "yo\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#TASK=2\n",
    "#x_source,y_source,x_target,y_target=load_task(TASK)\n",
    "from data import mnist\n",
    "from data import mnist_m as mnistm\n",
    "x_train, y_train, x_test, y_test = mnist.load_mnist()\n",
    "        \n",
    "        \n",
    "###### Add train and test together \n",
    "### MNIST all data\n",
    "x_full=np.append(x_train,x_test, axis=0)\n",
    "y_full=np.append(y_train,y_test, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## shift the distributions to create source and target distributions\n",
    "x_shift, y_shift, x_shift_target, y_shift_target = label_shift_linear(x_full,y_full,1/12,[0,1,2,3,4,5,6,7,8,9])\n",
    "\n",
    "\n",
    "#### MIXED MNIST and MNIST-m\n",
    "x_train_m, y_train_m, x_test_m, y_test_m = mnistm.load_mnistm(y_train,y_test)\n",
    "### MNIST-M all data\n",
    "x_full_m=np.append(x_train_m,x_test_m, axis=0)\n",
    "y_full_m=np.append(y_train_m,y_test_m, axis=0)\n",
    "## shift the distributions to create source and target distributions\n",
    "x_shift_m, y_shift_m,x_shift_target_m, y_shift_target_m = \\\n",
    "label_shift_linear(x_full_m,y_full_m,1/12,[0,1,2,3,4,5,6,7,8,9],decreasing=False)\n",
    "##### calculate the label densities here\n",
    "densities=[]\n",
    "densities.append(np.sum(y_shift,axis=0))\n",
    "densities.append(np.sum(y_shift_m,axis=0))\n",
    "densities.append(np.sum(y_shift_target,axis=0))\n",
    "densities.append(np.sum(y_shift_target_m,axis=0))\n",
    "\n",
    "            \n",
    "L=len(densities[0])\n",
    "interdomain_densities = [[] for x in range(2)]\n",
    "for i in range(L):\n",
    "    ## all densities are (#samples from mnist) over (#samples from mnist-m)\n",
    "    interdomain_densities[0].append(densities[0][i]/densities[1][i])\n",
    "    interdomain_densities[1].append(densities[2][i]/densities[3][i])\n",
    "print(interdomain_densities)\n",
    "## add the shifted data together to create source and target\n",
    "x_source=np.append(x_shift,x_shift_m, axis=0)\n",
    "y_source=np.append(y_shift,y_shift_m, axis=0)\n",
    "x_target=np.append(x_shift_target,x_shift_target_m, axis=0)\n",
    "y_target=np.append(y_shift_target,y_shift_target_m, axis=0)\n",
    "\n",
    "plot_splitbars([0,1,2,3,4,5,6,7,8,9],y_shift,y_shift_m,\"MNIST\",\"MNIST-M\",save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.label_shift import plot_splitbars\n",
    "\n",
    "fig=plt.imshow(x_shift[205])\n",
    "fig.axes.get_xaxis().set_visible(False)\n",
    "fig.axes.get_yaxis().set_visible(False)\n",
    "plt.savefig(\"mnist_ex.png\",dpi=600)\n",
    "#plt.imshow(x_shift_m[205])\n",
    "\n",
    "#plot_splitbars([0,1,2,3,4,5,6,7,8,9],y_shift,y_shift_m,\"MNIST\",\"MNIST-M\",save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_tf():\n",
    "    ### making sure that we have the GPU to work on\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    #logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    if gpus:\n",
    "      # Restrict TensorFlow to only allocate 4GB of memory on the first GPU\n",
    "      # I do not know why I have to do this but gpu does not work otherwise.\n",
    "        try:\n",
    "            tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)])\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        except RuntimeError as e:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "            print(e)\n",
    "\n",
    "    \n",
    "## custom callback to terminate training at some specific value of a metric\n",
    "    \n",
    "class stop_callback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, monitor='accuracy', value=0.001, verbose=0):\n",
    "        super(tf.keras.callbacks.Callback, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.value = value\n",
    "        self.verbose = verbose\n",
    "    def on_epoch_end(self, epoch, logs={}): \n",
    "        if(logs.get('accuracy')> self.value): # select the accuracy\n",
    "            print(\"\\n !!! training error threshold reached, no further training !!!\")\n",
    "            self.model.stop_training = True\n",
    "            \n",
    "class fast_checkpoints(tf.keras.callbacks.Callback):\n",
    "    def __init__(self,checkpoint_path,save_freq):\n",
    "        super(tf.keras.callbacks.Callback, self).__init__()\n",
    "        self.save_freq=save_freq\n",
    "        self.filepath=checkpoint_path\n",
    "        self.verbose=1\n",
    "        self.save_best_only=False\n",
    "        self.save_weights_only=True\n",
    "    def on_train_batch_begin(self, batch, epoch, logs=None):\n",
    "         if batch%self.save_freq==0:\n",
    "\n",
    "            #print(\"\\n Saved weights at the start of batch\"+str(batch)+\"\\n\")\n",
    "\n",
    "            ## Create folder\n",
    "            weight_path = self.filepath+\"/1_\"+str(batch)+\".ckpt\"\n",
    "            os.makedirs(os.path.dirname(weight_path), exist_ok=True)\n",
    "            \n",
    "            self.model.save_weights(weight_path)\n",
    "            \n",
    "def train_posterior(alpha,x_train,y_train,prior_weights=None,x_test=[],y_test=[],save=True,epsilon=0.01,Task=2,Binary=False,batch_size=128):\n",
    "        \n",
    "        TASK=Task\n",
    "        \n",
    "        ### x_test should be the whole of S for early stopping purposes\n",
    "    \n",
    "        checkpoint_path = \"posteriors/\"+\"task\"+str(TASK)+\"/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))\n",
    "        if Binary:\n",
    "            checkpoint_path = \"posteriors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))\n",
    "        \n",
    "        \n",
    "        # Create a callback that saves the model's weights every epoch\n",
    "        checkpoint_freq=np.ceil(len(x_train)/batch_size)\n",
    "        #print(checkpoint_freq)\n",
    "        cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "            save_freq=\"epoch\",#int(checkpoint_freq),### 547 = ceiling(70000/128) i.e training set for MNIST/MNIST-M,\n",
    "            filepath=checkpoint_path+\"/2_{epoch:0d}.ckpt\", \n",
    "            verbose=1,\n",
    "            save_best_only=False,\n",
    "            save_weights_only=True,\n",
    "                ## tune when to save as needed for plots\n",
    "        )\n",
    "        ## callback for first part\n",
    "        fast_checkpoint_freq=np.ceil(len(x_train)/(batch_size*10))\n",
    "        fast_cp_callback =fast_checkpoints(checkpoint_path,int(fast_checkpoint_freq))\n",
    "        stopping_callback=stop_callback(monitor='val_acc',value=1-epsilon)\n",
    "    \n",
    "        M=init_task_model(TASK,Binary)\n",
    "\n",
    "        \n",
    "            \n",
    "        ## choose loss function, optimiser etc. and train\n",
    "        \n",
    "        M.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "               optimizer=tf.keras.optimizers.SGD(learning_rate=0.003, momentum=0.95),\n",
    "                      metrics=['accuracy'],)\n",
    "        \n",
    "        \n",
    "        ## Create the folder\n",
    "        os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
    "        ## remove previous weights\n",
    "        import glob\n",
    "        files=glob.glob(os.path.join(checkpoint_path+'/*'))\n",
    "        \n",
    "        \n",
    "        for file in files:\n",
    "            if file==(checkpoint_path+'/params.txt'):\n",
    "                files.remove(checkpoint_path+'/params.txt')\n",
    "            os.remove(file)\n",
    "        \n",
    "        ### load the prior weights\n",
    "        if prior_weights is not None:\n",
    "            M.set_weights(prior_weights)\n",
    "        elif(alpha==0):\n",
    "            ### save the rand. init as the prior\n",
    "            if Binary:\n",
    "                prior_path=\"priors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(100*alpha))+\"/prior.ckpt\"\n",
    "            else:\n",
    "                prior_path=\"priors/\"+\"task\"+str(TASK)+\"/\"+str(int(100*alpha))+\"/prior.ckpt\"\n",
    "            \n",
    "            ## Create the folder\n",
    "            os.makedirs(os.path.dirname(prior_path), exist_ok=True)\n",
    "\n",
    "            ## Save the weights\n",
    "            M.save_weights(prior_path)\n",
    "        else:\n",
    "            if Binary:\n",
    "                prior_path=\"priors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(100*alpha))+\"/prior.ckpt\"\n",
    "                M.load_weights(prior_path).expect_partial()\n",
    "            else:\n",
    "                \n",
    "                M.load_weights(prior_path).expect_partial()\n",
    "        \n",
    "    \n",
    "        if save:\n",
    "            CALLBACK=[fast_cp_callback,stopping_callback]\n",
    "        else:\n",
    "            CALLBACK=[stopping_callback]\n",
    "        ### train for one epoch with more checkpoints to be able to plot more there\n",
    "        fit_info = M.fit(x_train, y_train,\n",
    "           batch_size=batch_size,\n",
    "           epochs=1, \n",
    "           callbacks=CALLBACK,\n",
    "           validation_data=(x_test, y_test),\n",
    "           verbose=0,\n",
    "                        )\n",
    "        \n",
    "        print(\"-\"*40)\n",
    "        print(\"Finished training first posterior epoch\")\n",
    "        if save:\n",
    "            CALLBACK=[cp_callback,stopping_callback]\n",
    "        else:\n",
    "            CALLBACK=[stopping_callback]\n",
    "            \n",
    "        fit_info = M.fit(x_train, y_train,\n",
    "           batch_size=batch_size,\n",
    "           epochs=2000, # we should have done early stopping before this completes\n",
    "           callbacks=CALLBACK,\n",
    "           validation_data=(x_test, y_test),\n",
    "           verbose=0,\n",
    "                        )\n",
    "        print(\"-\"*40)\n",
    "        print(\"Finished training posterior\")\n",
    "        \n",
    "         #### save the last posterior weights to disk\n",
    "        epochs_trained=len(fit_info.history['loss'])\n",
    "        #if save:\n",
    "            ## Create the folder\n",
    "            #os.makedirs(os.path.dirname(checkpoint_path+\"/2_\"+str(epochs_trained)), exist_ok=True)\n",
    "            \n",
    "            \n",
    "            #M.save_weights(checkpoint_path+\"/2_\"+str(epochs_trained)) ###### check if we need this; TODO!!!!!!\n",
    "            \n",
    "        #### save textfile with parameters, i.e. alpha ,epochs trained and epsilon\n",
    "        if Binary:\n",
    "            with open('posteriors/'+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+'/params.txt', 'w') as f:\n",
    "                f.write('\\n'.join([str(alpha), str(epsilon), str(epochs_trained)]))     \n",
    "            f.close()\n",
    "        else:\n",
    "            with open('posteriors/'+\"task\"+str(TASK)+\"/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+'/params.txt', 'w') as f:\n",
    "                f.write('\\n'.join([str(alpha), str(epsilon), str(epochs_trained)]))     \n",
    "            f.close()\n",
    "        W=M.get_weights()\n",
    "        return W\n",
    "    \n",
    "def train_prior(alpha,total_epochs,x_train=[],y_train=[],x_target=[],y_target=[],save=True,Task=2,Binary=False,batch_size=128):\n",
    "    TASK=Task\n",
    "    \n",
    "    if Binary:\n",
    "        ## Create the folders\n",
    "        os.makedirs(os.path.dirname(\"priors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(100*alpha)))+\"/\", exist_ok=True)\n",
    "        checkpoint_path = \"priors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(100*alpha))\n",
    "    else:\n",
    "        os.makedirs(os.path.dirname(\"priors/\"+\"task\"+str(TASK)+\"/\"+str(int(100*alpha)))+\"/\", exist_ok=True)\n",
    "        checkpoint_path = \"priors/\"+\"task\"+str(TASK)+\"/\"+str(int(100*alpha))\n",
    "    ## remove previous weights\n",
    "    import glob\n",
    "    files=glob.glob(os.path.join(checkpoint_path+'/*'))\n",
    "    for file in files:\n",
    "        os.remove(file)\n",
    "    M=init_task_model(TASK,Binary)\n",
    "    #sys.exit(-1)\n",
    "    ### save checkpoints 10 times over the training of the prior\n",
    "    l=len(x_train)\n",
    "    checkpoint_freq=np.ceil(l/(batch_size*10))\n",
    "    fast_cp_callback =fast_checkpoints(checkpoint_path,int(checkpoint_freq))\n",
    "    if save:\n",
    "            CALLBACK=[fast_cp_callback]\n",
    "    else:\n",
    "            CALLBACK=[]\n",
    "            \n",
    "    ## choose loss function, optimiser etc. and train\n",
    "    M.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "               optimizer=tf.keras.optimizers.SGD(learning_rate=0.003, momentum=0.95),\n",
    "                      metrics=['accuracy'],)\n",
    "    fit_info = M.fit(x_train, y_train,\n",
    "           batch_size=batch_size,\n",
    "           callbacks=CALLBACK,\n",
    "           epochs=total_epochs,\n",
    "           verbose=0,\n",
    "                        )\n",
    "    print(\"-\"*40)\n",
    "    print(\"Finished training prior\")\n",
    "    #### save the final prior weights to disk\n",
    "    if save:\n",
    "        os.makedirs(checkpoint_path, exist_ok=True)\n",
    "        M.save_weights(checkpoint_path+\"/prior.ckpt\")\n",
    "     \n",
    "    \n",
    "    list1=[]\n",
    "    \n",
    "    dirFiles = os.listdir(checkpoint_path) #list of directory files\n",
    "    \n",
    "    ## remove the ckpt.index and sort so that we get the epochs that are in the directory\n",
    "    for files in dirFiles: #filter out all non weights\n",
    "        if '.ckpt.index' in files:\n",
    "            name = re.sub('\\.ckpt.index$', '', files)\n",
    "            if (name[0]==\"1\"):\n",
    "                list1.append(name)\n",
    "        \n",
    "    list1.sort(key=lambda f: int(re.sub('\\D', '', f)))\n",
    "    list1.append(\"prior\")    ## add the final weights which has no number\n",
    "    \n",
    "    Ws=list1\n",
    "    weight_updates=[]\n",
    "    for i in Ws:\n",
    "        if i[0]==\"1\":\n",
    "            if i[1]==\"_\":\n",
    "                weight_updates.append(int(i[2:]))\n",
    "    weight_updates.append(int(np.ceil(len(y_train)/batch_size)))\n",
    "    \n",
    "    error=[]\n",
    "    target_error=[]\n",
    "    for checkpoint in Ws:\n",
    "\n",
    "        model=init_task_model(TASK,Binary)\n",
    "        model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "               optimizer=tf.keras.optimizers.SGD(learning_rate=0.003, momentum=0.95),\n",
    "                  metrics=['accuracy'],)\n",
    "      \n",
    "            \n",
    "        model.load_weights(checkpoint_path+\"/\"+str(checkpoint)+\".ckpt\").expect_partial()\n",
    "        target_error.append(1-model.evaluate(x_target,y_target,verbose=0)[1])\n",
    "        error.append(1-model.evaluate(x_train,y_train,verbose=0)[1])\n",
    "    print(\"-\"*40)\n",
    "    print(\"Finished computing prior sample and target errors\")\n",
    "    if save:\n",
    "        results=pd.DataFrame({'Weightupdates': weight_updates,\n",
    "            'Trainerror': error,\n",
    "            'targeterror':target_error,\n",
    "            })\n",
    "       \n",
    "        ## Create the folders\n",
    "        os.makedirs(os.path.dirname(project_folder+\"mnist_transfer/results/\"+\"task\"+str(TASK)+\"/Binary/\"), exist_ok=True)\n",
    "        if Binary:\n",
    "            with open(project_folder+\"mnist_transfer/results/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(100*alpha))+\"_prior_results.pkl\",'wb') as f:\n",
    "                pickle.dump(results,f)\n",
    "            f.close()\n",
    "        else:\n",
    "            with open(project_folder+\"mnist_transfer/results/\"+\"task\"+str(TASK)+\"/\"+str(int(100*alpha))+\"_prior_results.pkl\",'wb') as f:\n",
    "                pickle.dump(results,f)\n",
    "            f.close()\n",
    "    return model.get_weights()\n",
    "    \n",
    "\n",
    "\n",
    "def plot_result_file(epsilon,alpha,sigma,Binary=False,Task=2):\n",
    "    sigma_tmp=sigma\n",
    "    sigma=sigma[0]*10**(-1*sigma[1])\n",
    "    \n",
    "    if Binary:\n",
    "        result_path=\"results/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"+str(sigma_tmp[0])+str(sigma_tmp[1])\n",
    "        plt.title(\"Binary: \"+r\"$\\alpha$=\"+str(alpha)+r\" $\\epsilon$=\"+str(epsilon)+r\" $\\sigma$=\"+str(sigma))\n",
    "    else:\n",
    "        result_path=\"results/\"+\"task\"+str(TASK)+\"/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"+str(sigma_tmp[0])+str(sigma_tmp[1])\n",
    "        plt.title(r\"$\\alpha$=\"+str(alpha)+r\" $\\epsilon$=\"+str(epsilon)+r\" $\\sigma$=\"+str(sigma))\n",
    "    results=pd.read_pickle(result_path+\"_results.pkl\")\n",
    "    \n",
    "    ### do the plots\n",
    "    plt.plot(results[\"Weightupdates\"],results[\"Bound\"],'r*-')\n",
    "    plt.plot(results[\"Weightupdates\"],results[\"Trainerror\"],'m^-')\n",
    "    \n",
    "    plt.xlabel(\"Weight updates\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    \n",
    "    plt.legend([\"Bound\",\"Empirical error\"])\n",
    "    plt.show()\n",
    "\n",
    "def find_optimal_sigma(sigmas,epsilon, alpha,Binary=False,Task=2):\n",
    "    #### to find the optimal sigma just do a search through all the results \n",
    "    #### and save the one for each parameter which has the minimal bound\n",
    "    #### Do we do this per epoch or for some other value? The sigma which yields the lowest bound overall for some epoch?\n",
    "    optimal=[0,1]\n",
    "    # search through all epochs and pick the sigma which yields the smallest bound during the whole training process\n",
    "    for sigma in sigmas:\n",
    "        sigma_tmp=sigma\n",
    "        sigma=sigma[0]*10**(-1*sigma[1])\n",
    "        if Binary:\n",
    "            result_path=\"results/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"+str(sigma_tmp[0])+str(sigma_tmp[1])\n",
    "        else:\n",
    "            result_path=\"results/\"+\"task\"+str(TASK)+\"/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"+str(sigma_tmp[0])+str(sigma_tmp[1])\n",
    "        results=pd.read_pickle(result_path+\"_results.pkl\")\n",
    "        MIN=np.min(results[\"Bound\"])\n",
    "        if (MIN<optimal[1]):\n",
    "            optimal[1]=MIN\n",
    "            optimal[0]=sigma\n",
    "    print(\"The optimal sigma is {} with bound value {}\".format(optimal[0],optimal[1]))\n",
    "\n",
    "   \n",
    "#### find the optimal sigma for every combination of parameters\n",
    "\n",
    "#### use the optimal sigmas to calculate the bound 50 times(with different data orders and initialisation)\n",
    "#### (Note: also delta=13*delta_0) for every combination \n",
    "# def read_prior(alpha,TASK=2,Binary=True):\n",
    "#     checkpoint_path = \"priors/\"+\"task\"+str(TASK)+\"/\"+str(int(100*alpha))\n",
    "#     if Binary:\n",
    "#         checkpoint_path = \"priors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(100*alpha))\n",
    "#     result_path=project_folder+'mnist_transfer/'+checkpoint_path+\"/results.pkl\"\n",
    "#     results=pd.read_pickle(result_path)\n",
    "#     plt.title(r\"$\\alpha$=\"+str(alpha))\n",
    "#     plt.plot(results[\"Weightupdates\"],results[\"Trainerror\"],'m^-')\n",
    "#     plt.plot(results[\"Weightupdates\"],results[\"targeterror\"],'k^-')\n",
    "#     plt.legend([\"Training error\",\"Target error\"])\n",
    "    \n",
    "# def plot_prior_and_posterior(alpha,epsilon,sigma,TASK=2,Binary=True):\n",
    "#     ### load in the prior data\n",
    "#     sigma_tmp=sigma\n",
    "#     sigma=sigma[0]*10**(-1*sigma[1])\n",
    "#     checkpoint_path = \"priors/\"+\"task\"+str(TASK)+\"/\"+str(int(100*alpha))\n",
    "#     if Binary:\n",
    "#         checkpoint_path = \"priors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(100*alpha))\n",
    "#     result_path=project_folder+'mnist_transfer/'+checkpoint_path+\"/results.pkl\"\n",
    "#     results=pd.read_pickle(result_path)\n",
    "#     result_path_post=\"results/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"+str(sigma_tmp[0])+str(sigma_tmp[1])\n",
    "#     results2=pd.read_pickle(result_path_post+\"_results.pkl\")\n",
    "    \n",
    "    \n",
    "#     ### remove/ignore the last entry of the prior data \n",
    "#     ### as it should be a duplication of the first one from the posterior results\n",
    "    \n",
    "#     ### training error\n",
    "#     A=list(results2[\"Weightupdates\"]+list(results[\"Weightupdates\"])[-1])\n",
    "#     B=list(results[\"Weightupdates\"])[:-1]\n",
    "#     B.extend(A)\n",
    "#     C=list(results[\"Trainerror\"])[:-1]\n",
    "#     C.extend(list(results2[\"train_germain\"]))\n",
    "#     plt.plot(B,C,'-m^')\n",
    "    \n",
    "#     ## target error\n",
    "#     D=list(results[\"targeterror\"])[:-1]\n",
    "#     D.extend(list(results2[\"target_germain\"]))\n",
    "#     plt.plot(B,D,'-k*')\n",
    "    \n",
    "#     ### bound\n",
    "#     E=results2[\"germain_bound\"]\n",
    "#     plt.plot(A,E,'-D')\n",
    "#     F=results2['boundpart3_germain']\n",
    "#     plt.plot(A,F,'-o')\n",
    "#     print(results2[\"target_germain\"])\n",
    "#     print(results2[\"germain_bound\"])\n",
    "#     ### lines for uninformative region and worse than random guessing; also for end of prior training\n",
    "#     plt.axvline(A[0],color=\"grey\")\n",
    "#     plt.axhline(y=0.5, color=\"black\", linestyle=\"--\")\n",
    "#     plt.axhline(y=1, color=\"red\", linestyle=\"--\")\n",
    "#     plt.legend([\"Training error\",\"Target error\",\"Bound\",\"KL-part\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alphas=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "alphas=[0,0.8]\n",
    "epsilons=[0.03]#,0.01,0.001]\n",
    "tasks=[2]\n",
    "for t in tasks:\n",
    "    x_source,y_source,x_target,y_target=load_task(t)\n",
    "\n",
    "    y_source_bin=np.array(make_mnist_binary(y_source))\n",
    "    y_target_bin=np.array(make_mnist_binary(y_target))\n",
    "    for alpha in alphas:\n",
    "        if alpha==0:\n",
    "            pass\n",
    "        else:\n",
    "            print(\"Alpha is:\"+str(alpha))\n",
    "            x_bound, x_prior, y_bound , y_prior = train_test_split(x_source,y_source_bin,test_size=alpha,random_state=69105)\n",
    "            w_a=train_prior(alpha,1,x_source,y_source_bin,x_target=x_target,y_target=y_target_bin,save=True,Task=t,Binary=True,batch_size=128)\n",
    "        for epsilon in epsilons:\n",
    "            w_s=train_posterior(alpha,x_source,y_source_bin,None,x_test=x_source,y_test=y_source_bin,epsilon=epsilon,Task=t,Binary=True,batch_size=128)\n",
    "            #for sigma in sigmas:\n",
    "                #res=read_and_prepare_results(alpha,x_source,y_source_bin,x_target,y_target_bin,sigma,delta,len(x_source),epsilon,Binary=True)#x_bound,y_bound,x_target,y_target_bin,sigma,delta,len(x_bound),epsilon,Binary=True)\n",
    "                #plot_result_file(epsilon,alpha,sigma,TASK,Binary=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
