{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function \n",
    "\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import gc, re, copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.keras.layers import deserialize, serialize\n",
    "from tensorflow.python.keras.saving import saving_utils\n",
    "\n",
    "# Project imports \n",
    "from data import mnist,mnist_m\n",
    "from data.tasks import *\n",
    "from data.label_shift import label_shift_linear, plot_labeldist, plot_splitbars\n",
    "from experiments.training import *\n",
    "from experiments.SL_bound import *\n",
    "from util.kl import *\n",
    "from util.misc import *\n",
    "from results.plotting import *\n",
    "\n",
    "# Hyper-parameters\n",
    "#batch_size = 128\n",
    "num_classes = 10\n",
    "make_plots = False\n",
    "\n",
    "delta=0.05 ## what would this be?   \n",
    "\n",
    "alphas=[]\n",
    "length=10\n",
    "for i in range(length-1):\n",
    "    alphas.append((i+1)/length)\n",
    "\n",
    "epsilons=[0.03,0.01,0.001]\n",
    "\n",
    "sigmas=[]\n",
    "for i in range(2,9):  \n",
    "    sigmas.append([3,i])\n",
    "    if(i==8):\n",
    "        break\n",
    "    sigmas.append([1,i])\n",
    "\n",
    "img_rows, img_cols = 32, 32\n",
    "project_folder = \"/cephyr/users/adambre/Alvis/\"\n",
    "\n",
    "\n",
    "### TODO\n",
    "\n",
    "# # check output_dir, create it if not exists\n",
    "#     if not os.path.isdir(output_dir):\n",
    "#         os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_source,y_source,x_target,y_target=load_task(TASK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-5d96f6f580d0>, line 150)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-5d96f6f580d0>\"\u001b[0;36m, line \u001b[0;32m150\u001b[0m\n\u001b[0;31m    filepath=checkpoint_path+\"/2_{epoch:0d}.ckpt\",\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def read_weights(model,w_a,x_bound,y_bound,x_target,y_target,sigma,epsilon,alpha,Binary=False,Task=2):\n",
    "    batch_size=128\n",
    "    batches_per_epoch=np.ceil(len(y_target)/batch_size) ## should be 547\n",
    "    epoch=1\n",
    "    \n",
    "    # Run the function to fix pickling issue\n",
    "    make_keras_picklable()\n",
    "    \n",
    "    \n",
    "    sigma=sigma[0]*10**(-1*sigma[1])    \n",
    "    \n",
    "    ### Here we do something more intelligent to not have to hardcode the epoch amounts. \n",
    "    ### we parse the filenames and sort them in numerical order and then load the weights\n",
    "    if Binary:\n",
    "        path=\"posteriors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))\n",
    "    else:\n",
    "        path=\"posteriors/\"+\"task\"+str(TASK)+\"/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))\n",
    "        #epochs_trained\n",
    "    #epochs = [] #list of \n",
    "    list1=[]\n",
    "    list2=[]\n",
    "    dirFiles = os.listdir(path) #list of directory files\n",
    "    ## remove the ckpt.index and sort so that we get the epochs that are in the directory\n",
    "    for files in dirFiles: #filter out all non checkpoints\n",
    "        if '.ckpt.index' in files:\n",
    "            name = re.sub('\\.ckpt.index$', '', files)\n",
    "            ### if it has a one it goes in one list and if it starts with a two it goes in the other\n",
    "            if (name[0]==\"1\"):\n",
    "                list1.append(name)\n",
    "            elif (name[0]==\"2\"):\n",
    "                list2.append(name)\n",
    "            #epochs.append(name)\n",
    "    #epochs.sort(key=lambda f: int(re.sub('\\D', '', f)))\n",
    "    list1.sort(key=lambda f: int(re.sub('\\D', '', f)))\n",
    "    num_batchweights=len(list1)\n",
    "    list2.sort(key=lambda f: int(re.sub('\\D', '', f)))\n",
    "    list1.extend(list2)\n",
    "    Ws=list1 ## vector of checkpoint filenames\n",
    "    \n",
    "    weight_updates=[]\n",
    "    for i in Ws:\n",
    "        #print(i)\n",
    "        if i[0]==\"1\":\n",
    "            if i[1]==\"_\":\n",
    "                weight_updates.append(int(i[2:]))\n",
    "    for i in list2:\n",
    "        weight_updates.append((int(i[2:])+1)*batches_per_epoch)\n",
    "   \n",
    "    ### load the model and the weights\n",
    "    N_checkpoints=len(Ws)\n",
    "    KLs=np.zeros(N_checkpoints)\n",
    "    errors=np.zeros(N_checkpoints)\n",
    "    targeterrors=np.zeros(N_checkpoints)\n",
    "    epochs=np.zeros(N_checkpoints)\n",
    "#     for checkpoint in Ws:\n",
    "    \n",
    "    #### here we should pass all the checkpoints to different processes and evaluate on the dataset\n",
    "    args=[]\n",
    "    #for i in range(N_checkpoints):\n",
    "    #for i in range(2):\n",
    "        #args.append(nnp.array(i,Ws[i],path,KLs, errors,targeterrors,epochs,model,w_a,copy.deepcopy(x_bound),copy.deepcopy(y_bound),copy.deepcopy(x_target),copy.deepcopy(y_target),sigma,epsilon,alpha,Binary,TASK))\n",
    "        #args.append(np.array([i,Ws[i],path,KLs, errors,targeterrors,epochs,model,w_a,x_bound,y_bound,x_target,y_target,sigma,epsilon,alpha,Binary,TASK]))\n",
    "    args.append([Ws[0],x_bound,y_bound])   \n",
    "    args.append([Ws[1],copy.deepcopy(x_bound),copy.deepcopy(y_bound)])\n",
    "    args.append([Ws[2],copy.deepcopy(x_bound),copy.deepcopy(y_bound)])   \n",
    "    args.append([Ws[3],copy.deepcopy(x_bound),copy.deepcopy(y_bound)])\n",
    "    p = Pool(processes = 3)\n",
    "    print(\"could deep copy\")\n",
    "    #for arg in args:\n",
    "    if __name__ == '__main__':\n",
    "        p.imap(dumb_func,args)\n",
    "        p.close()\n",
    "        p.join()\n",
    "\n",
    "    #print(\"we made it here!!!!\")\n",
    "    print(KLs)\n",
    "    print(errors)\n",
    "    print(targeterrors)\n",
    "    sys.exit(-1)\n",
    "    \n",
    "    return KLs,errors,targeterrors,Ws,Xvector\n",
    "\n",
    "    \n",
    "def init_tf():\n",
    "    ### making sure that we have the GPU to work on\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    #logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    if gpus:\n",
    "      # Restrict TensorFlow to only allocate 4GB of memory on the first GPU\n",
    "      # I do not know why I have to do this but gpu does not work otherwise.\n",
    "        try:\n",
    "            tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)])\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        except RuntimeError as e:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "            print(e)\n",
    "\n",
    "    \n",
    "## custom callback to terminate training at some specific value of a metric\n",
    "    \n",
    "class stop_callback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, monitor='accuracy', value=0.001, verbose=0):\n",
    "        super(tf.keras.callbacks.Callback, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.value = value\n",
    "        self.verbose = verbose\n",
    "    def on_epoch_end(self, epoch, logs={}): \n",
    "        if(logs.get('accuracy')> self.value): # select the accuracy\n",
    "            print(\"\\n !!! training error threshold reached, no further training !!!\")\n",
    "            self.model.stop_training = True\n",
    "            \n",
    "class fast_checkpoints(tf.keras.callbacks.Callback):\n",
    "    def __init__(self,checkpoint_path,save_freq):\n",
    "        super(tf.keras.callbacks.Callback, self).__init__()\n",
    "        self.save_freq=save_freq\n",
    "        self.filepath=checkpoint_path\n",
    "        self.verbose=1\n",
    "        self.save_best_only=False\n",
    "        self.save_weights_only=True\n",
    "    def on_train_batch_begin(self, batch, epoch, logs=None):\n",
    "         if batch%self.save_freq==0:\n",
    "\n",
    "            #print(\"\\n Saved weights at the start of batch\"+str(batch)+\"\\n\")\n",
    "\n",
    "            ## Create folder\n",
    "            weight_path = self.filepath+\"/1_\"+str(batch)+\".ckpt\"\n",
    "            os.makedirs(os.path.dirname(weight_path), exist_ok=True)\n",
    "            \n",
    "            self.model.save_weights(weight_path)\n",
    "            \n",
    "def train_posterior(alpha,x_train,y_train,prior_weights=None,x_test=[],y_test=[],save=True,epsilon=0.01,Task=2,Binary=False,batch_size=128):\n",
    "        \n",
    "        TASK=Task\n",
    "        \n",
    "        ### x_test should be the whole of S for early stopping purposes\n",
    "    \n",
    "        checkpoint_path = \"posteriors/\"+\"task\"+str(TASK)+\"/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))\n",
    "        if Binary:\n",
    "            checkpoint_path = \"posteriors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))\n",
    "        \n",
    "        \n",
    "        # Create a callback that saves the model's weights every epoch\n",
    "        checkpoint_freq=np.ceil(len(x_train)/batch_size)\n",
    "        print(checkpoint_freq)\n",
    "        cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "            save_freq=\"epoch\"#int(checkpoint_freq),### 547 = ceiling(70000/128) i.e training set for MNIST/MNIST-M,\n",
    "            filepath=checkpoint_path+\"/2_{epoch:0d}.ckpt\", \n",
    "            verbose=1,\n",
    "            save_best_only=False,\n",
    "            save_weights_only=True,\n",
    "                ## tune when to save as needed for plots\n",
    "        )\n",
    "        ## callback for first part\n",
    "        fast_checkpoint_freq=np.ceil(len(x_train)/(batch_size*10))\n",
    "        fast_cp_callback =fast_checkpoints(checkpoint_path,int(fast_checkpoint_freq))\n",
    "        stopping_callback=stop_callback(monitor='val_acc',value=1-epsilon)\n",
    "    \n",
    "        M=init_task_model(TASK,Binary)\n",
    "\n",
    "        \n",
    "            \n",
    "        ## choose loss function, optimiser etc. and train\n",
    "        \n",
    "        M.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "               optimizer=tf.keras.optimizers.SGD(learning_rate=0.003, momentum=0.95),\n",
    "                      metrics=['accuracy'],)\n",
    "        ### load the prior weights\n",
    "        if prior_weights is not None:\n",
    "            M.set_weights(prior_weights)\n",
    "        elif(alpha==0):\n",
    "            ### save the rand. init as the prior\n",
    "            if Binary:\n",
    "                prior_path=\"priors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(100*alpha))+\"/prior.ckpt\"\n",
    "            else:\n",
    "                prior_path=\"priors/\"+\"task\"+str(TASK)+\"/\"+str(int(100*alpha))+\"/prior.ckpt\"\n",
    "            \n",
    "            ## Create the folder\n",
    "            os.makedirs(os.path.dirname(prior_path), exist_ok=True)\n",
    "            \n",
    "            ## Save the weights\n",
    "            M.save_weights(prior_path)\n",
    "        else:\n",
    "            if Binary:\n",
    "                prior_path=\"priors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(100*alpha))+\"/prior.ckpt\"\n",
    "                M.load_weights(prior_path)#.expect_partial()\n",
    "            else:\n",
    "                \n",
    "                M.load_weights(prior_path)#.expect_partial()\n",
    "        \n",
    "    \n",
    "        if save:\n",
    "            CALLBACK=[fast_cp_callback,stopping_callback]\n",
    "        else:\n",
    "            CALLBACK=[stopping_callback]\n",
    "        ### train for one epoch with more checkpoints to be able to plot more there\n",
    "        fit_info = M.fit(x_train, y_train,\n",
    "           batch_size=batch_size,\n",
    "           epochs=1, \n",
    "           callbacks=CALLBACK,\n",
    "           validation_data=(x_test, y_test),\n",
    "           verbose=1,\n",
    "                        )\n",
    "        \n",
    "        print(\"-\"*40)\n",
    "        print(\"Finished training first posterior epoch\")\n",
    "        if save:\n",
    "            CALLBACK=[cp_callback,stopping_callback]\n",
    "        else:\n",
    "            CALLBACK=[stopping_callback]\n",
    "            \n",
    "        fit_info = M.fit(x_train, y_train,\n",
    "           batch_size=batch_size,\n",
    "           epochs=2000, # we should have done early stopping before this completes\n",
    "           callbacks=CALLBACK,\n",
    "           validation_data=(x_test, y_test),\n",
    "           verbose=1,\n",
    "                        )\n",
    "        print(\"-\"*40)\n",
    "        print(\"Finished training posterior\")\n",
    "        \n",
    "         #### save the last posterior weights to disk\n",
    "        epochs_trained=len(fit_info.history['loss'])\n",
    "        if save:\n",
    "            ## Create the folder\n",
    "            os.makedirs(os.path.dirname(checkpoint_path+\"/2_\"+str(epochs_trained)), exist_ok=True)\n",
    "            \n",
    "            M.save_weights(checkpoint_path+\"/2_\"+str(epochs_trained)) ###### check if we need this; TODO!!!!!!\n",
    "            \n",
    "        #### save textfile with parameters, i.e. alpha ,epochs trained and epsilon\n",
    "        if Binary:\n",
    "            with open('posteriors/'+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+'/params.txt', 'w') as f:\n",
    "                f.write('\\n'.join([str(alpha), str(epsilon), str(epochs_trained)]))     \n",
    "            f.close()\n",
    "        else:\n",
    "            with open('posteriors/'+\"task\"+str(TASK)+\"/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+'/params.txt', 'w') as f:\n",
    "                f.write('\\n'.join([str(alpha), str(epsilon), str(epochs_trained)]))     \n",
    "            f.close()\n",
    "        W=M.get_weights()\n",
    "        return W\n",
    "    \n",
    "def train_prior(alpha,total_epochs,x_train=[],y_train=[],x_target=[],y_target=[],save=True,Task=2,Binary=False,batch_size=128):\n",
    "    TASK=Task\n",
    "    \n",
    "    if Binary:\n",
    "        ## Create the folders\n",
    "        os.makedirs(os.path.dirname(\"priors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(100*alpha)))+\"/\", exist_ok=True)\n",
    "        checkpoint_path = \"priors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(100*alpha))\n",
    "    else:\n",
    "        os.makedirs(os.path.dirname(\"priors/\"+\"task\"+str(TASK)+\"/\"+str(int(100*alpha)))+\"/\", exist_ok=True)\n",
    "        checkpoint_path = \"priors/\"+\"task\"+str(TASK)+\"/\"+str(int(100*alpha))\n",
    "    \n",
    "    M=init_task_model(TASK,Binary)\n",
    "    #sys.exit(-1)\n",
    "    ### save checkpoints 10 times over the training of the prior\n",
    "    l=len(x_train)\n",
    "    checkpoint_freq=np.ceil(l/(batch_size*10))\n",
    "    fast_cp_callback =fast_checkpoints(checkpoint_path,int(checkpoint_freq))\n",
    "    if save:\n",
    "            CALLBACK=[fast_cp_callback]\n",
    "    else:\n",
    "            CALLBACK=[]\n",
    "            \n",
    "    ## choose loss function, optimiser etc. and train\n",
    "    M.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "               optimizer=tf.keras.optimizers.SGD(learning_rate=0.003, momentum=0.95),\n",
    "                      metrics=['accuracy'],)\n",
    "    fit_info = M.fit(x_train, y_train,\n",
    "           batch_size=batch_size,\n",
    "           callbacks=CALLBACK,\n",
    "           epochs=total_epochs,\n",
    "           verbose=0,\n",
    "                        )\n",
    "    print(\"-\"*40)\n",
    "    print(\"Finished training prior\")\n",
    "    #### save the final prior weights to disk\n",
    "    if save:\n",
    "        os.makedirs(checkpoint_path, exist_ok=True)\n",
    "        M.save_weights(checkpoint_path+\"/prior.ckpt\")\n",
    "     \n",
    "    \n",
    "    list1=[]\n",
    "    \n",
    "    dirFiles = os.listdir(checkpoint_path) #list of directory files\n",
    "    \n",
    "    ## remove the ckpt.index and sort so that we get the epochs that are in the directory\n",
    "    for files in dirFiles: #filter out all non weights\n",
    "        if '.ckpt.index' in files:\n",
    "            name = re.sub('\\.ckpt.index$', '', files)\n",
    "            if (name[0]==\"1\"):\n",
    "                list1.append(name)\n",
    "        \n",
    "    list1.sort(key=lambda f: int(re.sub('\\D', '', f)))\n",
    "    list1.append(\"prior\")    ## add the final weights which has no number\n",
    "    \n",
    "    Ws=list1\n",
    "    weight_updates=[]\n",
    "    for i in Ws:\n",
    "        if i[0]==\"1\":\n",
    "            if i[1]==\"_\":\n",
    "                weight_updates.append(int(i[2:]))\n",
    "    weight_updates.append(int(np.ceil(len(y_train)/batch_size)))\n",
    "    \n",
    "    error=[]\n",
    "    target_error=[]\n",
    "    for checkpoint in Ws:\n",
    "\n",
    "        model=init_task_model(TASK,Binary)\n",
    "        model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "               optimizer=tf.keras.optimizers.SGD(learning_rate=0.003, momentum=0.95),\n",
    "                  metrics=['accuracy'],)\n",
    "      \n",
    "            \n",
    "        model.load_weights(checkpoint_path+\"/\"+str(checkpoint)+\".ckpt\")#.expect_partial()\n",
    "        target_error.append(1-model.evaluate(x_target,y_target,verbose=0)[1])\n",
    "        error.append(1-model.evaluate(x_train,y_train,verbose=0)[1])\n",
    "    print(\"-\"*40)\n",
    "    print(\"Finished computing prior sample and target errors\")\n",
    "    if save:\n",
    "        results=pd.DataFrame({'Weightupdates': weight_updates,\n",
    "            'Trainerror': error,\n",
    "            'targeterror':target_error,\n",
    "            })\n",
    "        with open(project_folder+'mnist_transfer/'+checkpoint_path+\"/results.pkl\",'wb') as f:\n",
    "            pickle.dump(results,f)\n",
    "        f.close()\n",
    "    \n",
    "    return model.get_weights()\n",
    "    \n",
    "def read_and_prepare_results(alpha,x_bound,y_bound,x_target,y_target,sigma,delta,N,epsilon,Binary=False,Task=2):\n",
    "    \n",
    "    sigma_tmp=sigma\n",
    "    sigma=sigma[0]*10**(-1*sigma[1])\n",
    "    \n",
    "    ## read params.txt for the desired alpha and get the parameters\n",
    "    if Binary:\n",
    "        with open('posteriors/'+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+'/params.txt', 'rb+') as f:\n",
    "            params=f.readlines()\n",
    "        f.close()\n",
    "        prior_path=\"priors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(100*alpha))+\"/prior.ckpt\"\n",
    "        result_path=\"results/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"\n",
    "    else:\n",
    "        with open('posteriors/'+\"task\"+str(TASK)+\"/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+'/params.txt', 'rb+') as f:\n",
    "            params=f.readlines()\n",
    "        f.close()\n",
    "        prior_path=\"priors/\"+\"task\"+str(TASK)+\"/\"+str(int(100*alpha))+\"/prior.ckpt\"\n",
    "        result_path=\"results/\"+\"task\"+str(TASK)+\"/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"\n",
    "        \n",
    "    epsilon=float(params[1])\n",
    "    epochs_trained=int(params[2])\n",
    "    \n",
    "    # initialise model\n",
    "    M=init_task_model(TASK,Binary)\n",
    "    M.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "               optimizer=tf.keras.optimizers.SGD(learning_rate=0.003, momentum=0.95),\n",
    "                      metrics=['accuracy'],)\n",
    "    ### load the prior weights if there are any\n",
    "    if alpha==0:\n",
    "        ### do nothing, i.e take the random init\n",
    "        w_a=M.get_weights()\n",
    "    else:\n",
    "        M.load_weights(prior_path).expect_partial()\n",
    "        w_a=M.get_weights()\n",
    "    print(Binary)\n",
    "    # read the weights and calculate what is needed for the bound\n",
    "    [KLs,errors,targeterrors,Ws,weight_updates]=read_weights(M,w_a,x_bound,y_bound,x_target,y_target,sigma_tmp,epsilon,alpha,Binary=Binary,Task=TASK)    \n",
    "\n",
    "    bound=[]\n",
    "    ### calculate the bound\n",
    "    for i in range(len(weight_updates)):\n",
    "        bound.append(calculate_bound(KLs[i],alpha,delta,N,errors[i]))\n",
    "    \n",
    "    \n",
    "    #save the results to a pickled dataframe in results\n",
    "    results=pd.DataFrame({'Weightupdates': weight_updates,\n",
    "        'Trainerror': errors,\n",
    "        'targeterror':targeterrors,\n",
    "        'KL': KLs,\n",
    "        'Bound': bound})\n",
    "    with open(project_folder+'mnist_transfer/'+result_path+str(sigma_tmp[0])+str(sigma_tmp[1])+\"_results.pkl\",'wb') as f:#int(sigma*10**8)\n",
    "        pickle.dump(results,f)\n",
    "    f.close()\n",
    "    return results\n",
    "\n",
    "def plot_result_file(epsilon,alpha,sigma,Binary=False,Task=2):\n",
    "    sigma_tmp=sigma\n",
    "    sigma=sigma[0]*10**(-1*sigma[1])\n",
    "    \n",
    "    if Binary:\n",
    "        result_path=\"results/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"+str(sigma_tmp[0])+str(sigma_tmp[1])\n",
    "        plt.title(\"Binary: \"+r\"$\\alpha$=\"+str(alpha)+r\" $\\epsilon$=\"+str(epsilon)+r\" $\\sigma$=\"+str(sigma))\n",
    "    else:\n",
    "        result_path=\"results/\"+\"task\"+str(TASK)+\"/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"+str(sigma_tmp[0])+str(sigma_tmp[1])\n",
    "        plt.title(r\"$\\alpha$=\"+str(alpha)+r\" $\\epsilon$=\"+str(epsilon)+r\" $\\sigma$=\"+str(sigma))\n",
    "    results=pd.read_pickle(result_path+\"_results.pkl\")\n",
    "    \n",
    "    ### do the plots\n",
    "    plt.plot(results[\"Weightupdates\"],results[\"Bound\"],'r*-')\n",
    "    plt.plot(results[\"Weightupdates\"],results[\"Trainerror\"],'m^-')\n",
    "    \n",
    "    plt.xlabel(\"Weight updates\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    \n",
    "    plt.legend([\"Bound\",\"Empirical error\"])\n",
    "    plt.show()\n",
    "\n",
    "def find_optimal_sigma(sigmas,epsilon, alpha,Binary=False,Task=2):\n",
    "    #### to find the optimal sigma just do a search through all the results \n",
    "    #### and save the one for each parameter which has the minimal bound\n",
    "    #### Do we do this per epoch or for some other value? The sigma which yields the lowest bound overall for some epoch?\n",
    "    optimal=[0,1]\n",
    "    # search through all epochs and pick the sigma which yields the smallest bound during the whole training process\n",
    "    for sigma in sigmas:\n",
    "        sigma_tmp=sigma\n",
    "        sigma=sigma[0]*10**(-1*sigma[1])\n",
    "        if Binary:\n",
    "            result_path=\"results/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"+str(sigma_tmp[0])+str(sigma_tmp[1])\n",
    "        else:\n",
    "            result_path=\"results/\"+\"task\"+str(TASK)+\"/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"+str(sigma_tmp[0])+str(sigma_tmp[1])\n",
    "        results=pd.read_pickle(result_path+\"_results.pkl\")\n",
    "        MIN=np.min(results[\"Bound\"])\n",
    "        if (MIN<optimal[1]):\n",
    "            optimal[1]=MIN\n",
    "            optimal[0]=sigma\n",
    "    print(\"The optimal sigma is {} with bound value {}\".format(optimal[0],optimal[1]))\n",
    "\n",
    "   \n",
    "#### find the optimal sigma for every combination of parameters\n",
    "\n",
    "#### use the optimal sigmas to calculate the bound 50 times(with different data orders and initialisation)\n",
    "#### (Note: also delta=13*delta_0) for every combination \n",
    "def read_prior(alpha,TASK=2,Binary=True):\n",
    "    checkpoint_path = \"priors/\"+\"task\"+str(TASK)+\"/\"+str(int(100*alpha))\n",
    "    if Binary:\n",
    "        checkpoint_path = \"priors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(100*alpha))\n",
    "    result_path=project_folder+'mnist_transfer/'+checkpoint_path+\"/results.pkl\"\n",
    "    results=pd.read_pickle(result_path)\n",
    "    plt.title(r\"$\\alpha$=\"+str(alpha))\n",
    "    plt.plot(results[\"Weightupdates\"],results[\"Trainerror\"],'m^-')\n",
    "    plt.plot(results[\"Weightupdates\"],results[\"targeterror\"],'k^-')\n",
    "    plt.legend([\"Training error\",\"Target error\"])\n",
    "    \n",
    "def plot_prior_and_posterior(alpha,epsilon,sigma,TASK=2,Binary=True):\n",
    "    ### load in the prior data\n",
    "    sigma_tmp=sigma\n",
    "    sigma=sigma[0]*10**(-1*sigma[1])\n",
    "    checkpoint_path = \"priors/\"+\"task\"+str(TASK)+\"/\"+str(int(100*alpha))\n",
    "    if Binary:\n",
    "        checkpoint_path = \"priors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(100*alpha))\n",
    "    result_path=project_folder+'mnist_transfer/'+checkpoint_path+\"/results.pkl\"\n",
    "    results=pd.read_pickle(result_path)\n",
    "    result_path_post=\"results/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"+str(sigma_tmp[0])+str(sigma_tmp[1])\n",
    "    results2=pd.read_pickle(result_path_post+\"_results.pkl\")\n",
    "    \n",
    "    \n",
    "    ### remove/ignore the last entry of the prior data \n",
    "    ### as it should be a duplication of the first one from the posterior results\n",
    "    \n",
    "    ### training error\n",
    "    A=list(results2[\"Weightupdates\"]+list(results[\"Weightupdates\"])[-1])\n",
    "    B=list(results[\"Weightupdates\"])[:-1]\n",
    "    B.extend(A)\n",
    "    C=list(results[\"Trainerror\"])[:-1]\n",
    "    C.extend(list(results2[\"train_germain\"]))\n",
    "    plt.plot(B,C,'-m^')\n",
    "    \n",
    "    ## target error\n",
    "    D=list(results[\"targeterror\"])[:-1]\n",
    "    D.extend(list(results2[\"target_germain\"]))\n",
    "    plt.plot(B,D,'-k*')\n",
    "    \n",
    "    ### bound\n",
    "    E=results2[\"germain_bound\"]\n",
    "    plt.plot(A,E,'-D')\n",
    "    F=results2['boundpart3_germain']\n",
    "    plt.plot(A,F,'-o')\n",
    "    print(results2[\"target_germain\"])\n",
    "    print(results2[\"germain_bound\"])\n",
    "    ### lines for uninformative region and worse than random guessing; also for end of prior training\n",
    "    plt.axvline(A[0],color=\"grey\")\n",
    "    plt.axhline(y=0.5, color=\"black\", linestyle=\"--\")\n",
    "    plt.axhline(y=1, color=\"red\", linestyle=\"--\")\n",
    "    plt.legend([\"Training error\",\"Target error\",\"Bound\",\"KL-part\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean, variance 0.36348352 70.18035\n",
      "---------------Load MNIST----------------\n",
      "Training set (60000, 32, 32, 3) (60000, 10)\n",
      "Test set (10000, 32, 32, 3) (10000, 10)\n",
      "\n",
      "\n",
      "mean, variance 1.1809415 74.36859\n",
      "---------------Load MNIST-M----------------\n",
      "Training set (60000, 32, 32, 3) (60000, 10)\n",
      "Test set (10000, 32, 32, 3) (10000, 10)\n",
      "[[10.986111, 5.0, 2.999428, 1.99958, 1.399789, 1.0, 0.71435696, 0.5, 0.33346358, 0.20003448], [0.09088036, 0.19987813, 0.33326975, 0.5, 0.7143216, 1.0, 1.4001397, 2.0, 3.0, 5.0025883]]\n",
      "Alpha is:0.1\n",
      "547.0\n",
      "Train on 70010 samples, validate on 70010 samples\n",
      "70010/70010 [==============================] - 4s 56us/sample - loss: 0.1535 - accuracy: 0.9383 - val_loss: 0.1163 - val_accuracy: 0.9556\n",
      "----------------------------------------\n",
      "Finished training first posterior epoch\n",
      "Train on 70010 samples, validate on 70010 samples\n",
      "Epoch 1/2000\n",
      "  128/70010 [..............................] - ETA: 5s - loss: 0.1038 - accuracy: 0.9609\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      " 1280/70010 [..............................] - ETA: 4s - loss: 0.1199 - accuracy: 0.9547\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      " 2432/70010 [>.............................] - ETA: 3s - loss: 0.1249 - accuracy: 0.9511\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      " 3200/70010 [>.............................] - ETA: 3s - loss: 0.1243 - accuracy: 0.9509\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      " 3840/70010 [>.............................] - ETA: 4s - loss: 0.1206 - accuracy: 0.9523\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      " 4864/70010 [=>............................] - ETA: 4s - loss: 0.1181 - accuracy: 0.9550\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      " 5760/70010 [=>............................] - ETA: 4s - loss: 0.1202 - accuracy: 0.9533\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      " 7040/70010 [==>...........................] - ETA: 4s - loss: 0.1197 - accuracy: 0.9541\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      " 8320/70010 [==>...........................] - ETA: 4s - loss: 0.1201 - accuracy: 0.9536\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      " 9472/70010 [===>..........................] - ETA: 3s - loss: 0.1207 - accuracy: 0.9533\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "10240/70010 [===>..........................] - ETA: 3s - loss: 0.1225 - accuracy: 0.9527\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "11136/70010 [===>..........................] - ETA: 3s - loss: 0.1219 - accuracy: 0.9531\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "11520/70010 [===>..........................] - ETA: 3s - loss: 0.1210 - accuracy: 0.9534\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "12160/70010 [====>.........................] - ETA: 3s - loss: 0.1205 - accuracy: 0.9534\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "13312/70010 [====>.........................] - ETA: 3s - loss: 0.1203 - accuracy: 0.9537\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "14080/70010 [=====>........................] - ETA: 3s - loss: 0.1200 - accuracy: 0.9533\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "15232/70010 [=====>........................] - ETA: 3s - loss: 0.1196 - accuracy: 0.9534\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "16000/70010 [=====>........................] - ETA: 3s - loss: 0.1196 - accuracy: 0.9533\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "17152/70010 [======>.......................] - ETA: 3s - loss: 0.1196 - accuracy: 0.9539\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "17920/70010 [======>.......................] - ETA: 3s - loss: 0.1192 - accuracy: 0.9540\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "19200/70010 [=======>......................] - ETA: 3s - loss: 0.1194 - accuracy: 0.9540\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "20352/70010 [=======>......................] - ETA: 3s - loss: 0.1186 - accuracy: 0.9545\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "20864/70010 [=======>......................] - ETA: 3s - loss: 0.1185 - accuracy: 0.9546\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "21376/70010 [========>.....................] - ETA: 3s - loss: 0.1180 - accuracy: 0.9548\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "22400/70010 [========>.....................] - ETA: 3s - loss: 0.1171 - accuracy: 0.9552\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "23168/70010 [========>.....................] - ETA: 3s - loss: 0.1172 - accuracy: 0.9554\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "24320/70010 [=========>....................] - ETA: 3s - loss: 0.1174 - accuracy: 0.9554\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "25472/70010 [=========>....................] - ETA: 2s - loss: 0.1163 - accuracy: 0.9558\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "26240/70010 [==========>...................] - ETA: 2s - loss: 0.1169 - accuracy: 0.9557\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "26880/70010 [==========>...................] - ETA: 2s - loss: 0.1166 - accuracy: 0.9558\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "27520/70010 [==========>...................] - ETA: 2s - loss: 0.1159 - accuracy: 0.9560\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "28160/70010 [===========>..................] - ETA: 2s - loss: 0.1159 - accuracy: 0.9558\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "29312/70010 [===========>..................] - ETA: 2s - loss: 0.1156 - accuracy: 0.9556\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "30080/70010 [===========>..................] - ETA: 2s - loss: 0.1150 - accuracy: 0.9560\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "31104/70010 [============>.................] - ETA: 2s - loss: 0.1150 - accuracy: 0.9560\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "32000/70010 [============>.................] - ETA: 2s - loss: 0.1145 - accuracy: 0.9563\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "33152/70010 [=============>................] - ETA: 2s - loss: 0.1145 - accuracy: 0.9562\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "33920/70010 [=============>................] - ETA: 2s - loss: 0.1140 - accuracy: 0.9563\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "35072/70010 [==============>...............] - ETA: 2s - loss: 0.1131 - accuracy: 0.9566\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "35200/70010 [==============>...............] - ETA: 2s - loss: 0.1131 - accuracy: 0.9566\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35840/70010 [==============>...............] - ETA: 2s - loss: 0.1131 - accuracy: 0.9567\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "36992/70010 [==============>...............] - ETA: 2s - loss: 0.1127 - accuracy: 0.9568\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "37760/70010 [===============>..............] - ETA: 2s - loss: 0.1126 - accuracy: 0.9567\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "39040/70010 [===============>..............] - ETA: 2s - loss: 0.1130 - accuracy: 0.9565\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "40320/70010 [================>.............] - ETA: 1s - loss: 0.1127 - accuracy: 0.9564\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "41472/70010 [================>.............] - ETA: 1s - loss: 0.1121 - accuracy: 0.9567\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "42240/70010 [=================>............] - ETA: 1s - loss: 0.1127 - accuracy: 0.9565\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "42880/70010 [=================>............] - ETA: 1s - loss: 0.1125 - accuracy: 0.9565\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "43520/70010 [=================>............] - ETA: 1s - loss: 0.1126 - accuracy: 0.9564\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "44160/70010 [=================>............] - ETA: 1s - loss: 0.1125 - accuracy: 0.9564\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "45312/70010 [==================>...........] - ETA: 1s - loss: 0.1122 - accuracy: 0.9564\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "46080/70010 [==================>...........] - ETA: 1s - loss: 0.1118 - accuracy: 0.9566\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "47360/70010 [===================>..........] - ETA: 1s - loss: 0.1115 - accuracy: 0.9567\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "48512/70010 [===================>..........] - ETA: 1s - loss: 0.1110 - accuracy: 0.9569\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "49280/70010 [====================>.........] - ETA: 1s - loss: 0.1109 - accuracy: 0.9572\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "50432/70010 [====================>.........] - ETA: 1s - loss: 0.1106 - accuracy: 0.9572\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "50560/70010 [====================>.........] - ETA: 1s - loss: 0.1106 - accuracy: 0.9572\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "51200/70010 [====================>.........] - ETA: 1s - loss: 0.1105 - accuracy: 0.9571\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "51840/70010 [=====================>........] - ETA: 1s - loss: 0.1104 - accuracy: 0.9572\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "53120/70010 [=====================>........] - ETA: 1s - loss: 0.1101 - accuracy: 0.9573\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "54400/70010 [======================>.......] - ETA: 1s - loss: 0.1099 - accuracy: 0.9574\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "55168/70010 [======================>.......] - ETA: 1s - loss: 0.1097 - accuracy: 0.9574\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "56320/70010 [=======================>......] - ETA: 0s - loss: 0.1099 - accuracy: 0.9574\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "57472/70010 [=======================>......] - ETA: 0s - loss: 0.1094 - accuracy: 0.9577\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "58240/70010 [=======================>......] - ETA: 0s - loss: 0.1090 - accuracy: 0.9578\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "59264/70010 [========================>.....] - ETA: 0s - loss: 0.1090 - accuracy: 0.9578\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "60160/70010 [========================>.....] - ETA: 0s - loss: 0.1090 - accuracy: 0.9578\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "61440/70010 [=========================>....] - ETA: 0s - loss: 0.1086 - accuracy: 0.9579\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "62592/70010 [=========================>....] - ETA: 0s - loss: 0.1083 - accuracy: 0.9581\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "63360/70010 [==========================>...] - ETA: 0s - loss: 0.1081 - accuracy: 0.9582\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "64512/70010 [==========================>...] - ETA: 0s - loss: 0.1079 - accuracy: 0.9581\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "65280/70010 [==========================>...] - ETA: 0s - loss: 0.1080 - accuracy: 0.9581\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "65920/70010 [===========================>..] - ETA: 0s - loss: 0.1080 - accuracy: 0.9581\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "67072/70010 [===========================>..] - ETA: 0s - loss: 0.1081 - accuracy: 0.9581\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "67840/70010 [============================>.] - ETA: 0s - loss: 0.1080 - accuracy: 0.9582\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "68992/70010 [============================>.] - ETA: 0s - loss: 0.1076 - accuracy: 0.9584\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_10/2_1.ckpt\n",
      "70010/70010 [==============================] - 6s 85us/sample - loss: 0.1075 - accuracy: 0.9585 - val_loss: 0.0909 - val_accuracy: 0.9642\n",
      "Epoch 2/2000\n",
      "  128/70010 [..............................] - ETA: 2s - loss: 0.1429 - accuracy: 0.9141\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      " 1024/70010 [..............................] - ETA: 5s - loss: 0.1039 - accuracy: 0.9502\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      " 2304/70010 [..............................] - ETA: 5s - loss: 0.1024 - accuracy: 0.9575\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      " 3456/70010 [>.............................] - ETA: 4s - loss: 0.0989 - accuracy: 0.9580\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      " 4224/70010 [>.............................] - ETA: 4s - loss: 0.0954 - accuracy: 0.9600\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      " 5376/70010 [=>............................] - ETA: 4s - loss: 0.0911 - accuracy: 0.9630\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 6144/70010 [=>............................] - ETA: 4s - loss: 0.0906 - accuracy: 0.9639\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      " 6784/70010 [=>............................] - ETA: 4s - loss: 0.0905 - accuracy: 0.9649\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      " 7936/70010 [==>...........................] - ETA: 4s - loss: 0.0878 - accuracy: 0.9659\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      " 8320/70010 [==>...........................] - ETA: 4s - loss: 0.0874 - accuracy: 0.9657\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      " 9344/70010 [===>..........................] - ETA: 4s - loss: 0.0873 - accuracy: 0.9653\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "10496/70010 [===>..........................] - ETA: 3s - loss: 0.0885 - accuracy: 0.9649\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "11264/70010 [===>..........................] - ETA: 3s - loss: 0.0866 - accuracy: 0.9658\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "12416/70010 [====>.........................] - ETA: 3s - loss: 0.0874 - accuracy: 0.9662\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "13184/70010 [====>.........................] - ETA: 3s - loss: 0.0880 - accuracy: 0.9663\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "13824/70010 [====>.........................] - ETA: 3s - loss: 0.0874 - accuracy: 0.9667\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "14464/70010 [=====>........................] - ETA: 3s - loss: 0.0875 - accuracy: 0.9666\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "15744/70010 [=====>........................] - ETA: 3s - loss: 0.0884 - accuracy: 0.9663\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "16896/70010 [======>.......................] - ETA: 3s - loss: 0.0897 - accuracy: 0.9661\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "17664/70010 [======>.......................] - ETA: 3s - loss: 0.0898 - accuracy: 0.9664\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "18816/70010 [=======>......................] - ETA: 3s - loss: 0.0899 - accuracy: 0.9663\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "19584/70010 [=======>......................] - ETA: 3s - loss: 0.0894 - accuracy: 0.9666\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "20736/70010 [=======>......................] - ETA: 3s - loss: 0.0894 - accuracy: 0.9665\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "21504/70010 [========>.....................] - ETA: 3s - loss: 0.0886 - accuracy: 0.9668\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "22144/70010 [========>.....................] - ETA: 3s - loss: 0.0887 - accuracy: 0.9668\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "23296/70010 [========>.....................] - ETA: 3s - loss: 0.0892 - accuracy: 0.9666\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "24064/70010 [=========>....................] - ETA: 3s - loss: 0.0893 - accuracy: 0.9666\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "25216/70010 [=========>....................] - ETA: 2s - loss: 0.0892 - accuracy: 0.9668\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "25984/70010 [==========>...................] - ETA: 2s - loss: 0.0889 - accuracy: 0.9668\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "27136/70010 [==========>...................] - ETA: 2s - loss: 0.0888 - accuracy: 0.9668\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "27904/70010 [==========>...................] - ETA: 2s - loss: 0.0888 - accuracy: 0.9668\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "28928/70010 [===========>..................] - ETA: 2s - loss: 0.0885 - accuracy: 0.9667\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "29824/70010 [===========>..................] - ETA: 2s - loss: 0.0883 - accuracy: 0.9668\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "30848/70010 [============>.................] - ETA: 2s - loss: 0.0882 - accuracy: 0.9666\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "31488/70010 [============>.................] - ETA: 2s - loss: 0.0888 - accuracy: 0.9666\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "31744/70010 [============>.................] - ETA: 2s - loss: 0.0885 - accuracy: 0.9667\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "32896/70010 [=============>................] - ETA: 2s - loss: 0.0888 - accuracy: 0.9667\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "33152/70010 [=============>................] - ETA: 2s - loss: 0.0889 - accuracy: 0.9667\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "34304/70010 [=============>................] - ETA: 2s - loss: 0.0884 - accuracy: 0.9671\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "35584/70010 [==============>...............] - ETA: 2s - loss: 0.0878 - accuracy: 0.9673\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "36224/70010 [==============>...............] - ETA: 2s - loss: 0.0880 - accuracy: 0.9672\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "37376/70010 [===============>..............] - ETA: 2s - loss: 0.0879 - accuracy: 0.9671\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "38144/70010 [===============>..............] - ETA: 2s - loss: 0.0877 - accuracy: 0.9673\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "39296/70010 [===============>..............] - ETA: 2s - loss: 0.0878 - accuracy: 0.9673\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "39424/70010 [===============>..............] - ETA: 2s - loss: 0.0878 - accuracy: 0.9673\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "40576/70010 [================>.............] - ETA: 2s - loss: 0.0875 - accuracy: 0.9674\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "41344/70010 [================>.............] - ETA: 2s - loss: 0.0871 - accuracy: 0.9676\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "42368/70010 [=================>............] - ETA: 1s - loss: 0.0875 - accuracy: 0.9675\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "42624/70010 [=================>............] - ETA: 1s - loss: 0.0872 - accuracy: 0.9676\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "43776/70010 [=================>............] - ETA: 1s - loss: 0.0874 - accuracy: 0.9675\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "44544/70010 [==================>...........] - ETA: 1s - loss: 0.0871 - accuracy: 0.9675\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45184/70010 [==================>...........] - ETA: 1s - loss: 0.0871 - accuracy: 0.9675\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "46336/70010 [==================>...........] - ETA: 1s - loss: 0.0867 - accuracy: 0.9677\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "47104/70010 [===================>..........] - ETA: 1s - loss: 0.0866 - accuracy: 0.9678\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "48256/70010 [===================>..........] - ETA: 1s - loss: 0.0871 - accuracy: 0.9676\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "49024/70010 [====================>.........] - ETA: 1s - loss: 0.0872 - accuracy: 0.9676\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "50176/70010 [====================>.........] - ETA: 1s - loss: 0.0874 - accuracy: 0.9675\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "50304/70010 [====================>.........] - ETA: 1s - loss: 0.0874 - accuracy: 0.9675\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "51328/70010 [====================>.........] - ETA: 1s - loss: 0.0875 - accuracy: 0.9675\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "52224/70010 [=====================>........] - ETA: 1s - loss: 0.0873 - accuracy: 0.9675\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "53376/70010 [=====================>........] - ETA: 1s - loss: 0.0874 - accuracy: 0.9675\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "54144/70010 [======================>.......] - ETA: 1s - loss: 0.0873 - accuracy: 0.9675\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "55296/70010 [======================>.......] - ETA: 1s - loss: 0.0875 - accuracy: 0.9674\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "56064/70010 [=======================>......] - ETA: 0s - loss: 0.0873 - accuracy: 0.9676\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "57216/70010 [=======================>......] - ETA: 0s - loss: 0.0875 - accuracy: 0.9674\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "57984/70010 [=======================>......] - ETA: 0s - loss: 0.0875 - accuracy: 0.9675\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "58624/70010 [========================>.....] - ETA: 0s - loss: 0.0874 - accuracy: 0.9675\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "59264/70010 [========================>.....] - ETA: 0s - loss: 0.0872 - accuracy: 0.9676\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "60416/70010 [========================>.....] - ETA: 0s - loss: 0.0869 - accuracy: 0.9677\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "61184/70010 [=========================>....] - ETA: 0s - loss: 0.0867 - accuracy: 0.9678\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "62464/70010 [=========================>....] - ETA: 0s - loss: 0.0867 - accuracy: 0.9678\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "63744/70010 [==========================>...] - ETA: 0s - loss: 0.0865 - accuracy: 0.9679\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "65024/70010 [==========================>...] - ETA: 0s - loss: 0.0865 - accuracy: 0.9679\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "66176/70010 [===========================>..] - ETA: 0s - loss: 0.0866 - accuracy: 0.9678\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "66304/70010 [===========================>..] - ETA: 0s - loss: 0.0866 - accuracy: 0.9678\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "67456/70010 [===========================>..] - ETA: 0s - loss: 0.0866 - accuracy: 0.9677\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "68224/70010 [============================>.] - ETA: 0s - loss: 0.0866 - accuracy: 0.9678\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "69376/70010 [============================>.] - ETA: 0s - loss: 0.0864 - accuracy: 0.9679\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_10/2_2.ckpt\n",
      "70010/70010 [==============================] - 6s 89us/sample - loss: 0.0862 - accuracy: 0.9679 - val_loss: 0.0727 - val_accuracy: 0.9740\n",
      "Epoch 3/2000\n",
      "\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      "  128/70010 [..............................] - ETA: 12s - loss: 0.0673 - accuracy: 0.9688\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      " 1152/70010 [..............................] - ETA: 4s - loss: 0.0683 - accuracy: 0.9766 \n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      "\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      " 2048/70010 [..............................] - ETA: 4s - loss: 0.0655 - accuracy: 0.9766\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      " 2688/70010 [>.............................] - ETA: 4s - loss: 0.0700 - accuracy: 0.9766\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      " 3328/70010 [>.............................] - ETA: 4s - loss: 0.0724 - accuracy: 0.9754\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      " 3968/70010 [>.............................] - ETA: 5s - loss: 0.0688 - accuracy: 0.9761\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      "\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      " 5248/70010 [=>............................] - ETA: 5s - loss: 0.0690 - accuracy: 0.9756\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      " 6400/70010 [=>............................] - ETA: 4s - loss: 0.0700 - accuracy: 0.9756\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      " 6528/70010 [=>............................] - ETA: 5s - loss: 0.0708 - accuracy: 0.9753\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      "\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      " 7808/70010 [==>...........................] - ETA: 5s - loss: 0.0710 - accuracy: 0.9743\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      " 8960/70010 [==>...........................] - ETA: 4s - loss: 0.0730 - accuracy: 0.9738\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      "\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      " 9728/70010 [===>..........................] - ETA: 4s - loss: 0.0716 - accuracy: 0.9743\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      "10368/70010 [===>..........................] - ETA: 4s - loss: 0.0720 - accuracy: 0.9735\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      "11520/70010 [===>..........................] - ETA: 4s - loss: 0.0723 - accuracy: 0.9729\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      "11648/70010 [===>..........................] - ETA: 4s - loss: 0.0721 - accuracy: 0.9730\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      "12288/70010 [====>.........................] - ETA: 4s - loss: 0.0721 - accuracy: 0.9727\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      "13440/70010 [====>.........................] - ETA: 4s - loss: 0.0736 - accuracy: 0.9717\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13952/70010 [====>.........................] - ETA: 4s - loss: 0.0736 - accuracy: 0.9716\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      "14720/70010 [=====>........................] - ETA: 4s - loss: 0.0741 - accuracy: 0.9717\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      "15104/70010 [=====>........................] - ETA: 4s - loss: 0.0737 - accuracy: 0.9721\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      "15872/70010 [=====>........................] - ETA: 4s - loss: 0.0738 - accuracy: 0.9722\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      "\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      "16768/70010 [======>.......................] - ETA: 4s - loss: 0.0729 - accuracy: 0.9726\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      "\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      "18048/70010 [======>.......................] - ETA: 3s - loss: 0.0722 - accuracy: 0.9731\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      "\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      "19328/70010 [=======>......................] - ETA: 3s - loss: 0.0719 - accuracy: 0.9731\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      "19968/70010 [=======>......................] - ETA: 4s - loss: 0.0715 - accuracy: 0.9733\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      "20608/70010 [=======>......................] - ETA: 4s - loss: 0.0712 - accuracy: 0.9734\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      "21248/70010 [========>.....................] - ETA: 4s - loss: 0.0715 - accuracy: 0.9733\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_10/2_3.ckpt\n",
      "\n",
      " !!! training error threshold reached, no further training !!!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_FallbackException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/apps/Alvis/software/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_io_ops.py\u001b[0m in \u001b[0;36mmerge_v2_checkpoints\u001b[0;34m(checkpoint_prefixes, destination_prefix, delete_old_dirs, name)\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_prefixes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestination_prefix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m         \"delete_old_dirs\", delete_old_dirs)\n\u001b[0m\u001b[1;32m    494\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31m_FallbackException\u001b[0m: This function does not handle the case of the path where all inputs are not already EagerTensors.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-cf47c26da6b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepsilons\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mw_s\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_posterior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_source\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_source_bin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_source\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_source_bin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mBinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0;31m#for sigma in sigmas:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0;31m#res=read_and_prepare_results(alpha,x_source,y_source_bin,x_target,y_target_bin,sigma,delta,len(x_source),epsilon,Binary=True)#x_bound,y_bound,x_target,y_target_bin,sigma,delta,len(x_bound),epsilon,Binary=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-b456054f4bd1>\u001b[0m in \u001b[0;36mtrain_posterior\u001b[0;34m(alpha, x_train, y_train, prior_weights, x_test, y_test, save, epsilon, Task, Binary, batch_size)\u001b[0m\n\u001b[1;32m    217\u001b[0m            \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCALLBACK\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m            \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m            \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m                         )\n\u001b[1;32m    221\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/Alvis/software/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/apps/Alvis/software/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/Alvis/software/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    179\u001b[0m             batch_end=step * batch_size + current_batch_size)\n\u001b[1;32m    180\u001b[0m       \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m       \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/Alvis/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/Alvis/software/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mon_batch\u001b[0;34m(self, step, mode, size)\u001b[0m\n\u001b[1;32m    786\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data_exhausted'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m           self.callbacks._call_batch_hook(\n\u001b[0;32m--> 788\u001b[0;31m               mode, 'end', step, batch_logs)\n\u001b[0m\u001b[1;32m    789\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/Alvis/software/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m       \u001b[0mbatch_hook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m       \u001b[0mbatch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/Alvis/software/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    526\u001b[0m     \"\"\"\n\u001b[1;32m    527\u001b[0m     \u001b[0;31m# For backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mdoc_controls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_subclass_implementers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/Alvis/software/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    975\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_samples_seen_since_last_saving\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_samples_seen_since_last_saving\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_freq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_samples_seen_since_last_saving\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/Alvis/software/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_save_model\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m   1036\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nEpoch %05d: saving model to %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/Alvis/software/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36msave_weights\u001b[0;34m(self, filepath, overwrite, save_format)\u001b[0m\n\u001b[1;32m   1121\u001b[0m              'saved.\\n\\nConsider using a TensorFlow optimizer from `tf.train`.')\n\u001b[1;32m   1122\u001b[0m             % (optimizer,))\n\u001b[0;32m-> 1123\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trackable_saver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1124\u001b[0m       \u001b[0;31m# Record this checkpoint so it's visible from tf.train.latest_checkpoint.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m       checkpoint_management.update_checkpoint_state_internal(\n",
      "\u001b[0;32m/apps/Alvis/software/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/util.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, file_prefix, checkpoint_number, session)\u001b[0m\n\u001b[1;32m   1166\u001b[0m     \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecursive_create_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m     save_path, new_feed_additions = self._save_cached_when_graph_building(\n\u001b[0;32m-> 1168\u001b[0;31m         file_prefix=file_prefix_tensor, object_graph_tensor=object_graph_tensor)\n\u001b[0m\u001b[1;32m   1169\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnew_feed_additions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m       \u001b[0mfeed_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_feed_additions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/Alvis/software/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/util.py\u001b[0m in \u001b[0;36m_save_cached_when_graph_building\u001b[0;34m(self, file_prefix, object_graph_tensor)\u001b[0m\n\u001b[1;32m   1114\u001b[0m         or context.executing_eagerly() or ops.inside_function()):\n\u001b[1;32m   1115\u001b[0m       \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctional_saver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultiDeviceSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnamed_saveable_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m       \u001b[0msave_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1117\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/cpu:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msave_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/Alvis/software/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/training/saving/functional_saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, file_prefix)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;31m# attempts to delete the temporary directory, \"<user-fed prefix>_temp\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         return gen_io_ops.merge_v2_checkpoints(\n\u001b[0;32m--> 238\u001b[0;31m             sharded_prefixes, file_prefix, delete_old_dirs=True)\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/Alvis/software/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_io_ops.py\u001b[0m in \u001b[0;36mmerge_v2_checkpoints\u001b[0;34m(checkpoint_prefixes, destination_prefix, delete_old_dirs, name)\u001b[0m\n\u001b[1;32m    497\u001b[0m         return merge_v2_checkpoints_eager_fallback(\n\u001b[1;32m    498\u001b[0m             \u001b[0mcheckpoint_prefixes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestination_prefix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m             delete_old_dirs=delete_old_dirs, name=name, ctx=_ctx)\n\u001b[0m\u001b[1;32m    500\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/Alvis/software/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_io_ops.py\u001b[0m in \u001b[0;36mmerge_v2_checkpoints_eager_fallback\u001b[0;34m(checkpoint_prefixes, destination_prefix, delete_old_dirs, name, ctx)\u001b[0m\n\u001b[1;32m    523\u001b[0m   \u001b[0m_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"delete_old_dirs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelete_old_dirs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m   _result = _execute.execute(b\"MergeV2Checkpoints\", 0, inputs=_inputs_flat,\n\u001b[0;32m--> 525\u001b[0;31m                              attrs=_attrs, ctx=ctx, name=name)\n\u001b[0m\u001b[1;32m    526\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/Alvis/software/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/Alvis/software/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mTFE_Py_Execute\u001b[0;34m(ctx, device_name, op_name, inputs, attrs, outputs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mTFE_Py_Execute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pywrap_tensorflow_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFE_Py_Execute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mTFE_Py_ExecuteCancelable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "alphas=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "tasks=[2]\n",
    "for t in tasks:\n",
    "    x_source,y_source,x_target,y_target=load_task(t)\n",
    "\n",
    "    y_source_bin=np.array(make_mnist_binary(y_source))\n",
    "    y_target_bin=np.array(make_mnist_binary(y_target))\n",
    "    for alpha in alphas:\n",
    "        print(\"Alpha is:\"+str(alpha))\n",
    "        x_bound, x_prior, y_bound , y_prior = train_test_split(x_source,y_source_bin,test_size=alpha,random_state=69105)\n",
    "        #w_a=train_prior(alpha,1,x_source,y_source_bin,x_target=x_target,y_target=y_target_bin,save=True,Task=t,Binary=True,batch_size=128)\n",
    "\n",
    "        for epsilon in epsilons:\n",
    "            w_s=train_posterior(alpha,x_source,y_source_bin,None,x_test=x_source,y_test=y_source_bin,epsilon=epsilon,Task=t,Binary=True,batch_size=128)\n",
    "            #for sigma in sigmas:\n",
    "                #res=read_and_prepare_results(alpha,x_source,y_source_bin,x_target,y_target_bin,sigma,delta,len(x_source),epsilon,Binary=True)#x_bound,y_bound,x_target,y_target_bin,sigma,delta,len(x_bound),epsilon,Binary=True)\n",
    "                #plot_result_file(epsilon,alpha,sigma,TASK,Binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
