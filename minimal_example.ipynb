{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function \n",
    "\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import gc, re, copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.keras.layers import deserialize, serialize\n",
    "from tensorflow.python.keras.saving import saving_utils\n",
    "\n",
    "# Project imports \n",
    "from data import mnist,mnist_m\n",
    "from data.tasks import *\n",
    "from data.label_shift import label_shift_linear, plot_labeldist, plot_splitbars\n",
    "from experiments.training import *\n",
    "from experiments.SL_bound import *\n",
    "from util.kl import *\n",
    "from util.misc import *\n",
    "from results.plotting import *\n",
    "\n",
    "# Hyper-parameters\n",
    "#batch_size = 128\n",
    "num_classes = 10\n",
    "make_plots = False\n",
    "\n",
    "delta=0.05 ## what would this be?   \n",
    "\n",
    "alphas=[0]\n",
    "#length=10\n",
    "#for i in range(length-1):\n",
    "#    alphas.append((i+1)/length)\n",
    "\n",
    "\n",
    "\n",
    "sigmas=[]\n",
    "for i in range(2,9):  \n",
    "    sigmas.append([3,i])\n",
    "    if(i==8):\n",
    "        break\n",
    "    sigmas.append([1,i])\n",
    "\n",
    "img_rows, img_cols = 32, 32\n",
    "project_folder = \"/cephyr/users/adambre/Alvis/\"\n",
    "\n",
    "\n",
    "### TODO\n",
    "\n",
    "# # check output_dir, create it if not exists\n",
    "#     if not os.path.isdir(output_dir):\n",
    "#         os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/cephyr/NOBACKUP/groups/snic2021-23-538/chestXray14/images/images'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-7dba81791187>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m \u001b[0mcrop_and_resize_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/images\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/resized/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-7dba81791187>\u001b[0m in \u001b[0;36mcrop_and_resize_images\u001b[0;34m(path, new_path, img_size)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \"\"\"\n\u001b[1;32m     31\u001b[0m     \u001b[0mcreate_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mdirs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'.DS_Store'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0;31m# total = 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/cephyr/NOBACKUP/groups/snic2021-23-538/chestXray14/images/images'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# data_path=\"/cephyr/NOBACKUP/groups/snic2021-23-538/chestXray14\"\n",
    "\n",
    "# def create_directory(directory):\n",
    "#     \"\"\"\n",
    "#     Creates a new folder in the specified directory if folder doesn't exist.\n",
    "#     INPUT\n",
    "#         directory: Folder to be created, called as \"folder/\".\n",
    "#     OUTPUT\n",
    "#         New folder in the current directory.\n",
    "#     \"\"\"\n",
    "#     if not os.path.exists(directory):\n",
    "#         os.makedirs(directory)\n",
    "\n",
    "\n",
    "# def crop_and_resize_images(path, new_path, img_size):\n",
    "#     \"\"\"\n",
    "#     Crops, resizes, and stores all images from a directory in a new directory.\n",
    "#     INPUT\n",
    "#         path: Path where the current, unscaled images are contained.\n",
    "#         new_path: Path to save the resized images.\n",
    "#         img_size: New size for the rescaled images.\n",
    "#     OUTPUT\n",
    "#         All images cropped, resized, and saved to the new folder.\n",
    "#     \"\"\"\n",
    "#     create_directory(new_path)\n",
    "#     dirs = [l for l in os.listdir(path) if l != '.DS_Store']\n",
    "#     # total = 0\n",
    "\n",
    "#     for item in dirs:\n",
    "#         print(item)\n",
    "#         # Read in all images as grayscale\n",
    "#         img = cv2.imread(path + item, cv2.IMREAD_GRAYSCALE)\n",
    "#         cv2.imshow(\"ok\",img)\n",
    "#         sys.exit(-1)\n",
    "#         img = cv2.resize(img, (img_size, img_size))\n",
    "#         cv2.imwrite(str(new_path + item), img)\n",
    "#         # total += 1\n",
    "#         # print(\"Saving: \", item, total)\n",
    "\n",
    "# def convert_images_to_arrays(file_path, df):\n",
    "#     \"\"\"\n",
    "#     Converts each image to an array, and appends each array to a new NumPy\n",
    "#     array, based on the image column equaling the image file name.\n",
    "\n",
    "#     INPUT\n",
    "#         file_path: Specified file path for resized test and train images.\n",
    "#         df: Pandas DataFrame being used to assist file imports.\n",
    "\n",
    "#     OUTPUT\n",
    "#         NumPy array of image arrays.\n",
    "#     \"\"\"\n",
    "\n",
    "#     lst_imgs = [l for l in df['Image_Index']]\n",
    "\n",
    "#     return np.array([np.array(cv2.imread(file_path + img, cv2.IMREAD_GRAYSCALE)) for img in lst_imgs])\n",
    "\n",
    "\n",
    "# def save_to_array(arr_name, arr_object):\n",
    "#     \"\"\"\n",
    "#     Saves data object as a NumPy file. Used for saving train and test arrays.\n",
    "\n",
    "#     INPUT\n",
    "#         arr_name: The name of the file you want to save.\n",
    "#             This input takes a directory string.\n",
    "#         arr_object: NumPy array of arrays. This object is saved as a NumPy file.\n",
    "#     \"\"\"\n",
    "#     return np.save(arr_name, arr_object)\n",
    "\n",
    "\n",
    "# run=False\n",
    "# crop_and_resize_images(path=data_path+\"/images/\", new_path=data_path+\"/resized/\", img_size=128)\n",
    "# if run:\n",
    "    \n",
    "    \n",
    "    \n",
    "#     print(\"Writing Train Array\")\n",
    "#     X_train = convert_images_to_arrays(data_path, labels)\n",
    "\n",
    "#     print(X_train.shape)\n",
    "\n",
    "#     print(\"Saving Train Array\")\n",
    "#     save_to_array('../X_sample.npy', X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean, variance 0.36348352 70.18035\n",
      "---------------Load MNIST----------------\n",
      "Training set (60000, 32, 32, 3) (60000, 10)\n",
      "Test set (10000, 32, 32, 3) (10000, 10)\n",
      "\n",
      "\n",
      "mean, variance 1.1809415 74.36859\n",
      "---------------Load MNIST-M----------------\n",
      "Training set (60000, 32, 32, 3) (60000, 10)\n",
      "Test set (10000, 32, 32, 3) (10000, 10)\n",
      "[[10.986111, 5.0, 2.999428, 1.99958, 1.399789, 1.0, 0.71435696, 0.5, 0.33346358, 0.20003448], [0.09088036, 0.19987813, 0.33326975, 0.5, 0.7143216, 1.0, 1.4001397, 2.0, 3.0, 5.0025883]]\n"
     ]
    }
   ],
   "source": [
    "TASK=2\n",
    "x_source,y_source,x_target,y_target=load_task(TASK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_tf():\n",
    "    ### making sure that we have the GPU to work on\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    #logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    if gpus:\n",
    "      # Restrict TensorFlow to only allocate 4GB of memory on the first GPU\n",
    "      # I do not know why I have to do this but gpu does not work otherwise.\n",
    "        try:\n",
    "            tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)])\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        except RuntimeError as e:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "            print(e)\n",
    "\n",
    "    \n",
    "## custom callback to terminate training at some specific value of a metric\n",
    "    \n",
    "class stop_callback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, monitor='accuracy', value=0.001, verbose=0):\n",
    "        super(tf.keras.callbacks.Callback, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.value = value\n",
    "        self.verbose = verbose\n",
    "    def on_epoch_end(self, epoch, logs={}): \n",
    "        if(logs.get('accuracy')> self.value): # select the accuracy\n",
    "            print(\"\\n !!! training error threshold reached, no further training !!!\")\n",
    "            self.model.stop_training = True\n",
    "            \n",
    "class fast_checkpoints(tf.keras.callbacks.Callback):\n",
    "    def __init__(self,checkpoint_path,save_freq):\n",
    "        super(tf.keras.callbacks.Callback, self).__init__()\n",
    "        self.save_freq=save_freq\n",
    "        self.filepath=checkpoint_path\n",
    "        self.verbose=1\n",
    "        self.save_best_only=False\n",
    "        self.save_weights_only=True\n",
    "    def on_train_batch_begin(self, batch, epoch, logs=None):\n",
    "         if batch%self.save_freq==0:\n",
    "\n",
    "            #print(\"\\n Saved weights at the start of batch\"+str(batch)+\"\\n\")\n",
    "\n",
    "            ## Create folder\n",
    "            weight_path = self.filepath+\"/1_\"+str(batch)+\".ckpt\"\n",
    "            os.makedirs(os.path.dirname(weight_path), exist_ok=True)\n",
    "            \n",
    "            self.model.save_weights(weight_path)\n",
    "            \n",
    "def train_posterior(alpha,x_train,y_train,prior_weights=None,x_test=[],y_test=[],save=True,epsilon=0.01,Task=2,Binary=False,batch_size=128):\n",
    "        \n",
    "        TASK=Task\n",
    "        \n",
    "        ### x_test should be the whole of S for early stopping purposes\n",
    "    \n",
    "        checkpoint_path = \"posteriors/\"+\"task\"+str(TASK)+\"/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))\n",
    "        if Binary:\n",
    "            checkpoint_path = \"posteriors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))\n",
    "        \n",
    "        \n",
    "        # Create a callback that saves the model's weights every epoch\n",
    "        checkpoint_freq=np.ceil(len(x_train)/batch_size)\n",
    "        #print(checkpoint_freq)\n",
    "        cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "            save_freq=\"epoch\",#int(checkpoint_freq),### 547 = ceiling(70000/128) i.e training set for MNIST/MNIST-M,\n",
    "            filepath=checkpoint_path+\"/2_{epoch:0d}.ckpt\", \n",
    "            verbose=1,\n",
    "            save_best_only=False,\n",
    "            save_weights_only=True,\n",
    "                ## tune when to save as needed for plots\n",
    "        )\n",
    "        ## callback for first part\n",
    "        fast_checkpoint_freq=np.ceil(len(x_train)/(batch_size*10))\n",
    "        fast_cp_callback =fast_checkpoints(checkpoint_path,int(fast_checkpoint_freq))\n",
    "        stopping_callback=stop_callback(monitor='val_acc',value=1-epsilon)\n",
    "    \n",
    "        M=init_task_model(TASK,Binary)\n",
    "\n",
    "        \n",
    "            \n",
    "        ## choose loss function, optimiser etc. and train\n",
    "        \n",
    "        M.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "               optimizer=tf.keras.optimizers.SGD(learning_rate=0.003, momentum=0.95),\n",
    "                      metrics=['accuracy'],)\n",
    "        \n",
    "        \n",
    "        ## Create the folder\n",
    "        os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
    "        ## remove previous weights\n",
    "        import glob\n",
    "        files=glob.glob(os.path.join(checkpoint_path+'/*'))\n",
    "        \n",
    "        \n",
    "        for file in files:\n",
    "            if file==(checkpoint_path+'/params.txt'):\n",
    "                files.remove(checkpoint_path+'/params.txt')\n",
    "            os.remove(file)\n",
    "        \n",
    "        ### load the prior weights\n",
    "        if prior_weights is not None:\n",
    "            M.set_weights(prior_weights)\n",
    "        elif(alpha==0):\n",
    "            ### save the rand. init as the prior\n",
    "            if Binary:\n",
    "                prior_path=\"priors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(100*alpha))+\"/prior.ckpt\"\n",
    "            else:\n",
    "                prior_path=\"priors/\"+\"task\"+str(TASK)+\"/\"+str(int(100*alpha))+\"/prior.ckpt\"\n",
    "            \n",
    "            ## Create the folder\n",
    "            os.makedirs(os.path.dirname(prior_path), exist_ok=True)\n",
    "\n",
    "            ## Save the weights\n",
    "            M.save_weights(prior_path)\n",
    "        else:\n",
    "            if Binary:\n",
    "                prior_path=\"priors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(100*alpha))+\"/prior.ckpt\"\n",
    "                M.load_weights(prior_path).expect_partial()\n",
    "            else:\n",
    "                \n",
    "                M.load_weights(prior_path).expect_partial()\n",
    "        \n",
    "    \n",
    "        if save:\n",
    "            CALLBACK=[fast_cp_callback,stopping_callback]\n",
    "        else:\n",
    "            CALLBACK=[stopping_callback]\n",
    "        ### train for one epoch with more checkpoints to be able to plot more there\n",
    "        fit_info = M.fit(x_train, y_train,\n",
    "           batch_size=batch_size,\n",
    "           epochs=1, \n",
    "           callbacks=CALLBACK,\n",
    "           validation_data=(x_test, y_test),\n",
    "           verbose=0,\n",
    "                        )\n",
    "        \n",
    "        print(\"-\"*40)\n",
    "        print(\"Finished training first posterior epoch\")\n",
    "        if save:\n",
    "            CALLBACK=[cp_callback,stopping_callback]\n",
    "        else:\n",
    "            CALLBACK=[stopping_callback]\n",
    "            \n",
    "        fit_info = M.fit(x_train, y_train,\n",
    "           batch_size=batch_size,\n",
    "           epochs=2000, # we should have done early stopping before this completes\n",
    "           callbacks=CALLBACK,\n",
    "           validation_data=(x_test, y_test),\n",
    "           verbose=0,\n",
    "                        )\n",
    "        print(\"-\"*40)\n",
    "        print(\"Finished training posterior\")\n",
    "        \n",
    "         #### save the last posterior weights to disk\n",
    "        epochs_trained=len(fit_info.history['loss'])\n",
    "        #if save:\n",
    "            ## Create the folder\n",
    "            #os.makedirs(os.path.dirname(checkpoint_path+\"/2_\"+str(epochs_trained)), exist_ok=True)\n",
    "            \n",
    "            \n",
    "            #M.save_weights(checkpoint_path+\"/2_\"+str(epochs_trained)) ###### check if we need this; TODO!!!!!!\n",
    "            \n",
    "        #### save textfile with parameters, i.e. alpha ,epochs trained and epsilon\n",
    "        if Binary:\n",
    "            with open('posteriors/'+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+'/params.txt', 'w') as f:\n",
    "                f.write('\\n'.join([str(alpha), str(epsilon), str(epochs_trained)]))     \n",
    "            f.close()\n",
    "        else:\n",
    "            with open('posteriors/'+\"task\"+str(TASK)+\"/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+'/params.txt', 'w') as f:\n",
    "                f.write('\\n'.join([str(alpha), str(epsilon), str(epochs_trained)]))     \n",
    "            f.close()\n",
    "        W=M.get_weights()\n",
    "        return W\n",
    "    \n",
    "def train_prior(alpha,total_epochs,x_train=[],y_train=[],x_target=[],y_target=[],save=True,Task=2,Binary=False,batch_size=128):\n",
    "    TASK=Task\n",
    "    \n",
    "    if Binary:\n",
    "        ## Create the folders\n",
    "        os.makedirs(os.path.dirname(\"priors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(100*alpha)))+\"/\", exist_ok=True)\n",
    "        checkpoint_path = \"priors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(100*alpha))\n",
    "    else:\n",
    "        os.makedirs(os.path.dirname(\"priors/\"+\"task\"+str(TASK)+\"/\"+str(int(100*alpha)))+\"/\", exist_ok=True)\n",
    "        checkpoint_path = \"priors/\"+\"task\"+str(TASK)+\"/\"+str(int(100*alpha))\n",
    "    ## remove previous weights\n",
    "    import glob\n",
    "    files=glob.glob(os.path.join(checkpoint_path+'/*'))\n",
    "    for file in files:\n",
    "        os.remove(file)\n",
    "    M=init_task_model(TASK,Binary)\n",
    "    #sys.exit(-1)\n",
    "    ### save checkpoints 10 times over the training of the prior\n",
    "    l=len(x_train)\n",
    "    checkpoint_freq=np.ceil(l/(batch_size*10))\n",
    "    fast_cp_callback =fast_checkpoints(checkpoint_path,int(checkpoint_freq))\n",
    "    if save:\n",
    "            CALLBACK=[fast_cp_callback]\n",
    "    else:\n",
    "            CALLBACK=[]\n",
    "            \n",
    "    ## choose loss function, optimiser etc. and train\n",
    "    M.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "               optimizer=tf.keras.optimizers.SGD(learning_rate=0.003, momentum=0.95),\n",
    "                      metrics=['accuracy'],)\n",
    "    fit_info = M.fit(x_train, y_train,\n",
    "           batch_size=batch_size,\n",
    "           callbacks=CALLBACK,\n",
    "           epochs=total_epochs,\n",
    "           verbose=0,\n",
    "                        )\n",
    "    print(\"-\"*40)\n",
    "    print(\"Finished training prior\")\n",
    "    #### save the final prior weights to disk\n",
    "    if save:\n",
    "        os.makedirs(checkpoint_path, exist_ok=True)\n",
    "        M.save_weights(checkpoint_path+\"/prior.ckpt\")\n",
    "     \n",
    "    \n",
    "    list1=[]\n",
    "    \n",
    "    dirFiles = os.listdir(checkpoint_path) #list of directory files\n",
    "    \n",
    "    ## remove the ckpt.index and sort so that we get the epochs that are in the directory\n",
    "    for files in dirFiles: #filter out all non weights\n",
    "        if '.ckpt.index' in files:\n",
    "            name = re.sub('\\.ckpt.index$', '', files)\n",
    "            if (name[0]==\"1\"):\n",
    "                list1.append(name)\n",
    "        \n",
    "    list1.sort(key=lambda f: int(re.sub('\\D', '', f)))\n",
    "    list1.append(\"prior\")    ## add the final weights which has no number\n",
    "    \n",
    "    Ws=list1\n",
    "    weight_updates=[]\n",
    "    for i in Ws:\n",
    "        if i[0]==\"1\":\n",
    "            if i[1]==\"_\":\n",
    "                weight_updates.append(int(i[2:]))\n",
    "    weight_updates.append(int(np.ceil(len(y_train)/batch_size)))\n",
    "    \n",
    "    error=[]\n",
    "    target_error=[]\n",
    "    for checkpoint in Ws:\n",
    "\n",
    "        model=init_task_model(TASK,Binary)\n",
    "        model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "               optimizer=tf.keras.optimizers.SGD(learning_rate=0.003, momentum=0.95),\n",
    "                  metrics=['accuracy'],)\n",
    "      \n",
    "            \n",
    "        model.load_weights(checkpoint_path+\"/\"+str(checkpoint)+\".ckpt\").expect_partial()\n",
    "        target_error.append(1-model.evaluate(x_target,y_target,verbose=0)[1])\n",
    "        error.append(1-model.evaluate(x_train,y_train,verbose=0)[1])\n",
    "    print(\"-\"*40)\n",
    "    print(\"Finished computing prior sample and target errors\")\n",
    "    if save:\n",
    "        results=pd.DataFrame({'Weightupdates': weight_updates,\n",
    "            'Trainerror': error,\n",
    "            'targeterror':target_error,\n",
    "            })\n",
    "       \n",
    "        ## Create the folders\n",
    "        os.makedirs(os.path.dirname(project_folder+\"mnist_transfer/results/\"+\"task\"+str(TASK)+\"/Binary/\"), exist_ok=True)\n",
    "        if Binary:\n",
    "            with open(project_folder+\"mnist_transfer/results/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(100*alpha))+\"_prior_results.pkl\",'wb') as f:\n",
    "                pickle.dump(results,f)\n",
    "            f.close()\n",
    "        else:\n",
    "            with open(project_folder+\"mnist_transfer/results/\"+\"task\"+str(TASK)+\"/\"+str(int(100*alpha))+\"_prior_results.pkl\",'wb') as f:\n",
    "                pickle.dump(results,f)\n",
    "            f.close()\n",
    "    return model.get_weights()\n",
    "    \n",
    "\n",
    "\n",
    "def plot_result_file(epsilon,alpha,sigma,Binary=False,Task=2):\n",
    "    sigma_tmp=sigma\n",
    "    sigma=sigma[0]*10**(-1*sigma[1])\n",
    "    \n",
    "    if Binary:\n",
    "        result_path=\"results/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"+str(sigma_tmp[0])+str(sigma_tmp[1])\n",
    "        plt.title(\"Binary: \"+r\"$\\alpha$=\"+str(alpha)+r\" $\\epsilon$=\"+str(epsilon)+r\" $\\sigma$=\"+str(sigma))\n",
    "    else:\n",
    "        result_path=\"results/\"+\"task\"+str(TASK)+\"/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"+str(sigma_tmp[0])+str(sigma_tmp[1])\n",
    "        plt.title(r\"$\\alpha$=\"+str(alpha)+r\" $\\epsilon$=\"+str(epsilon)+r\" $\\sigma$=\"+str(sigma))\n",
    "    results=pd.read_pickle(result_path+\"_results.pkl\")\n",
    "    \n",
    "    ### do the plots\n",
    "    plt.plot(results[\"Weightupdates\"],results[\"Bound\"],'r*-')\n",
    "    plt.plot(results[\"Weightupdates\"],results[\"Trainerror\"],'m^-')\n",
    "    \n",
    "    plt.xlabel(\"Weight updates\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    \n",
    "    plt.legend([\"Bound\",\"Empirical error\"])\n",
    "    plt.show()\n",
    "\n",
    "def find_optimal_sigma(sigmas,epsilon, alpha,Binary=False,Task=2):\n",
    "    #### to find the optimal sigma just do a search through all the results \n",
    "    #### and save the one for each parameter which has the minimal bound\n",
    "    #### Do we do this per epoch or for some other value? The sigma which yields the lowest bound overall for some epoch?\n",
    "    optimal=[0,1]\n",
    "    # search through all epochs and pick the sigma which yields the smallest bound during the whole training process\n",
    "    for sigma in sigmas:\n",
    "        sigma_tmp=sigma\n",
    "        sigma=sigma[0]*10**(-1*sigma[1])\n",
    "        if Binary:\n",
    "            result_path=\"results/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"+str(sigma_tmp[0])+str(sigma_tmp[1])\n",
    "        else:\n",
    "            result_path=\"results/\"+\"task\"+str(TASK)+\"/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"+str(sigma_tmp[0])+str(sigma_tmp[1])\n",
    "        results=pd.read_pickle(result_path+\"_results.pkl\")\n",
    "        MIN=np.min(results[\"Bound\"])\n",
    "        if (MIN<optimal[1]):\n",
    "            optimal[1]=MIN\n",
    "            optimal[0]=sigma\n",
    "    print(\"The optimal sigma is {} with bound value {}\".format(optimal[0],optimal[1]))\n",
    "\n",
    "   \n",
    "#### find the optimal sigma for every combination of parameters\n",
    "\n",
    "#### use the optimal sigmas to calculate the bound 50 times(with different data orders and initialisation)\n",
    "#### (Note: also delta=13*delta_0) for every combination \n",
    "# def read_prior(alpha,TASK=2,Binary=True):\n",
    "#     checkpoint_path = \"priors/\"+\"task\"+str(TASK)+\"/\"+str(int(100*alpha))\n",
    "#     if Binary:\n",
    "#         checkpoint_path = \"priors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(100*alpha))\n",
    "#     result_path=project_folder+'mnist_transfer/'+checkpoint_path+\"/results.pkl\"\n",
    "#     results=pd.read_pickle(result_path)\n",
    "#     plt.title(r\"$\\alpha$=\"+str(alpha))\n",
    "#     plt.plot(results[\"Weightupdates\"],results[\"Trainerror\"],'m^-')\n",
    "#     plt.plot(results[\"Weightupdates\"],results[\"targeterror\"],'k^-')\n",
    "#     plt.legend([\"Training error\",\"Target error\"])\n",
    "    \n",
    "# def plot_prior_and_posterior(alpha,epsilon,sigma,TASK=2,Binary=True):\n",
    "#     ### load in the prior data\n",
    "#     sigma_tmp=sigma\n",
    "#     sigma=sigma[0]*10**(-1*sigma[1])\n",
    "#     checkpoint_path = \"priors/\"+\"task\"+str(TASK)+\"/\"+str(int(100*alpha))\n",
    "#     if Binary:\n",
    "#         checkpoint_path = \"priors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(100*alpha))\n",
    "#     result_path=project_folder+'mnist_transfer/'+checkpoint_path+\"/results.pkl\"\n",
    "#     results=pd.read_pickle(result_path)\n",
    "#     result_path_post=\"results/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"+str(sigma_tmp[0])+str(sigma_tmp[1])\n",
    "#     results2=pd.read_pickle(result_path_post+\"_results.pkl\")\n",
    "    \n",
    "    \n",
    "#     ### remove/ignore the last entry of the prior data \n",
    "#     ### as it should be a duplication of the first one from the posterior results\n",
    "    \n",
    "#     ### training error\n",
    "#     A=list(results2[\"Weightupdates\"]+list(results[\"Weightupdates\"])[-1])\n",
    "#     B=list(results[\"Weightupdates\"])[:-1]\n",
    "#     B.extend(A)\n",
    "#     C=list(results[\"Trainerror\"])[:-1]\n",
    "#     C.extend(list(results2[\"train_germain\"]))\n",
    "#     plt.plot(B,C,'-m^')\n",
    "    \n",
    "#     ## target error\n",
    "#     D=list(results[\"targeterror\"])[:-1]\n",
    "#     D.extend(list(results2[\"target_germain\"]))\n",
    "#     plt.plot(B,D,'-k*')\n",
    "    \n",
    "#     ### bound\n",
    "#     E=results2[\"germain_bound\"]\n",
    "#     plt.plot(A,E,'-D')\n",
    "#     F=results2['boundpart3_germain']\n",
    "#     plt.plot(A,F,'-o')\n",
    "#     print(results2[\"target_germain\"])\n",
    "#     print(results2[\"germain_bound\"])\n",
    "#     ### lines for uninformative region and worse than random guessing; also for end of prior training\n",
    "#     plt.axvline(A[0],color=\"grey\")\n",
    "#     plt.axhline(y=0.5, color=\"black\", linestyle=\"--\")\n",
    "#     plt.axhline(y=1, color=\"red\", linestyle=\"--\")\n",
    "#     plt.legend([\"Training error\",\"Target error\",\"Bound\",\"KL-part\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean, variance 0.36348352 70.18035\n",
      "---------------Load MNIST----------------\n",
      "Training set (60000, 32, 32, 3) (60000, 10)\n",
      "Test set (10000, 32, 32, 3) (10000, 10)\n",
      "\n",
      "\n",
      "mean, variance 1.1809415 74.36859\n",
      "---------------Load MNIST-M----------------\n",
      "Training set (60000, 32, 32, 3) (60000, 10)\n",
      "Test set (10000, 32, 32, 3) (10000, 10)\n",
      "[[10.986111, 5.0, 2.999428, 1.99958, 1.399789, 1.0, 0.71435696, 0.5, 0.33346358, 0.20003448], [0.09088036, 0.19987813, 0.33326975, 0.5, 0.7143216, 1.0, 1.4001397, 2.0, 3.0, 5.0025883]]\n",
      "----------------------------------------\n",
      "Finished training first posterior epoch\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_0/2_1.ckpt\n",
      "\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_0/2_2.ckpt\n",
      "\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_0/2_3.ckpt\n",
      "\n",
      "Epoch 00004: saving model to posteriors/task2/Binary/30_0/2_4.ckpt\n",
      "\n",
      " !!! training error threshold reached, no further training !!!\n",
      "----------------------------------------\n",
      "Finished training posterior\n",
      "Alpha is:0.8\n",
      "----------------------------------------\n",
      "Finished training prior\n",
      "----------------------------------------\n",
      "Finished computing prior sample and target errors\n",
      "----------------------------------------\n",
      "Finished training first posterior epoch\n",
      "\n",
      "Epoch 00001: saving model to posteriors/task2/Binary/30_80/2_1.ckpt\n",
      "\n",
      "Epoch 00002: saving model to posteriors/task2/Binary/30_80/2_2.ckpt\n",
      "\n",
      "Epoch 00003: saving model to posteriors/task2/Binary/30_80/2_3.ckpt\n",
      "\n",
      " !!! training error threshold reached, no further training !!!\n",
      "----------------------------------------\n",
      "Finished training posterior\n"
     ]
    }
   ],
   "source": [
    "#alphas=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "alphas=[0,0.8]\n",
    "epsilons=[0.03]#,0.01,0.001]\n",
    "tasks=[2]\n",
    "for t in tasks:\n",
    "    x_source,y_source,x_target,y_target=load_task(t)\n",
    "\n",
    "    y_source_bin=np.array(make_mnist_binary(y_source))\n",
    "    y_target_bin=np.array(make_mnist_binary(y_target))\n",
    "    for alpha in alphas:\n",
    "        if alpha==0:\n",
    "            pass\n",
    "        else:\n",
    "            print(\"Alpha is:\"+str(alpha))\n",
    "            x_bound, x_prior, y_bound , y_prior = train_test_split(x_source,y_source_bin,test_size=alpha,random_state=69105)\n",
    "            w_a=train_prior(alpha,1,x_source,y_source_bin,x_target=x_target,y_target=y_target_bin,save=True,Task=t,Binary=True,batch_size=128)\n",
    "        for epsilon in epsilons:\n",
    "            w_s=train_posterior(alpha,x_source,y_source_bin,None,x_test=x_source,y_test=y_source_bin,epsilon=epsilon,Task=t,Binary=True,batch_size=128)\n",
    "            #for sigma in sigmas:\n",
    "                #res=read_and_prepare_results(alpha,x_source,y_source_bin,x_target,y_target_bin,sigma,delta,len(x_source),epsilon,Binary=True)#x_bound,y_bound,x_target,y_target_bin,sigma,delta,len(x_bound),epsilon,Binary=True)\n",
    "                #plot_result_file(epsilon,alpha,sigma,TASK,Binary=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
