{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function \n",
    "\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import gc, re, copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.keras.layers import deserialize, serialize\n",
    "from tensorflow.python.keras.saving import saving_utils\n",
    "\n",
    "# Project imports \n",
    "from data import mnist,mnist_m\n",
    "from data.tasks import *\n",
    "from data.label_shift import label_shift_linear, plot_labeldist, plot_splitbars\n",
    "from experiments.training import *\n",
    "from experiments.SL_bound import *\n",
    "from util.kl import *\n",
    "from util.misc import *\n",
    "from results.plotting import *\n",
    "\n",
    "# Hyper-parameters\n",
    "#batch_size = 128\n",
    "num_classes = 10\n",
    "make_plots = False\n",
    "\n",
    "delta=0.05 ## what would this be?   \n",
    "\n",
    "#alphas=[0]\n",
    "#length=10\n",
    "#for i in range(length-1):\n",
    "#    alphas.append((i+1)/length)\n",
    "\n",
    "\n",
    "\n",
    "sigmas=[]\n",
    "for i in range(2,9):  \n",
    "    sigmas.append([3,i])\n",
    "    if(i==8):\n",
    "        break\n",
    "    sigmas.append([1,i])\n",
    "\n",
    "img_rows, img_cols = 32, 32\n",
    "project_folder = \"/cephyr/users/adambre/Alvis/\"\n",
    "\n",
    "\n",
    "### TODO\n",
    "\n",
    "# # check output_dir, create it if not exists\n",
    "#     if not os.path.isdir(output_dir):\n",
    "#         os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def binarize(y,x):\n",
    "    ## take in one hot label encoding and make it into either 'label x' or 'not label x'\n",
    "    ## x is in [0,5] as we have at least 6 overlapping labels in our chestxray data\n",
    "    ## labeldict={\"No Finding\":0,\"Cardiomegaly\":1,\"Edema\":2,\"Consolidation\":3,\"Atelectasis\":4,\"Effusion\":5}\n",
    "    y_new=[]\n",
    "    mask=np.zeros(6)\n",
    "    mask[x]=1\n",
    "    for i in y:\n",
    "        if np.dot(i,mask)==1:\n",
    "            ## we have the label\n",
    "            y_new.append([0,1])\n",
    "        else:\n",
    "            ## we do not have the label\n",
    "            y_new.append([1,0])\n",
    "    return np.array(y_new)\n",
    "def load_xray(img_size,label):\n",
    "    #data_path2=\"/cephyr/NOBACKUP/groups/snic2021-23-538/\"\n",
    "    \n",
    "    data_path=\"/cephyr/users/adambre/Alvis/mnist_transfer/\"\n",
    "    x_chest=np.load(data_path+\"chestxray14_\"+str(img_size)+\".npy\",allow_pickle=True)\n",
    "    y_chest=np.load(data_path+\"chestxray14_\"+str(img_size)+\"_labels.npy\",allow_pickle=True)\n",
    "    \n",
    "    x_chex=np.load(data_path+\"chexpert_\"+str(img_size)+\".npy\",allow_pickle=True)\n",
    "    y_chex=np.load(data_path+\"chexpert_\"+str(img_size)+\"_labels.npy\",allow_pickle=True)\n",
    "    \n",
    "    ### Binarize labels\n",
    "    \n",
    "    y1=binarize(y_chest,2)\n",
    "    y2=binarize(y_chex,2)\n",
    "   \n",
    "    \n",
    "    ### do standard scaling\n",
    "    x_chest = x_chest.astype('float32')\n",
    "    sigma=np.std(x_chest)\n",
    "    x_chest /=sigma\n",
    "\n",
    "    x_chex = x_chex.astype('float32')\n",
    "    sigma2=np.std(x_chex)\n",
    "    x_chex /=sigma\n",
    "    ## mean subtraction\n",
    "    mu=np.mean(x_chest)\n",
    "    x_chest -= mu\n",
    "\n",
    "    mu2=np.mean(x_chex)\n",
    "    x_chex -= mu\n",
    "    \n",
    "    print('mean, variance', mu, sigma)\n",
    "    print(\"---------------Load ChestXray14----------------\")\n",
    "    print(x_chest.shape, y1.shape)\n",
    "    print('mean, variance', mu2, sigma2)\n",
    "    print(\"---------------Load CheXpert----------------\")\n",
    "    print(x_chex.shape, y2.shape)\n",
    "    \n",
    "    ## add them to each other\n",
    "    X=np.append(x_chest, x_chex,axis=0)\n",
    "    y=np.append(y1, y2,axis=0)\n",
    " \n",
    "    ### \n",
    "    x_source, x_target, y_source , y_target = train_test_split(X,y,test_size=0.5,random_state=69105)\n",
    "\n",
    "    return x_source, x_target, y_source , y_target\n",
    "\n",
    "#x_source, x_target, y_source , y_target=load_xray(32,2)\n",
    "#x_train, x_test, y_train , y_test = train_test_split(x,y,test_size=0.25,random_state=69105)\n",
    "# x_source, y_source, x_target, y_target=load_task(6)\n",
    "# M=init_task_model(6,True)\n",
    "\n",
    "# k=0\n",
    "# for y in y_source:\n",
    "#     if np.dot(y,[1,0])==0: ## not [1,0]\n",
    "#         k+=1\n",
    "# k2=0\n",
    "# for y in y_target:\n",
    "#     if np.dot(y,[1,0])==0:\n",
    "#         k2+=1\n",
    "# print(k/len(y_source))\n",
    "# print(k2/len(y_target))\n",
    "# ## choose loss function, optimiser etc. and train\n",
    "# M.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "#            optimizer=tf.keras.optimizers.SGD(learning_rate=0.00003, momentum=0.95),\n",
    "#                   metrics=['accuracy'])\n",
    "\n",
    "# M.evaluate(x_target,y_target)\n",
    "\n",
    "# fit_info = M.fit(x_source, y_source,\n",
    "#                  validation_data=(x_target,y_target),\n",
    "#        batch_size=128,\n",
    "#        epochs=50,\n",
    "#        verbose=1,\n",
    "#                     )\n",
    "# M.evaluate(x_target,y_target)\n",
    "# #print(fit_info.history)\n",
    "# L=len(fit_info.history[\"val_accuracy\"])\n",
    "# x=np.linspace(1,L,num=L)\n",
    "# plt.plot(x,fit_info.history[\"val_accuracy\"])\n",
    "# #print(sum(y_new))"
    "import tensorflow_datasets as tfds\n",
    "import collections\n",
    "import csv\n",
    "\n",
    "\n",
    "#data_path=\"/home/adam/Code/Datasets/chexpert/1.0.0/\"\n",
    "#data_path2=\"/home/adam/Code/Datasets/chexpert/CheXpert-v1.0-small\"\n",
    "def tf_load_image(filename):\n",
    "    image_string = tf.io.read_file(filename)\n",
    "    image = tf.image.decode_jpeg(image_string, channels=3)\n",
    "    image=tf.cast(image, tf.float32)\n",
    "    return image\n",
    "def tf_read_and_resize_image(filename,img_size):\n",
    "    ### takes file path and image size and resizes the image\n",
    "    image_string = tf.io.read_file(filename)\n",
    "    image = tf.image.decode_jpeg(image_string, channels=3)\n",
    "    image=tf.cast(image, tf.float32)\n",
    "    image = tf.image.resize(image, (img_size, img_size))#, method=tf.image.ResizeMethod.BILINEAR)\n",
    "    return image\n",
    "\n",
    "def load_from_csv(imgs_path,csv_path,resized=False):\n",
    "    _LABELS = collections.OrderedDict({\n",
    "            \"-1.0\": \"uncertain\",\n",
    "            \"1.0\": \"positive\",\n",
    "            \"0.0\": \"negative\",\n",
    "            \"\": \"unmentioned\",\n",
    "        })\n",
    "    labeldict={\"positive\":1,\"negative\":0, \"unmentioned\":0,\"uncertain\":1} ### sets all uncertain labels to 1\n",
    "    overlapping_labels=[0,2,5,6,8,10] ## according to pham et al. NF,CM,ED,CD,AC,PE\n",
    "    ##### loads chexpert filenames and labels from files\n",
    "    label_arr=[]\n",
    "    arr=[]\n",
    "    with tf.io.gfile.GFile(csv_path) as csv_f:\n",
    "        reader = csv.DictReader(csv_f)\n",
    "        # Get keys for each label from csv\n",
    "        label_keys = reader.fieldnames[5:]\n",
    "\n",
    "        for row in reader:\n",
    "            # Get image based on indicated path in csv\n",
    "            name = row[\"Path\"]\n",
    "            labels = [_LABELS[row[key]] for key in label_keys]\n",
    "            labels_overlap=[labeldict[labels[i]] for i in overlapping_labels]\n",
    "            if resized:\n",
    "                A=name.split('/')[1:]\n",
    "                path=\"resized32chex\"\n",
    "                for a in A:\n",
    "                    path+=\"/\"+a\n",
    "                name=path\n",
    "                \n",
    "                    \n",
    "            ## save the image_name and the label array\n",
    "            label_arr.append(labels_overlap)\n",
    "            #print(name)\n",
    "            \n",
    "            arr.append(os.path.join(imgs_path, name))\n",
    "        return arr,label_arr\n",
    "    \n",
    "def make_xray14_labels():\n",
    "    ##### load the labels and convert the labels to binary vectors which maps the occurrence of a label to a 1\n",
    "    data_path=\"/home/adam/Code/Datasets/chestXray14/\"\n",
    "    labeldict={\"No Finding\":0,\"Cardiomegaly\":1,\"Edema\":2,\"Consolidation\":3,\"Atelectasis\":4,\"Effusion\":5}\n",
    "    data = pd.read_csv(data_path+\"Data_Entry_2017_v2020.csv\")\n",
    "    sample = os.listdir(data_path+\"resized32/\")\n",
    "\n",
    "    sample = pd.DataFrame({'Image Index': sample})\n",
    "\n",
    "    sample = pd.merge(sample, data, how='left', on='Image Index')\n",
    "\n",
    "    sample.columns = ['Image_Index', 'Finding_Labels', 'Follow_Up_#', 'Patient_ID',\n",
    "                      'Patient_Age', 'Patient_Gender', 'View_Position',\n",
    "                      'Original_Image_Width', 'Original_Image_Height',\n",
    "                      'Original_Image_Pixel_Spacing_X',\n",
    "                      'Original_Image_Pixel_Spacing_Y']#, 'Unnamed']\n",
    "    def make_one_hot(label_string):\n",
    "        result=np.zeros(6)\n",
    "        labels=label_string.split('|')\n",
    "        for l in labels:\n",
    "            if l not in [\"No Finding\",\"Cardiomegaly\",\"Edema\",\"Consolidation\",\"Atelectasis\",\"Effusion\"]:\n",
    "                pass\n",
    "            else:\n",
    "                result[labeldict[l]]=1\n",
    "        return result.astype(int)\n",
    "\n",
    "    sample['Finding_Labels'] = sample['Finding_Labels'].apply(lambda x: make_one_hot(x))\n",
    "    #print(sample['Finding_Labels'].shape)\n",
    "    y=sample['Finding_Labels']\n",
    "    return np.array(y)\n",
    "\n",
    "def load_resize_and_save(chexpert=False,img_size=32):\n",
    "##### loads the xray datasets from the raw images and then resizes to desired size and saves them in a new directory\n",
    "    if chexpert:\n",
    "        data_path=\"/home/adam/Code/Datasets/chexpert/\"\n",
    "        _DATA_DIR = \"/home/adam/Code/Datasets/chexpert/CheXpert-v1.0-small\" ### where is the source data?\n",
    "        _TRAIN_DIR = os.path.join(_DATA_DIR, \"train\")\n",
    "        _VALIDATION_DIR = os.path.join(_DATA_DIR, \"valid\")\n",
    "        _TRAIN_LABELS_FNAME = os.path.join(_DATA_DIR, \"train.csv\")\n",
    "        _VALIDATION_LABELS_FNAME = os.path.join(_DATA_DIR, \"valid.csv\")\n",
    "        \n",
    "        filenames,labels=load_from_csv(data_path,_TRAIN_LABELS_FNAME)\n",
    "        \n",
    "        filenames_2,labels_2=load_from_csv(data_path,_VALIDATION_LABELS_FNAME)\n",
    "\n",
    "        filenames.extend(filenames_2)\n",
    "        labels.extend(labels_2)\n",
    "     \n",
    "        new_path=data_path+\"resized\"+str(img_size)+\"chex\"\n",
    "#         if not os.path.exists(new_path):\n",
    "#             os.makedirs(new_path)\n",
    "        ## load in, resize and save image in array file\n",
    "        for file in filenames:\n",
    "            img=np.array(tf_read_and_resize_image(file,img_size))\n",
    "            \n",
    "            A=file.split('/')[-4:]\n",
    "            path=\"\"\n",
    "            for a in A:\n",
    "                if a[-4:]==\".jpg\":\n",
    "                    if not os.path.exists(new_path+path):\n",
    "                        os.makedirs(new_path+path)\n",
    "                    \n",
    "                path+=\"/\"+a\n",
    "                \n",
    "            output_path=new_path+path\n",
    "            \n",
    "            #print(output_path)\n",
    "            tf.keras.preprocessing.image.save_img(output_path,img)\n",
    "            \n",
    "        #np.save(\"/home/adam/Code/Datasets/chexpert/chexpert128.npy\",np.array([np.array(tf_read_and_resize_image(file)) for file in filenames]))\n",
    "        #np.save(\"/home/adam/Code/Datasets/chexpert/labels.npy\",np.array(labels)) # save labels to separate file\n",
    "    else:\n",
    "        data_path=\"/home/adam/Code/Datasets/chestXray14/\"\n",
    "        new_path=data_path+\"resized\"+str(img_size)\n",
    "        if not os.path.exists(new_path):\n",
    "            os.makedirs(new_path)\n",
    "\n",
    "        dirs = [l for l in os.listdir(data_path+\"images/\") if l != '.DS_Store']\n",
    "        for file in dirs:\n",
    "            img=np.array(tf_read_and_resize_image(data_path+\"images/\"+file,img_size))\n",
    "            #print(img)\n",
    "            tf.keras.preprocessing.image.save_img(new_path+\"/\"+file,img)\n",
    "        \n",
    "#load_resize_and_save(chexpert=False,img_size=32)\n",
    "\n",
    "def load_to_array(chexpert=False,img_size=32):\n",
    "    if chexpert:\n",
    "        data_path=\"/home/adam/Code/Datasets/chexpert/\"\n",
    "        _DATA_DIR = \"/home/adam/Code/Datasets/chexpert/CheXpert-v1.0-small\" \n",
    "        _TRAIN_DIR = os.path.join(_DATA_DIR, \"train\")\n",
    "        _VALIDATION_DIR = os.path.join(_DATA_DIR, \"valid\")\n",
    "        _TRAIN_LABELS_FNAME = os.path.join(_DATA_DIR, \"train.csv\")\n",
    "        _VALIDATION_LABELS_FNAME = os.path.join(_DATA_DIR, \"valid.csv\")\n",
    "\n",
    "        filenames,labels=load_from_csv(data_path,_TRAIN_LABELS_FNAME,True)\n",
    "\n",
    "        filenames_2,labels_2=load_from_csv(data_path,_VALIDATION_LABELS_FNAME,True)\n",
    "        \n",
    "        filenames.extend(filenames_2)\n",
    "        arr=[]\n",
    "        labels.extend(labels_2)\n",
    "        #for file in filenames:\n",
    "            #print(np.array(tf_load_image(file)))\n",
    "            #sys.exit(-1)\n",
    "        arr= np.array([np.array(tf_load_image(img)) for img in filenames])\n",
    "        np.save(data_path+\"chexpert_\"+str(img_size)+\".npy\",arr)\n",
    "        np.save(data_path+\"chexpert_\"+str(img_size)+\"_labels.npy\",labels)\n",
    "        return arr, np.array(labels)\n",
    "    else:\n",
    "        data_path=\"/home/adam/Code/Datasets/chestXray14/\"\n",
    "        dirs = [l for l in os.listdir(data_path+\"resized\"+str(img_size)+\"/\") if l != '.DS_Store']\n",
    "        arr=[]\n",
    "        #arr= np.array([np.array(tf_load_image(data_path+\"resized\"+str(img_size)+\"/\"+img)) for img in dirs])\n",
    "        y=make_xray14_labels()\n",
    "        #np.save(data_path+\"chestxray14_\"+str(img_size)+\".npy\",arr)\n",
    "        np.save(data_path+\"chestxray14_\"+str(img_size)+\"_labels.npy\",y)\n",
    "        return arr, np.array(y)\n",
    "#X, y=load_to_array(chexpert=True,img_size=32)\n",
    "X, y=load_to_array(chexpert=True,img_size=32)\n",
    "print(X.shape, y.shape)\n",
    "###### load the resized files into numpy array and save that to a file\n",
    "\n",
    " #####load chestXray14 from files\n",
    "# x_data=np.load(data_path+\"X_sample.npy\")\n",
    "#y=pd.read_csv(data_path+\"sample_labels.csv\")\n",
    "#print(y)\n",
    "# print(y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(223414, 32, 32, 3) (223414, 6)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "X=np.load(\"/home/adam/Code/Datasets/chexpert/chexpert32.npy\")\n",
    "y=np.load(\"/home/adam/Code/Datasets/chexpert/labels.npy\")\n",
    "print(X.shape, y.shape)\n",
    "#for file in filenames:\n",
    "    #img=tf_read_and_resize_image(file)\n",
    "    #\n",
    "    #plt.imshow(tf.cast(img, dtype=tf.uint8))\n",
    "    #img=(img-np.mean(img))/np.std(img)\n",
    "    #plt.imshow(img)\n",
    "    #sys.exit(-1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "#TASK=2\n",
    "#x_source,y_source,x_target,y_target=load_task(TASK)\n",
    "from data import mnist\n",
    "from data import mnist_m as mnistm\n",
    "x_train, y_train, x_test, y_test = mnist.load_mnist()\n",
    "        \n",
    "        \n",
    "###### Add train and test together \n",
    "### MNIST all data\n",
    "x_full=np.append(x_train,x_test, axis=0)\n",
    "y_full=np.append(y_train,y_test, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## shift the distributions to create source and target distributions\n",
    "x_shift, y_shift, x_shift_target, y_shift_target = label_shift_linear(x_full,y_full,1/12,[0,1,2,3,4,5,6,7,8,9])\n",
    "\n",
    "\n",
    "#### MIXED MNIST and MNIST-m\n",
    "x_train_m, y_train_m, x_test_m, y_test_m = mnistm.load_mnistm(y_train,y_test)\n",
    "### MNIST-M all data\n",
    "x_full_m=np.append(x_train_m,x_test_m, axis=0)\n",
    "y_full_m=np.append(y_train_m,y_test_m, axis=0)\n",
    "## shift the distributions to create source and target distributions\n",
    "x_shift_m, y_shift_m,x_shift_target_m, y_shift_target_m = \\\n",
    "label_shift_linear(x_full_m,y_full_m,1/12,[0,1,2,3,4,5,6,7,8,9],decreasing=False)\n",
    "##### calculate the label densities here\n",
    "densities=[]\n",
    "densities.append(np.sum(y_shift,axis=0))\n",
    "densities.append(np.sum(y_shift_m,axis=0))\n",
    "densities.append(np.sum(y_shift_target,axis=0))\n",
    "densities.append(np.sum(y_shift_target_m,axis=0))\n",
    "\n",
    "            \n",
    "L=len(densities[0])\n",
    "interdomain_densities = [[] for x in range(2)]\n",
    "for i in range(L):\n",
    "    ## all densities are (#samples from mnist) over (#samples from mnist-m)\n",
    "    interdomain_densities[0].append(densities[0][i]/densities[1][i])\n",
    "    interdomain_densities[1].append(densities[2][i]/densities[3][i])\n",
    "print(interdomain_densities)\n",
    "## add the shifted data together to create source and target\n",
    "x_source=np.append(x_shift,x_shift_m, axis=0)\n",
    "y_source=np.append(y_shift,y_shift_m, axis=0)\n",
    "x_target=np.append(x_shift_target,x_shift_target_m, axis=0)\n",
    "y_target=np.append(y_shift_target,y_shift_target_m, axis=0)\n",
    "\n",
    "plot_splitbars([0,1,2,3,4,5,6,7,8,9],y_shift,y_shift_m,\"MNIST\",\"MNIST-M\",save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.label_shift import plot_splitbars\n",
    "\n",
    "fig=plt.imshow(x_shift[205])\n",
    "fig.axes.get_xaxis().set_visible(False)\n",
    "fig.axes.get_yaxis().set_visible(False)\n",
    "plt.savefig(\"mnist_ex.png\",dpi=600)\n",
    "#plt.imshow(x_shift_m[205])\n",
    "\n",
    "#plot_splitbars([0,1,2,3,4,5,6,7,8,9],y_shift,y_shift_m,\"MNIST\",\"MNIST-M\",save=True)"
  "source": [
    "def init_tf():\n",
    "    ### making sure that we have the GPU to work on\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    #logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    if gpus:\n",
    "      # Restrict TensorFlow to only allocate 4GB of memory on the first GPU\n",
    "      # I do not know why I have to do this but gpu does not work otherwise.\n",
    "        try:\n",
    "            tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)])\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        except RuntimeError as e:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "            print(e)\n",
    "\n",
    "    \n",
    "## custom callback to terminate training at some specific value of a metric\n",
    "    \n",
    "class stop_callback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, monitor='accuracy', value=0.001, verbose=0):\n",
    "        super(tf.keras.callbacks.Callback, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.value = value\n",
    "        self.verbose = verbose\n",
    "    def on_epoch_end(self, epoch, logs={}): \n",
    "        if(logs.get('accuracy')> self.value): # select the accuracy\n",
    "            print(\"\\n !!! training error threshold reached, no further training !!!\")\n",
    "            self.model.stop_training = True\n",
    "            \n",
    "class fast_checkpoints(tf.keras.callbacks.Callback):\n",
    "    def __init__(self,checkpoint_path,save_freq):\n",
    "        super(tf.keras.callbacks.Callback, self).__init__()\n",
    "        self.save_freq=save_freq\n",
    "        self.filepath=checkpoint_path\n",
    "        self.verbose=1\n",
    "        self.save_best_only=False\n",
    "        self.save_weights_only=True\n",
    "    def on_train_batch_begin(self, batch, epoch, logs=None):\n",
    "         if batch%self.save_freq==0:\n",
    "\n",
    "            #print(\"\\n Saved weights at the start of batch\"+str(batch)+\"\\n\")\n",
    "\n",
    "            ## Create folder\n",
    "            weight_path = self.filepath+\"/1_\"+str(batch)+\".ckpt\"\n",
    "            os.makedirs(os.path.dirname(weight_path), exist_ok=True)\n",
    "            \n",
    "            self.model.save_weights(weight_path)\n",
    "            \n",
    "def train_posterior(alpha,x_train,y_train,prior_weights=None,x_test=[],y_test=[],save=True,epsilon=0.01,Task=2,Binary=False,batch_size=128):\n",
    "        \n",
    "        TASK=Task\n",
    "        \n",
    "        ### x_test should be the whole of S for early stopping purposes\n",
    "    \n",
    "        checkpoint_path = \"posteriors/\"+\"task\"+str(TASK)+\"/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))\n",
    "        if Binary:\n",
    "            checkpoint_path = \"posteriors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))\n",
    "        \n",
    "        \n",
    "        # Create a callback that saves the model's weights every epoch\n",
    "        checkpoint_freq=np.ceil(len(x_train)/batch_size)\n",
    "        #print(checkpoint_freq)\n",
    "        cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "            save_freq=\"epoch\",#int(checkpoint_freq),### 547 = ceiling(70000/128) i.e training set for MNIST/MNIST-M,\n",
    "            filepath=checkpoint_path+\"/2_{epoch:0d}.ckpt\", \n",
    "            verbose=1,\n",
    "            save_best_only=False,\n",
    "            save_weights_only=True,\n",
    "                ## tune when to save as needed for plots\n",
    "        )\n",
    "        ## callback for first part\n",
    "        fast_checkpoint_freq=np.ceil(len(x_train)/(batch_size*10))\n",
    "        fast_cp_callback =fast_checkpoints(checkpoint_path,int(fast_checkpoint_freq))\n",
    "        stopping_callback=stop_callback(monitor='val_acc',value=1-epsilon)\n",
    "    \n",
    "        M=init_task_model(TASK,Binary)\n",
    "\n",
    "        \n",
    "            \n",
    "        ## choose loss function, optimiser etc. and train\n",
    "        \n",
    "        M.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "               optimizer=tf.keras.optimizers.SGD(learning_rate=0.003, momentum=0.95),\n",
    "                      metrics=['accuracy'],)\n",
    "        \n",
    "        \n",
    "        ## Create the folder\n",
    "        os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
    "        ## remove previous weights\n",
    "        import glob\n",
    "        files=glob.glob(os.path.join(checkpoint_path+'/*'))\n",
    "        \n",
    "        \n",
    "        for file in files:\n",
    "            if file==(checkpoint_path+'/params.txt'):\n",
    "                files.remove(checkpoint_path+'/params.txt')\n",
    "            os.remove(file)\n",
    "        \n",
    "        ### load the prior weights\n",
    "        if prior_weights is not None:\n",
    "            M.set_weights(prior_weights)\n",
    "        elif(alpha==0):\n",
    "            ### save the rand. init as the prior\n",
    "            if Binary:\n",
    "                prior_path=\"priors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(100*alpha))+\"/prior.ckpt\"\n",
    "            else:\n",
    "                prior_path=\"priors/\"+\"task\"+str(TASK)+\"/\"+str(int(100*alpha))+\"/prior.ckpt\"\n",
    "            \n",
    "            ## Create the folder\n",
    "            os.makedirs(os.path.dirname(prior_path), exist_ok=True)\n",
    "\n",
    "            ## Save the weights\n",
    "            M.save_weights(prior_path)\n",
    "        else:\n",
    "            if Binary:\n",
    "                prior_path=\"priors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(100*alpha))+\"/prior.ckpt\"\n",
    "                M.load_weights(prior_path).expect_partial()\n",
    "            else:\n",
    "                \n",
    "                M.load_weights(prior_path).expect_partial()\n",
    "        \n",
    "    \n",
    "        if save:\n",
    "            CALLBACK=[fast_cp_callback,stopping_callback]\n",
    "        else:\n",
    "            CALLBACK=[stopping_callback]\n",
    "        ### train for one epoch with more checkpoints to be able to plot more there\n",
    "        fit_info = M.fit(x_train, y_train,\n",
    "           batch_size=batch_size,\n",
    "           epochs=1, \n",
    "           callbacks=CALLBACK,\n",
    "           validation_data=(x_test, y_test),\n",
    "           verbose=0,\n",
    "                        )\n",
    "        \n",
    "        print(\"-\"*40)\n",
    "        print(\"Finished training first posterior epoch\")\n",
    "        if save:\n",
    "            CALLBACK=[cp_callback,stopping_callback]\n",
    "        else:\n",
    "            CALLBACK=[stopping_callback]\n",
    "            \n",
    "        fit_info = M.fit(x_train, y_train,\n",
    "           batch_size=batch_size,\n",
    "           epochs=2000, # we should have done early stopping before this completes\n",
    "           callbacks=CALLBACK,\n",
    "           validation_data=(x_test, y_test),\n",
    "           verbose=0,\n",
    "                        )\n",
    "        print(\"-\"*40)\n",
    "        print(\"Finished training posterior\")\n",
    "        \n",
    "         #### save the last posterior weights to disk\n",
    "        epochs_trained=len(fit_info.history['loss'])\n",
    "        #if save:\n",
    "            ## Create the folder\n",
    "            #os.makedirs(os.path.dirname(checkpoint_path+\"/2_\"+str(epochs_trained)), exist_ok=True)\n",
    "            \n",
    "            \n",
    "            #M.save_weights(checkpoint_path+\"/2_\"+str(epochs_trained)) ###### check if we need this; TODO!!!!!!\n",
    "            \n",
    "        #### save textfile with parameters, i.e. alpha ,epochs trained and epsilon\n",
    "        if Binary:\n",
    "            with open('posteriors/'+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+'/params.txt', 'w') as f:\n",
    "                f.write('\\n'.join([str(alpha), str(epsilon), str(epochs_trained)]))     \n",
    "            f.close()\n",
    "        else:\n",
    "            with open('posteriors/'+\"task\"+str(TASK)+\"/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+'/params.txt', 'w') as f:\n",
    "                f.write('\\n'.join([str(alpha), str(epsilon), str(epochs_trained)]))     \n",
    "            f.close()\n",
    "        W=M.get_weights()\n",
    "        return W\n",
    "    \n",
    "def train_prior(alpha,total_epochs,x_train=[],y_train=[],x_target=[],y_target=[],save=True,Task=2,Binary=False,batch_size=128):\n",
    "    TASK=Task\n",
    "    \n",
    "    if Binary:\n",
    "        ## Create the folders\n",
    "        os.makedirs(os.path.dirname(\"priors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(100*alpha)))+\"/\", exist_ok=True)\n",
    "        checkpoint_path = \"priors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(100*alpha))\n",
    "    else:\n",
    "        os.makedirs(os.path.dirname(\"priors/\"+\"task\"+str(TASK)+\"/\"+str(int(100*alpha)))+\"/\", exist_ok=True)\n",
    "        checkpoint_path = \"priors/\"+\"task\"+str(TASK)+\"/\"+str(int(100*alpha))\n",
    "    ## remove previous weights\n",
    "    import glob\n",
    "    files=glob.glob(os.path.join(checkpoint_path+'/*'))\n",
    "    for file in files:\n",
    "        os.remove(file)\n",
    "    M=init_task_model(TASK,Binary)\n",
    "    #sys.exit(-1)\n",
    "    ### save checkpoints 10 times over the training of the prior\n",
    "    l=len(x_train)\n",
    "    checkpoint_freq=np.ceil(l/(batch_size*10))\n",
    "    fast_cp_callback =fast_checkpoints(checkpoint_path,int(checkpoint_freq))\n",
    "    if save:\n",
    "            CALLBACK=[fast_cp_callback]\n",
    "    else:\n",
    "            CALLBACK=[]\n",
    "            \n",
    "    ## choose loss function, optimiser etc. and train\n",
    "    M.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "               optimizer=tf.keras.optimizers.SGD(learning_rate=0.003, momentum=0.95),\n",
    "                      metrics=['accuracy'],)\n",
    "    fit_info = M.fit(x_train, y_train,\n",
    "           batch_size=batch_size,\n",
    "           callbacks=CALLBACK,\n",
    "           epochs=total_epochs,\n",
    "           verbose=0,\n",
    "                        )\n",
    "    print(\"-\"*40)\n",
    "    print(\"Finished training prior\")\n",
    "    #### save the final prior weights to disk\n",
    "    if save:\n",
    "        os.makedirs(checkpoint_path, exist_ok=True)\n",
    "        M.save_weights(checkpoint_path+\"/prior.ckpt\")\n",
    "     \n",
    "    \n",
    "    list1=[]\n",
    "    \n",
    "    dirFiles = os.listdir(checkpoint_path) #list of directory files\n",
    "    \n",
    "    ## remove the ckpt.index and sort so that we get the epochs that are in the directory\n",
    "    for files in dirFiles: #filter out all non weights\n",
    "        if '.ckpt.index' in files:\n",
    "            name = re.sub('\\.ckpt.index$', '', files)\n",
    "            if (name[0]==\"1\"):\n",
    "                list1.append(name)\n",
    "        \n",
    "    list1.sort(key=lambda f: int(re.sub('\\D', '', f)))\n",
    "    list1.append(\"prior\")    ## add the final weights which has no number\n",
    "    \n",
    "    Ws=list1\n",
    "    weight_updates=[]\n",
    "    for i in Ws:\n",
    "        if i[0]==\"1\":\n",
    "            if i[1]==\"_\":\n",
    "                weight_updates.append(int(i[2:]))\n",
    "    weight_updates.append(int(np.ceil(len(y_train)/batch_size)))\n",
    "    \n",
    "    error=[]\n",
    "    target_error=[]\n",
    "    for checkpoint in Ws:\n",
    "\n",
    "        model=init_task_model(TASK,Binary)\n",
    "        model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "               optimizer=tf.keras.optimizers.SGD(learning_rate=0.003, momentum=0.95),\n",
    "                  metrics=['accuracy'],)\n",
    "      \n",
    "            \n",
    "        model.load_weights(checkpoint_path+\"/\"+str(checkpoint)+\".ckpt\").expect_partial()\n",
    "        target_error.append(1-model.evaluate(x_target,y_target,verbose=0)[1])\n",
    "        error.append(1-model.evaluate(x_train,y_train,verbose=0)[1])\n",
    "    print(\"-\"*40)\n",
    "    print(\"Finished computing prior sample and target errors\")\n",
    "    if save:\n",
    "        results=pd.DataFrame({'Weightupdates': weight_updates,\n",
    "            'Trainerror': error,\n",
    "            'targeterror':target_error,\n",
    "            })\n",
    "       \n",
    "        ## Create the folders\n",
    "        os.makedirs(os.path.dirname(project_folder+\"mnist_transfer/results/\"+\"task\"+str(TASK)+\"/Binary/\"), exist_ok=True)\n",
    "        if Binary:\n",
    "            with open(project_folder+\"mnist_transfer/results/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(100*alpha))+\"_prior_results.pkl\",'wb') as f:\n",
    "                pickle.dump(results,f)\n",
    "            f.close()\n",
    "        else:\n",
    "            with open(project_folder+\"mnist_transfer/results/\"+\"task\"+str(TASK)+\"/\"+str(int(100*alpha))+\"_prior_results.pkl\",'wb') as f:\n",
    "                pickle.dump(results,f)\n",
    "            f.close()\n",
    "    return model.get_weights()\n",
    "    \n",
    "\n",
    "\n",
    "def plot_result_file(epsilon,alpha,sigma,Binary=False,Task=2):\n",
    "    sigma_tmp=sigma\n",
    "    sigma=sigma[0]*10**(-1*sigma[1])\n",
    "    \n",
    "    if Binary:\n",
    "        result_path=\"results/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"+str(sigma_tmp[0])+str(sigma_tmp[1])\n",
    "        plt.title(\"Binary: \"+r\"$\\alpha$=\"+str(alpha)+r\" $\\epsilon$=\"+str(epsilon)+r\" $\\sigma$=\"+str(sigma))\n",
    "    else:\n",
    "        result_path=\"results/\"+\"task\"+str(TASK)+\"/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"+str(sigma_tmp[0])+str(sigma_tmp[1])\n",
    "        plt.title(r\"$\\alpha$=\"+str(alpha)+r\" $\\epsilon$=\"+str(epsilon)+r\" $\\sigma$=\"+str(sigma))\n",
    "    results=pd.read_pickle(result_path+\"_results.pkl\")\n",
    "    \n",
    "    ### do the plots\n",
    "    plt.plot(results[\"Weightupdates\"],results[\"Bound\"],'r*-')\n",
    "    plt.plot(results[\"Weightupdates\"],results[\"Trainerror\"],'m^-')\n",
    "    \n",
    "    plt.xlabel(\"Weight updates\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    \n",
    "    plt.legend([\"Bound\",\"Empirical error\"])\n",
    "    plt.show()\n",
    "\n",
    "def find_optimal_sigma(sigmas,epsilon, alpha,Binary=False,Task=2):\n",
    "    #### to find the optimal sigma just do a search through all the results \n",
    "    #### and save the one for each parameter which has the minimal bound\n",
    "    #### Do we do this per epoch or for some other value? The sigma which yields the lowest bound overall for some epoch?\n",
    "    optimal=[0,1]\n",
    "    # search through all epochs and pick the sigma which yields the smallest bound during the whole training process\n",
    "    for sigma in sigmas:\n",
    "        sigma_tmp=sigma\n",
    "        sigma=sigma[0]*10**(-1*sigma[1])\n",
    "        if Binary:\n",
    "            result_path=\"results/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"+str(sigma_tmp[0])+str(sigma_tmp[1])\n",
    "        else:\n",
    "            result_path=\"results/\"+\"task\"+str(TASK)+\"/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"+str(sigma_tmp[0])+str(sigma_tmp[1])\n",
    "        results=pd.read_pickle(result_path+\"_results.pkl\")\n",
    "        MIN=np.min(results[\"Bound\"])\n",
    "        if (MIN<optimal[1]):\n",
    "            optimal[1]=MIN\n",
    "            optimal[0]=sigma\n",
    "    print(\"The optimal sigma is {} with bound value {}\".format(optimal[0],optimal[1]))\n",
    "    \n",
    "    \n",
    "    \n",
    "    #### chest xray 14 \n",
    "    \n",
    "    \n",
    "\"\"\"  \n",
    "  ['No Finding' 'Pneumothorax' 'Effusion' 'Pleural_Thickening'\n",
    "  'Edema' 'Atelectasis'  'Cardiomegaly' 'Consolidation' 'Pneumonia'\n",
    "  'Mass' 'Infiltration' 'Fibrosis' 'Nodule' \n",
    " 'Emphysema'   'Hernia']\n",
    "\"\"\"\n",
    "##### chexpert\n",
    "\"\"\"\n",
    "    [No Finding, Pneumothorax, Pleural Effusion, \n",
    "     Edema, Atelectasis, Cardiomegaly, Consolidation, Pneumonia,\n",
    "     Pleural Other, Lung Opacity,Lung Lesion,Fracture,Support Devices,Enlarged Cardiomediastinum]\n",
    "    \n",
    "\"\"\"\n",
    "### nodule = lung lesion ?, lung opacity = infiltration? \n",
    "\n",
    "### paper thinks that only overlap is Cardiomegaly (CM), Edema (EDMA),\n",
    "#Consolidation (CS), Atelectasis (ATL), and Pleural Effusion\n",
    "#(PE) and no finding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#alphas=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "alphas=[0,0.1]\n",
    "epsilons=[0.1]#,0.01,0.001]\n",
    "tasks=[4]\n",
    "for t in tasks:\n",
    "    x_source,y_source,x_target,y_target=load_task(t)\n",
    "    for alpha in alphas:\n",
    "        if alpha==0:\n",
    "            pass\n",
    "        else:\n",
    "            print(\"Alpha is:\"+str(alpha))\n",
    "            x_bound, x_prior, y_bound , y_prior = train_test_split(x_source,y_source_bin,test_size=alpha,random_state=69105)\n",
    "            w_a=train_prior(alpha,1,x_source,y_source,x_target=x_target,y_target=y_target,save=True,Task=t,Binary=True,batch_size=128)\n",
    "        for epsilon in epsilons:\n",
    "            w_s=train_posterior(alpha,x_source,y_source,None,x_test=x_source,y_test=y_source,epsilon=epsilon,Task=t,Binary=True,batch_size=128)\n",
    "            #for sigma in sigmas:\n",
    "                #res=read_and_prepare_results(alpha,x_source,y_source_bin,x_target,y_target_bin,sigma,delta,len(x_source),epsilon,Binary=True)#x_bound,y_bound,x_target,y_target_bin,sigma,delta,len(x_bound),epsilon,Binary=True)\n",
    "                #plot_result_file(epsilon,alpha,sigma,TASK,Binary=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
