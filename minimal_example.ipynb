{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function \n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import time\n",
    "import keras\n",
    "#from tensorflow.keras.models import Model\n",
    "from keras.models import Model\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import gc, re, copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.keras.layers import deserialize, serialize\n",
    "from tensorflow.python.keras.saving import saving_utils\n",
    "\n",
    "# Project imports \n",
    "from data import mnist_m as mnistm\n",
    "from data import mnist\n",
    "from data.label_shift import label_shift_linear, plot_labeldist, plot_splitbars\n",
    "from experiments.training import *\n",
    "from experiments.SL_bound import *\n",
    "from util.kl import *\n",
    "from util.misc import *\n",
    "from results.plotting import *\n",
    "\n",
    "# Hyper-parameters\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 10\n",
    "make_plots = False\n",
    "\n",
    "delta=0.05 ## what would this be?   \n",
    "\n",
    "alphas=[]\n",
    "length=10\n",
    "for i in range(length-1):\n",
    "    alphas.append((i+1)/length)\n",
    "\n",
    "epsilons=[0.03,0.01,0.001]\n",
    "\n",
    "sigmas=[]\n",
    "for i in range(2,9):  \n",
    "    sigmas.append([3,i])#3*10**(-i))\n",
    "    if(i==8):\n",
    "        break\n",
    "    sigmas.append([1,i])#10**(-i))\n",
    "\n",
    "img_rows, img_cols = 32, 32\n",
    "project_folder = \"/cephyr/users/frejohk/Alvis/projects/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean, variance 0.36348352 70.18035\n",
      "---------------Load MNIST----------------\n",
      "Training set (60000, 32, 32, 3) (60000, 10)\n",
      "Test set (10000, 32, 32, 3) (10000, 10)\n",
      "\n",
      "\n",
      "mean, variance 1.1809415 74.36859\n",
      "---------------Load MNIST-M----------------\n",
      "Training set (60000, 32, 32, 3) (60000, 10)\n",
      "Test set (10000, 32, 32, 3) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = mnist.load_mnist()\n",
    "x_train_m, y_train_m, x_test_m, y_test_m = mnistm.load_mnistm(y_train,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create label shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Add train and test together and shift the distributions to create source and target distributions\n",
    "### MNIST all data\n",
    "x_full=np.append(x_train,x_test, axis=0)\n",
    "y_full=np.append(y_train,y_test, axis=0)\n",
    "### MNIST-M all data\n",
    "x_full_m=np.append(x_train_m,x_test_m, axis=0)\n",
    "y_full_m=np.append(y_train_m,y_test_m, axis=0)\n",
    "#x_shift,y_shift,x_shift_target,y_shift_target =label_shift(x_train,y_train,1/2,7)\n",
    "x_shift, y_shift, x_shift_target, y_shift_target = label_shift_linear(x_full,y_full,1/12,[0,1,2,3,4,5,6,7,8,9])\n",
    "x_shift_m, y_shift_m,x_shift_target_m, y_shift_target_m = label_shift_linear(x_full_m,y_full_m,1/12,[0,1,2,3,4,5,6,7,8,9],decreasing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if make_plots:\n",
    "    plot_labeldist([0,1,2,3,4,5,6,7,8,9],y_shift_target,\"shifted, target\")\n",
    "    plot_labeldist([0,1,2,3,4,5,6,7,8,9],y_shift,\"shifted, source\")\n",
    "    plot_splitbars([0,1,2,3,4,5,6,7,8,9],y_shift,y_shift_m,\"MNIST, source\",\"MNIST-M, source\")\n",
    "    plot_splitbars([0,1,2,3,4,5,6,7,8,9],y_shift_target,y_shift_target_m,\"MNIST, target\",\"MNIST-M, target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10.986111, 5.0, 2.999428, 1.99958, 1.399789, 1.0, 0.71435696, 0.5, 0.33346358, 0.20003448], [0.09088036, 0.19987813, 0.33326975, 0.5, 0.7143216, 1.0, 1.4001397, 2.0, 3.0, 5.0025883]]\n"
     ]
    }
   ],
   "source": [
    "##### Add the label shifted datasets to each other creating the source and target domain for task 2\n",
    "\n",
    "##### calculate the label densities here\n",
    "densities=[]\n",
    "densities.append(np.sum(y_shift,axis=0))\n",
    "densities.append(np.sum(y_shift_m,axis=0))\n",
    "densities.append(np.sum(y_shift_target,axis=0))\n",
    "densities.append(np.sum(y_shift_target_m,axis=0))\n",
    "# mnist source, mnist-m source, mnist target,  mnist-m target\n",
    "#print(densities)\n",
    "TASK=2\n",
    "if TASK==1:\n",
    "    ###### label density shifted mnist\n",
    "    x_source=x_shift\n",
    "    y_source=y_shift\n",
    "    x_target=x_shift_target\n",
    "    y_target=y_shift_target\n",
    "elif TASK==2:\n",
    "    #### MIXED MNIST and MNIST-m\n",
    "    L=len(densities[0])\n",
    "    interdomain_densities = [[] for x in range(2)]\n",
    "    for i in range(L):\n",
    "        ## all densities are # in mnist over # in mnist-m\n",
    "        interdomain_densities[0].append(densities[0][i]/densities[1][i])\n",
    "        interdomain_densities[1].append(densities[2][i]/densities[3][i])\n",
    "    print(interdomain_densities)\n",
    "    x_source=np.append(x_shift,x_shift_m, axis=0)\n",
    "    y_source=np.append(y_shift,y_shift_m, axis=0)\n",
    "    x_target=np.append(x_shift_target,x_shift_target_m, axis=0)\n",
    "    y_target=np.append(y_shift_target,y_shift_target_m, axis=0)\n",
    "elif TASK==3:\n",
    "    #### MNIST -> MNIST-m\n",
    "    x_source=x_full\n",
    "    y_source=y_full\n",
    "    x_target=x_full_m\n",
    "    y_target=y_full_m\n",
    "elif TASK==4:\n",
    "    #### MNIST->USPS\n",
    "    x_source=x_full\n",
    "    y_source=y_full\n",
    "    x_target=x_usps\n",
    "    y_target=y_usps\n",
    "elif TASK==5:\n",
    "    #### MNIST -> SVHN\n",
    "    x_source=x_full\n",
    "    y_source=y_full\n",
    "    x_target=x_svhn\n",
    "    y_target=y_svhn\n",
    "elif TASK==6:\n",
    "    x_source=x_chexpert\n",
    "    y_source=y_chexpert\n",
    "    x_source=x_chest14\n",
    "    y_source=y_chest14\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack(model, training_config, weights):\n",
    "    restored_model = deserialize(model)\n",
    "    if training_config is not None:\n",
    "        restored_model.compile(\n",
    "            **saving_utils.compile_args_from_training_config(\n",
    "                training_config\n",
    "            )\n",
    "        )\n",
    "    restored_model.set_weights(weights)\n",
    "    return restored_model\n",
    "\n",
    "# Hotfix function\n",
    "def make_keras_picklable():\n",
    "    def __reduce__(self):\n",
    "        model_metadata = saving_utils.model_metadata(self)\n",
    "        training_config = model_metadata.get(\"training_config\", None)\n",
    "        model = serialize(self)\n",
    "        weights = self.get_weights()\n",
    "        return (unpack, (model, training_config, weights))\n",
    "\n",
    "    cls = Model\n",
    "    cls.__reduce__ = __reduce__\n",
    "\n",
    "def read_weights(model,w_a,x_bound,y_bound,x_target,y_target,sigma,epsilon,alpha,Binary=False,Task=TASK):\n",
    "    batch_size=128\n",
    "    batches_per_epoch=np.ceil(len(y_target)/batch_size) ## should be 547\n",
    "    epoch=1\n",
    "    \n",
    "    # Run the function to fix pickling issue\n",
    "    make_keras_picklable()\n",
    "    \n",
    "    \n",
    "    sigma=sigma[0]*10**(-1*sigma[1])    \n",
    "    \n",
    "    ### Here we do something more intelligent to not have to hardcode the epoch amounts. \n",
    "    ### we parse the filenames and sort them in numerical order and then load the weights\n",
    "    if Binary:\n",
    "        path=\"posteriors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))\n",
    "    else:\n",
    "        path=\"posteriors/\"+\"task\"+str(TASK)+\"/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))\n",
    "        #epochs_trained\n",
    "    #epochs = [] #list of \n",
    "    list1=[]\n",
    "    list2=[]\n",
    "    dirFiles = os.listdir(path) #list of directory files\n",
    "    ## remove the ckpt.index and sort so that we get the epochs that are in the directory\n",
    "    for files in dirFiles: #filter out all non checkpoints\n",
    "        if '.ckpt.index' in files:\n",
    "            name = re.sub('\\.ckpt.index$', '', files)\n",
    "            ### if it has a one it goes in one list and if it starts with a two it goes in the other\n",
    "            if (name[0]==\"1\"):\n",
    "                list1.append(name)\n",
    "            elif (name[0]==\"2\"):\n",
    "                list2.append(name)\n",
    "            #epochs.append(name)\n",
    "    #epochs.sort(key=lambda f: int(re.sub('\\D', '', f)))\n",
    "    list1.sort(key=lambda f: int(re.sub('\\D', '', f)))\n",
    "    num_batchweights=len(list1)\n",
    "    list2.sort(key=lambda f: int(re.sub('\\D', '', f)))\n",
    "    list1.extend(list2)\n",
    "    Ws=list1 ## vector of checkpoint filenames\n",
    "    \n",
    "    weight_updates=[]\n",
    "    for i in Ws:\n",
    "        #print(i)\n",
    "        if i[0]==\"1\":\n",
    "            if i[1]==\"_\":\n",
    "                weight_updates.append(int(i[2:]))\n",
    "    for i in list2:\n",
    "        weight_updates.append((int(i[2:])+1)*batches_per_epoch)\n",
    "   \n",
    "    ### load the model and the weights\n",
    "    N_checkpoints=len(Ws)\n",
    "    KLs=np.zeros(N_checkpoints)\n",
    "    errors=np.zeros(N_checkpoints)\n",
    "    targeterrors=np.zeros(N_checkpoints)\n",
    "    epochs=np.zeros(N_checkpoints)\n",
    "#     for checkpoint in Ws:\n",
    "    \n",
    "    #### here we should pass all the checkpoints to different processes and evaluate on the dataset\n",
    "    args=[]\n",
    "    #for i in range(N_checkpoints):\n",
    "    #for i in range(2):\n",
    "        #args.append(nnp.array(i,Ws[i],path,KLs, errors,targeterrors,epochs,model,w_a,copy.deepcopy(x_bound),copy.deepcopy(y_bound),copy.deepcopy(x_target),copy.deepcopy(y_target),sigma,epsilon,alpha,Binary,TASK))\n",
    "        #args.append(np.array([i,Ws[i],path,KLs, errors,targeterrors,epochs,model,w_a,x_bound,y_bound,x_target,y_target,sigma,epsilon,alpha,Binary,TASK]))\n",
    "    args.append([Ws[0],x_bound,y_bound])   \n",
    "    args.append([Ws[1],copy.deepcopy(x_bound),copy.deepcopy(y_bound)])\n",
    "    args.append([Ws[2],copy.deepcopy(x_bound),copy.deepcopy(y_bound)])   \n",
    "    args.append([Ws[3],copy.deepcopy(x_bound),copy.deepcopy(y_bound)])\n",
    "    p = Pool(processes = 3)\n",
    "    print(\"could deep copy\")\n",
    "    #for arg in args:\n",
    "    if __name__ == '__main__':\n",
    "        p.imap(dumb_func,args)\n",
    "        p.close()\n",
    "        p.join()\n",
    "\n",
    "    #print(\"we made it here!!!!\")\n",
    "    print(KLs)\n",
    "    print(errors)\n",
    "    print(targeterrors)\n",
    "    sys.exit(-1)\n",
    "    \n",
    "    return KLs,errors,targeterrors,Ws,Xvector\n",
    "\n",
    "def dumb_func(args):\n",
    "    #print(args)\n",
    "    init_tf()\n",
    "    \n",
    "    \n",
    "    print(\"!!!!\")\n",
    "    model=init_MNIST_model_binary()\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.003, momentum=0.95),metrics=['accuracy'])\n",
    "    print(\"Model compiled\")\n",
    "    path=\"posteriors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))\n",
    "    model.load_weights(path+\"/\"+str(checkpoint_name)+\".ckpt\").expect_partial()\n",
    "    print(w_s=model.get_weights())\n",
    "    #model.evaluate(args[1],args[2])\n",
    "    sum=0\n",
    "    for i in range(1000000):\n",
    "        sum+=i\n",
    "    print(sum)\n",
    "    print(len(args[0]))\n",
    "    \n",
    "def init_tf():\n",
    "    ### making sure that we have the GPU to work on\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    #logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    if gpus:\n",
    "      # Restrict TensorFlow to only allocate 4GB of memory on the first GPU\n",
    "      # I do not know why I have to do this but gpu does not work otherwise.\n",
    "        try:\n",
    "            tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)])\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        except RuntimeError as e:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "            print(e)\n",
    "\n",
    "    \n",
    "## custom callback to terminate training at some specific value of a metric\n",
    "    \n",
    "class stop_callback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, monitor='accuracy', value=0.001, verbose=0):\n",
    "        super(tf.keras.callbacks.Callback, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.value = value\n",
    "        self.verbose = verbose\n",
    "    def on_epoch_end(self, epoch, logs={}): \n",
    "        if(logs.get('accuracy')> self.value): # select the accuracy\n",
    "            print(\"\\n !!! training error threshold reached, no further training !!!\")\n",
    "            self.model.stop_training = True\n",
    "            \n",
    "class fast_checkpoints(tf.keras.callbacks.Callback):\n",
    "    def __init__(self,checkpoint_path,save_freq):\n",
    "        super(tf.keras.callbacks.Callback, self).__init__()\n",
    "        self.save_freq=save_freq\n",
    "        self.filepath=checkpoint_path\n",
    "        self.verbose=1\n",
    "        self.save_best_only=False\n",
    "        self.save_weights_only=True\n",
    "    def on_train_batch_begin(self, batch, epoch, logs=None):\n",
    "         if batch%self.save_freq==0:\n",
    "\n",
    "            print(\"\\n Saved weights at the start of batch\"+str(batch)+\"\\n\")\n",
    "\n",
    "            ## Create folder\n",
    "            weight_path = self.filepath+\"/1_\"+str(batch)+\".ckpt\"\n",
    "            os.makedirs(os.path.dirname(weight_path), exist_ok=True)\n",
    "            \n",
    "            self.model.save_weights(weight_path)\n",
    "            \n",
    "def train_posterior(alpha,x_train,y_train,prior_weights=None,x_test=[],y_test=[],save=True,epsilon=0.01,Task=2,Binary=False):\n",
    "        \n",
    "        TASK=Task\n",
    "        batch_size=128\n",
    "        \n",
    "        ### x_test should be the whole of S for early stopping purposes\n",
    "    \n",
    "        checkpoint_path = \"posteriors/\"+\"task\"+str(TASK)+\"/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))\n",
    "        if Binary:\n",
    "            checkpoint_path = \"posteriors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))\n",
    "        \n",
    "        \n",
    "        # Create a callback that saves the model's weights every epoch\n",
    "        cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        save_freq=547,   ### 547 = ceiling(70000/128) i.e training set for MNIST/MNIST-M,\n",
    "        filepath=checkpoint_path+\"/2_{epoch:0d}.ckpt\", \n",
    "        verbose=1,\n",
    "        save_best_only=False,\n",
    "        save_weights_only=True,\n",
    "            ## tune when to save as needed for plots\n",
    "        )\n",
    "        fast_cp_callback =fast_checkpoints(checkpoint_path,45)\n",
    "        stopping_callback=stop_callback(monitor='val_acc',value=1-epsilon)\n",
    "    \n",
    "        if Binary:\n",
    "            M=init_MNIST_model_binary()\n",
    "        else:\n",
    "            M=init_MNIST_model()\n",
    "\n",
    "        \n",
    "            \n",
    "        ## choose loss function, optimiser etc. and train\n",
    "        \n",
    "        M.compile(loss=keras.losses.categorical_crossentropy,\n",
    "               optimizer=keras.optimizers.SGD(learning_rate=0.003, momentum=0.95),\n",
    "                      metrics=['accuracy'],)\n",
    "        ### load the prior weights\n",
    "        if prior_weights is not None:\n",
    "            M.set_weights(prior_weights)\n",
    "        elif(alpha==0):\n",
    "            ### save the rand. init as the prior\n",
    "            prior_path=\"priors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(100*alpha))+\"/prior.ckpt\"\n",
    "            \n",
    "            ## Create the folder\n",
    "            os.makedirs(os.path.dirname(prior_path), exist_ok=True)\n",
    "            \n",
    "            ## Save the weights\n",
    "            M.save_weights(prior_path)\n",
    "        else:\n",
    "            if Binary:\n",
    "                prior_path=\"priors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(100*alpha))+\"/prior.ckpt\"\n",
    "                M.load_weights(prior_path)#.expect_partial()\n",
    "            else:\n",
    "                prior_path=\"priors/\"+\"task\"+str(TASK)+\"/\"+str(int(100*alpha))+\"/prior.ckpt\"\n",
    "                M.load_weights(prior_path)#.expect_partial()\n",
    "        \n",
    "    \n",
    "        if save:\n",
    "            CALLBACK=[fast_cp_callback,stopping_callback]\n",
    "        else:\n",
    "            CALLBACK=[stopping_callback]\n",
    "        ### train for one epoch with more checkpoints to be able to plot more there\n",
    "        fit_info = M.fit(x_train, y_train,\n",
    "           batch_size=batch_size,\n",
    "           epochs=1, \n",
    "           callbacks=CALLBACK,\n",
    "           validation_data=(x_test, y_test),\n",
    "           verbose=1,\n",
    "                        )\n",
    "        \n",
    "        \n",
    "        if save:\n",
    "            CALLBACK=[cp_callback,stopping_callback]\n",
    "        else:\n",
    "            CALLBACK=[stopping_callback]\n",
    "            \n",
    "        fit_info = M.fit(x_train, y_train,\n",
    "           batch_size=batch_size,\n",
    "           epochs=2000, # we should have done early stopping before this completes\n",
    "           callbacks=CALLBACK,\n",
    "           validation_data=(x_test, y_test),\n",
    "           verbose=1,\n",
    "                        )\n",
    "        \n",
    "        \n",
    "         #### save the last posterior weights to disk\n",
    "        epochs_trained=len(fit_info.history['loss'])\n",
    "        if save:\n",
    "            ## Create the folder\n",
    "            os.makedirs(os.path.dirname(checkpoint_path+\"/2_\"+str(epochs_trained)), exist_ok=True)\n",
    "            \n",
    "            M.save_weights(checkpoint_path+\"/2_\"+str(epochs_trained)) ###### check if we need this; TODO!!!!!!\n",
    "            \n",
    "        #### save textfile with parameters, i.e. alpha ,epochs trained and epsilon\n",
    "        if Binary:\n",
    "            with open('posteriors/'+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+'/params.txt', 'w') as f:\n",
    "                f.write('\\n'.join([str(alpha), str(epsilon), str(epochs_trained)]))     \n",
    "            f.close()\n",
    "        else:\n",
    "            with open('posteriors/'+\"task\"+str(TASK)+\"/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+'/params.txt', 'w') as f:\n",
    "                f.write('\\n'.join([str(alpha), str(epsilon), str(epochs_trained)]))     \n",
    "            f.close()\n",
    "        W=M.get_weights()\n",
    "        return W\n",
    "    \n",
    "def train_prior(alpha,total_epochs,x_train=[],y_train=[],x_target=[],y_target=[],save=True,Task=2,Binary=False):\n",
    "    TASK=Task\n",
    "    checkpoint_path = \"priors/\"+\"task\"+str(TASK)+\"/\"+str(int(100*alpha))\n",
    "    \n",
    "    if Binary:\n",
    "        M=init_MNIST_model_binary()\n",
    "        checkpoint_path = \"priors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(100*alpha))#+\"/prior.ckpt\"\n",
    "    else:\n",
    "        M=init_MNIST_model()\n",
    "        \n",
    "    fast_cp_callback =fast_checkpoints(checkpoint_path,10)\n",
    "    if save:\n",
    "            CALLBACK=[fast_cp_callback]\n",
    "    else:\n",
    "            CALLBACK=[]\n",
    "            \n",
    "    # now to use it in multiprocessing, the following is necessary\n",
    "    #M._make_predict_function()\n",
    "    #sess = tf.Session()\n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    #default_graph = tf.get_default_graph()\n",
    "    #default_graph.finalize()\n",
    "    \n",
    "    ## choose loss function, optimiser etc. and train\n",
    "    M.compile(loss=keras.losses.categorical_crossentropy,\n",
    "               optimizer=keras.optimizers.SGD(learning_rate=0.003, momentum=0.95),\n",
    "                      metrics=['accuracy'],)\n",
    "    fit_info = M.fit(x_train, y_train,\n",
    "           batch_size=batch_size,\n",
    "           callbacks=CALLBACK,\n",
    "           epochs=total_epochs,\n",
    "           verbose=1,\n",
    "                        )\n",
    "    #### save the final prior weights to disk\n",
    "    if save:\n",
    "        os.makedirs(checkpoint_path, exist_ok=True)\n",
    "        M.save_weights(checkpoint_path+\"/prior.ckpt\")\n",
    "    \n",
    " \n",
    "    \n",
    "    list1=[]\n",
    "    \n",
    "    dirFiles = os.listdir(checkpoint_path) #list of directory files\n",
    "    \n",
    "    ## remove the ckpt.index and sort so that we get the epochs that are in the directory\n",
    "    for files in dirFiles: #filter out all non weights\n",
    "        if '.ckpt.index' in files:\n",
    "            name = re.sub('\\.ckpt.index$', '', files)\n",
    "            if (name[0]==\"1\"):\n",
    "                list1.append(name)\n",
    "        \n",
    "    list1.sort(key=lambda f: int(re.sub('\\D', '', f)))\n",
    "    list1.append(\"prior\")    ## add the final weights which has no number\n",
    "    \n",
    "    Ws=list1\n",
    "    weight_updates=[]\n",
    "    for i in Ws:\n",
    "        if i[0]==\"1\":\n",
    "            if i[1]==\"_\":\n",
    "                weight_updates.append(int(i[2:]))\n",
    "    weight_updates.append(int(np.ceil(len(y_train)/batch_size)))\n",
    "    \n",
    "    error=[]\n",
    "    target_error=[]\n",
    "    for checkpoint in Ws:\n",
    "        if Binary:\n",
    "            model=init_MNIST_model_binary()\n",
    "            model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                   optimizer=keras.optimizers.SGD(learning_rate=0.003, momentum=0.95),\n",
    "                      metrics=['accuracy'],)\n",
    "        else:\n",
    "            model=init_MNIST_model()\n",
    "            model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                   optimizer=keras.optimizers.SGD(learning_rate=0.003, momentum=0.95),\n",
    "                      metrics=['accuracy'],)\n",
    "            \n",
    "        model.load_weights(checkpoint_path+\"/\"+str(checkpoint)+\".ckpt\")#.expect_partial()\n",
    "        target_error.append(1-model.evaluate(x_target,y_target,verbose=0)[1])\n",
    "        error.append(1-model.evaluate(x_train,y_train,verbose=0)[1])\n",
    "    \n",
    "    if save:\n",
    "        results=pd.DataFrame({'Weightupdates': weight_updates,\n",
    "            'Trainerror': error,\n",
    "            'targeterror':target_error,\n",
    "            })\n",
    "        with open(path_to_root_file+'mnist_transfer/'+checkpoint_path+\"/results.pkl\",'wb') as f:\n",
    "            pickle.dump(results,f)\n",
    "        f.close()\n",
    "    \n",
    "    return model.get_weights()\n",
    "    \n",
    "def read_and_prepare_results(alpha,x_bound,y_bound,x_target,y_target,sigma,delta,N,epsilon,Binary=False,Task=TASK):\n",
    "    \n",
    "    sigma_tmp=sigma\n",
    "    sigma=sigma[0]*10**(-1*sigma[1])\n",
    "    \n",
    "    ## read params.txt for the desired alpha and get the parameters\n",
    "    if Binary:\n",
    "        with open('posteriors/'+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+'/params.txt', 'rb+') as f:\n",
    "            params=f.readlines()\n",
    "        f.close()\n",
    "        prior_path=\"priors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(100*alpha))+\"/prior.ckpt\"\n",
    "        result_path=\"results/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"\n",
    "    else:\n",
    "        with open('posteriors/'+\"task\"+str(TASK)+\"/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+'/params.txt', 'rb+') as f:\n",
    "            params=f.readlines()\n",
    "        f.close()\n",
    "        prior_path=\"priors/\"+\"task\"+str(TASK)+\"/\"+str(int(100*alpha))+\"/prior.ckpt\"\n",
    "        result_path=\"results/\"+\"task\"+str(TASK)+\"/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"\n",
    "        \n",
    "    epsilon=float(params[1])\n",
    "    epochs_trained=int(params[2])\n",
    "    \n",
    "    # initialise model\n",
    "    if Binary:\n",
    "        M=init_MNIST_model_binary()\n",
    "    else:\n",
    "        M=init_MNIST_model()\n",
    "    M.compile(loss=keras.losses.categorical_crossentropy,\n",
    "               optimizer=keras.optimizers.SGD(learning_rate=0.003, momentum=0.95),\n",
    "                      metrics=['accuracy'],)\n",
    "    ### load the prior weights if there are any\n",
    "    if alpha==0:\n",
    "        ### do nothing, i.e take the random init\n",
    "        w_a=M.get_weights()\n",
    "    else:\n",
    "        M.load_weights(prior_path).expect_partial()\n",
    "        w_a=M.get_weights()\n",
    "    print(Binary)\n",
    "    # read the weights and calculate what is needed for the bound\n",
    "    [KLs,errors,targeterrors,Ws,weight_updates]=read_weights(M,w_a,x_bound,y_bound,x_target,y_target,sigma_tmp,epsilon,alpha,Binary=Binary,Task=TASK)    \n",
    "    \n",
    "    #print(KLs)\n",
    "    #print(errors)\n",
    "    #print(targeterrors)\n",
    "    #print(Ws)\n",
    "   \n",
    "    bound=[]\n",
    "    ### calculate the bound\n",
    "    for i in range(len(weight_updates)):\n",
    "        bound.append(calculate_bound(KLs[i],alpha,delta,N,errors[i]))\n",
    "    \n",
    "    \n",
    "    #save the results to a pickled dataframe in results\n",
    "    results=pd.DataFrame({'Weightupdates': weight_updates,\n",
    "        'Trainerror': errors,\n",
    "        'targeterror':targeterrors,\n",
    "        'KL': KLs,\n",
    "        'Bound': bound})\n",
    "    with open(path_to_root_file+'mnist_transfer/'+result_path+str(sigma_tmp[0])+str(sigma_tmp[1])+\"_results.pkl\",'wb') as f:#int(sigma*10**8)\n",
    "        pickle.dump(results,f)\n",
    "    f.close()\n",
    "    return results\n",
    "\n",
    "def plot_result_file(epsilon,alpha,sigma,Binary=False,Task=TASK):\n",
    "    sigma_tmp=sigma\n",
    "    sigma=sigma[0]*10**(-1*sigma[1])\n",
    "    \n",
    "    if Binary:\n",
    "        result_path=\"results/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"+str(sigma_tmp[0])+str(sigma_tmp[1])\n",
    "        plt.title(\"Binary: \"+r\"$\\alpha$=\"+str(alpha)+r\" $\\epsilon$=\"+str(epsilon)+r\" $\\sigma$=\"+str(sigma))\n",
    "    else:\n",
    "        result_path=\"results/\"+\"task\"+str(TASK)+\"/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"+str(sigma_tmp[0])+str(sigma_tmp[1])\n",
    "        plt.title(r\"$\\alpha$=\"+str(alpha)+r\" $\\epsilon$=\"+str(epsilon)+r\" $\\sigma$=\"+str(sigma))\n",
    "    results=pd.read_pickle(result_path+\"_results.pkl\")\n",
    "    \n",
    "    ### do the plots\n",
    "    plt.plot(results[\"Weightupdates\"],results[\"Bound\"],'r*-')\n",
    "    plt.plot(results[\"Weightupdates\"],results[\"Trainerror\"],'m^-')\n",
    "    \n",
    "    plt.xlabel(\"Weight updates\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    \n",
    "    plt.legend([\"Bound\",\"Empirical error\"])\n",
    "    plt.show()\n",
    "\n",
    "def find_optimal_sigma(sigmas,epsilon, alpha,Binary=False,Task=TASK):\n",
    "    #### to find the optimal sigma just do a search through all the results \n",
    "    #### and save the one for each parameter which has the minimal bound\n",
    "    #### Do we do this per epoch or for some other value? The sigma which yields the lowest bound overall for some epoch?\n",
    "    optimal=[0,1]\n",
    "    # search through all epochs and pick the sigma which yields the smallest bound during the whole training process\n",
    "    for sigma in sigmas:\n",
    "        sigma_tmp=sigma\n",
    "        sigma=sigma[0]*10**(-1*sigma[1])\n",
    "        if Binary:\n",
    "            result_path=\"results/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"+str(sigma_tmp[0])+str(sigma_tmp[1])\n",
    "        else:\n",
    "            result_path=\"results/\"+\"task\"+str(TASK)+\"/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"+str(sigma_tmp[0])+str(sigma_tmp[1])\n",
    "        results=pd.read_pickle(result_path+\"_results.pkl\")\n",
    "        MIN=np.min(results[\"Bound\"])\n",
    "        if (MIN<optimal[1]):\n",
    "            optimal[1]=MIN\n",
    "            optimal[0]=sigma\n",
    "    print(\"The optimal sigma is {} with bound value {}\".format(optimal[0],optimal[1]))\n",
    "\n",
    "   \n",
    "#### find the optimal sigma for every combination of parameters\n",
    "\n",
    "#### use the optimal sigmas to calculate the bound 50 times(with different data orders and initialisation)\n",
    "#### (Note: also delta=13*delta_0) for every combination and save the mean and std for plotting into a result file\n",
    "#for i in range(50):\n",
    "    ## take in the data and split with a new seed\n",
    " #   x_bound, x_prior, y_bound , y_prior = train_test_split(x_source,y_source,test_size=alpha,random_state=(69105+i))\n",
    "#### \n",
    "def read_prior(alpha,TASK=2,Binary=True):\n",
    "    checkpoint_path = \"priors/\"+\"task\"+str(TASK)+\"/\"+str(int(100*alpha))\n",
    "    if Binary:\n",
    "        checkpoint_path = \"priors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(100*alpha))\n",
    "    result_path=path_to_root_file+'mnist_transfer/'+checkpoint_path+\"/results.pkl\"\n",
    "    results=pd.read_pickle(result_path)\n",
    "    plt.title(r\"$\\alpha$=\"+str(alpha))\n",
    "    plt.plot(results[\"Weightupdates\"],results[\"Trainerror\"],'m^-')\n",
    "    plt.plot(results[\"Weightupdates\"],results[\"targeterror\"],'k^-')\n",
    "    plt.legend([\"Training error\",\"Target error\"])\n",
    "    \n",
    "def plot_prior_and_posterior(alpha,epsilon,sigma,TASK=2,Binary=True):\n",
    "    ### load in the prior data\n",
    "    sigma_tmp=sigma\n",
    "    sigma=sigma[0]*10**(-1*sigma[1])\n",
    "    checkpoint_path = \"priors/\"+\"task\"+str(TASK)+\"/\"+str(int(100*alpha))\n",
    "    if Binary:\n",
    "        checkpoint_path = \"priors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(100*alpha))\n",
    "    result_path=path_to_root_file+'mnist_transfer/'+checkpoint_path+\"/results.pkl\"\n",
    "    results=pd.read_pickle(result_path)\n",
    "    result_path_post=\"results/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"+str(sigma_tmp[0])+str(sigma_tmp[1])\n",
    "    results2=pd.read_pickle(result_path_post+\"_results.pkl\")\n",
    "    \n",
    "    \n",
    "    ### remove/ignore the last entry of the prior data \n",
    "    ### as it should be a duplication of the first one from the posterior results\n",
    "    \n",
    "    ### training error\n",
    "    A=list(results2[\"Weightupdates\"]+list(results[\"Weightupdates\"])[-1])\n",
    "    B=list(results[\"Weightupdates\"])[:-1]\n",
    "    B.extend(A)\n",
    "    C=list(results[\"Trainerror\"])[:-1]\n",
    "    C.extend(list(results2[\"train_germain\"]))\n",
    "    plt.plot(B,C,'-m^')\n",
    "    \n",
    "    ## target error\n",
    "    D=list(results[\"targeterror\"])[:-1]\n",
    "    D.extend(list(results2[\"target_germain\"]))\n",
    "    plt.plot(B,D,'-k*')\n",
    "    \n",
    "    ### bound\n",
    "    E=results2[\"germain_bound\"]\n",
    "    plt.plot(A,E,'-D')\n",
    "    F=results2['boundpart3_germain']\n",
    "    plt.plot(A,F,'-o')\n",
    "    print(results2[\"target_germain\"])\n",
    "    print(results2[\"germain_bound\"])\n",
    "    ### lines for uninformative region and worse than random guessing; also for end of prior training\n",
    "    plt.axvline(A[0],color=\"grey\")\n",
    "    plt.axhline(y=0.5, color=\"black\", linestyle=\"--\")\n",
    "    plt.axhline(y=1, color=\"red\", linestyle=\"--\")\n",
    "    plt.legend([\"Training error\",\"Target error\",\"Bound\",\"KL-part\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha is:0.1\n",
      "Train on 70010 samples, validate on 70010 samples\n",
      "Epoch 1/1\n",
      "\n",
      " Saved weights at the start of batch0\n",
      "\n",
      " 5760/70010 [=>............................] - ETA: 5s - loss: 0.1615 - accuracy: 0.9377\n",
      " Saved weights at the start of batch45\n",
      "\n",
      " 9728/70010 [===>..........................] - ETA: 4s - loss: 0.1599 - accuracy: 0.9384\n",
      " Saved weights at the start of batch90\n",
      "\n",
      "15488/70010 [=====>........................] - ETA: 3s - loss: 0.1582 - accuracy: 0.9378\n",
      " Saved weights at the start of batch135\n",
      "\n",
      "21248/70010 [========>.....................] - ETA: 2s - loss: 0.1559 - accuracy: 0.9386\n",
      " Saved weights at the start of batch180\n",
      "\n",
      "27008/70010 [==========>...................] - ETA: 2s - loss: 0.1523 - accuracy: 0.9399\n",
      " Saved weights at the start of batch225\n",
      "\n",
      "32768/70010 [=============>................] - ETA: 2s - loss: 0.1501 - accuracy: 0.9412\n",
      " Saved weights at the start of batch270\n",
      "\n",
      "38528/70010 [===============>..............] - ETA: 1s - loss: 0.1497 - accuracy: 0.9411\n",
      " Saved weights at the start of batch315\n",
      "\n",
      "44288/70010 [=================>............] - ETA: 1s - loss: 0.1491 - accuracy: 0.9416\n",
      " Saved weights at the start of batch360\n",
      "\n",
      "50048/70010 [====================>.........] - ETA: 0s - loss: 0.1485 - accuracy: 0.9420\n",
      " Saved weights at the start of batch405\n",
      "\n",
      "55808/70010 [======================>.......] - ETA: 0s - loss: 0.1465 - accuracy: 0.9428\n",
      " Saved weights at the start of batch450\n",
      "\n",
      "61568/70010 [=========================>....] - ETA: 0s - loss: 0.1446 - accuracy: 0.9432\n",
      " Saved weights at the start of batch495\n",
      "\n",
      "67328/70010 [===========================>..] - ETA: 0s - loss: 0.1431 - accuracy: 0.9440\n",
      " Saved weights at the start of batch540\n",
      "\n",
      "70010/70010 [==============================] - 5s 65us/step - loss: 0.1427 - accuracy: 0.9442 - val_loss: 0.1159 - val_accuracy: 0.9536\n",
      "Train on 70010 samples, validate on 70010 samples\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute '_in_multi_worker_mode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-e925e1a82220>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepsilons\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mw_s\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_posterior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_source\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_source_bin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_source\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_source_bin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTASK\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mBinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;31m#for sigma in sigmas:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;31m#res=read_and_prepare_results(alpha,x_source,y_source_bin,x_target,y_target_bin,sigma,delta,len(x_source),epsilon,Binary=True)#x_bound,y_bound,x_target,y_target_bin,sigma,delta,len(x_bound),epsilon,Binary=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-0c567b2f7c7d>\u001b[0m in \u001b[0;36mtrain_posterior\u001b[0;34m(alpha, x_train, y_train, prior_weights, x_test, y_test, save, epsilon, Task, Binary)\u001b[0m\n\u001b[1;32m    259\u001b[0m            \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCALLBACK\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m            \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m            \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m                         )\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/Alvis/software/Keras/2.3.1-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m/apps/Alvis/software/Keras/2.3.1-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;34m'metrics'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     })\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcbk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/Alvis/software/Keras/2.3.1-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/keras/callbacks/callbacks.py\u001b[0m in \u001b[0;36m_call_begin_hook\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;34m\"\"\"Helper function for on_{train|test|predict}_begin methods.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_TRAIN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_TEST\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/Alvis/software/Keras/2.3.1-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/keras/callbacks/callbacks.py\u001b[0m in \u001b[0;36mon_train_begin\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \"\"\"\n\u001b[1;32m    218\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/Alvis/software/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_begin\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    925\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m       \u001b[0;31m# MultiWorkerTrainingState is used to manage the training state needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m       \u001b[0;31m# for preemption-recovery of a worker in multi-worker training.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute '_in_multi_worker_mode'"
     ]
    }
   ],
   "source": [
    "alphas=[0.1]\n",
    "y_source_bin=np.array(make_mnist_binary(y_source))\n",
    "y_target_bin=np.array(make_mnist_binary(y_target))\n",
    "for alpha in alphas:\n",
    "    print(\"Alpha is:\"+str(alpha))\n",
    "    x_bound, x_prior, y_bound , y_prior = train_test_split(x_source,y_source_bin,test_size=alpha,random_state=69105)\n",
    "    #w_a=train_prior(alpha,1,x_source,y_source_bin,x_target=x_target,y_target=y_target_bin,save=True,Task=2,Binary=True)\n",
    "    \n",
    "    for epsilon in epsilons:\n",
    "        w_s=train_posterior(alpha,x_source,y_source_bin,None,x_test=x_source,y_test=y_source_bin,epsilon=epsilon,Task=TASK,Binary=True)\n",
    "        #for sigma in sigmas:\n",
    "            #res=read_and_prepare_results(alpha,x_source,y_source_bin,x_target,y_target_bin,sigma,delta,len(x_source),epsilon,Binary=True)#x_bound,y_bound,x_target,y_target_bin,sigma,delta,len(x_bound),epsilon,Binary=True)\n",
    "            #plot_result_file(epsilon,alpha,sigma,TASK,Binary=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
