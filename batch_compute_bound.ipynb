{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function \n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import gc, re, copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.keras.layers import deserialize, serialize\n",
    "from tensorflow.python.keras.saving import saving_utils\n",
    "\n",
    "# Project imports \n",
    "from data import mnist_m as mnistm\n",
    "from data import mnist\n",
    "from data.label_shift import label_shift_linear, plot_labeldist, plot_splitbars\n",
    "from data.tasks import load_task\n",
    "from experiments.training import *\n",
    "from experiments.SL_bound import *\n",
    "from experiments.DA_bound import *\n",
    "from bounds.bounds import *\n",
    "from util.kl import *\n",
    "from util.misc import *\n",
    "from results.plotting import *\n",
    "\n",
    "# Hyper-parameters\n",
    "TASK = 2\n",
    "seed = 69105\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "n_classifiers = 2\n",
    "epochs = 10\n",
    "make_plots = False\n",
    "delta=0.05 ## what would this be?   \n",
    "binary=True\n",
    "epsilons=[0.01]\n",
    "alphas=[0.1]#0,0.3]\n",
    "sigmas=[[3,2],[3,3]]\n",
    "\n",
    "project_folder = \"/cephyr/users/frejohk/Alvis/projects/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and structure data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean, variance 0.36348352 70.18035\n",
      "---------------Load MNIST----------------\n",
      "Training set (60000, 32, 32, 3) (60000, 10)\n",
      "Test set (10000, 32, 32, 3) (10000, 10)\n",
      "\n",
      "\n",
      "mean, variance 1.1809415 74.36859\n",
      "---------------Load MNIST-M----------------\n",
      "Training set (60000, 32, 32, 3) (60000, 10)\n",
      "Test set (10000, 32, 32, 3) (10000, 10)\n",
      "[[10.986111, 5.0, 2.999428, 1.99958, 1.399789, 1.0, 0.71435696, 0.5, 0.33346358, 0.20003448], [0.09088036, 0.19987813, 0.33326975, 0.5, 0.7143216, 1.0, 1.4001397, 2.0, 3.0, 5.0025883]]\n"
     ]
    }
   ],
   "source": [
    "x_source, y_source, x_target, y_target = load_task(TASK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_args(task, bound='germain', alpha=0.1, sigma=[3,2], epsilon=[0.01], binary=False, n_classifiers=4):\n",
    " \n",
    "    if binary:\n",
    "        with open('posteriors/'+\"task\"+str(task)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+'/params.txt', 'rb+') as f:\n",
    "            params=f.readlines()\n",
    "        f.close()\n",
    "        prior_path=\"priors/\"+\"task\"+str(task)+\"/Binary/\"+str(int(100*alpha))+\"/prior.ckpt\"\n",
    "        result_path=\"results/\"+\"task\"+str(task)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"\n",
    "    else:\n",
    "        with open('posteriors/'+\"task\"+str(task)+\"/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+'/params.txt', 'rb+') as f:\n",
    "            params=f.readlines()\n",
    "        f.close()\n",
    "        prior_path=\"priors/\"+\"task\"+str(task)+\"/\"+str(int(100*alpha))+\"/prior.ckpt\"\n",
    "        result_path=\"results/\"+\"task\"+str(task)+\"/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"\n",
    "\n",
    "    #epsilon=float(params[1]) # @TODO: Superfluous? Isn't this submitted?\n",
    "    #epochs_trained=int(params[2]) # @TODO: Unused as far as I can see\n",
    "     \n",
    "    posterior_paths = posterior_checkpoints(task, epsilon, alpha, binary=binary)\n",
    "    \n",
    "    arg_list = []\n",
    "    for post in posterior_paths: \n",
    "        args = {\n",
    "            'task': task, \n",
    "            'prior_path': prior_path, \n",
    "            'posterior_path': post,\n",
    "            'bound': bound, \n",
    "            'alpha': alpha,\n",
    "            'sigma': sigma, \n",
    "            'epsilon': epsilon, \n",
    "            'binary': binary,\n",
    "            'n_classifiers': n_classifiers\n",
    "        }\n",
    "        arg_list.append(args)\n",
    "        \n",
    "    return arg_list\n",
    "    \n",
    "def posterior_checkpoints(task, epsilon, alpha, binary=False):\n",
    "    ### Here we do something more intelligent to not have to hardcode the epoch amounts. \n",
    "    ### we parse the filenames and sort them in numerical order and then load the weights\n",
    "    if binary:\n",
    "        base_path=\"posteriors/\"+\"task\"+str(task)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))\n",
    "    else:\n",
    "        base_path=\"posteriors/\"+\"task\"+str(task)+\"/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))\n",
    "        \n",
    "    list1=[]\n",
    "    list2=[]\n",
    "    dirFiles = os.listdir(base_path) #list of directory files\n",
    "    ## remove the ckpt.index and sort so that we get the epochs that are in the directory\n",
    "    for files in dirFiles: #filter out all non jpgs\n",
    "        if '.ckpt.index' in files:\n",
    "            name = re.sub('\\.ckpt.index$', '', files)\n",
    "            ### if it has a one it goes in one list and if it starts with a two it goes in the other\n",
    "            if (name[0]==\"1\"):\n",
    "                list1.append(name)\n",
    "            elif (name[0]==\"2\"):\n",
    "                list2.append(name)\n",
    "                \n",
    "    list1.sort(key=lambda f: int(re.sub('\\D', '', f)))\n",
    "    num_batchweights=len(list1)\n",
    "    list2.sort(key=lambda f: int(re.sub('\\D', '', f)))\n",
    "    list1.extend(list2)\n",
    "    Ws=list1\n",
    "        \n",
    "    path=\"posteriors/\"+\"task\"+str(TASK)+\"/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"/\"#\"{epoch:0d}.ckpt\"\n",
    "    if Binary:\n",
    "        path=\"posteriors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"/\"#\"{epoch:0d}.ckpt\"\n",
    "    \n",
    "    posterior_paths = [os.path.join(path, str(checkpoint)+\".ckpt\") for checkpoint in Ws]\n",
    "    \n",
    "    return posterior_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating over experiments...\n",
      "\n",
      "alpha:0.1\n",
      "  epsilon:0.01\n",
      "    sigma:[3, 2]\n",
      "\n",
      "----------------------------------------\n",
      "Computing bound components for\n",
      "   Prior: priors/task2/Binary/10/prior.ckpt\n",
      "   Posterior: posteriors/task2/Binary/10_10/1_0.ckpt\n",
      "Clearing session...\n",
      "Initializing models...\n",
      "Loading weights...\n",
      "Drawing classifiers...\n",
      "Time spent drawing the classifiers: 0.0200s\n",
      "Calculating errors...\n",
      "Time spent calculating errors: 28.3190s\n",
      "Computing joint errors and disagreements...\n"
     ]
    }
   ],
   "source": [
    "y_target_bin = make_mnist_binary(y_target)\n",
    "y_source_bin = make_mnist_binary(y_source)\n",
    "\n",
    "print('Iterating over experiments...\\n')\n",
    "\n",
    "np.random.seed(seed)\n",
    "for alpha in alphas:\n",
    "    \n",
    "    print(\"alpha:\"+str(alpha))\n",
    "    \n",
    "    if alpha==0:\n",
    "        x_bound=x_source\n",
    "        y_bound=y_source_bin\n",
    "    else:\n",
    "        x_bound, x_prior, y_bound, y_prior = train_test_split(x_source, y_source_bin, test_size=alpha)\n",
    "    for epsilon in epsilons:\n",
    "        print(\"  epsilon:\"+str(epsilon))\n",
    "        for sigma in sigmas:    # @TODO: This loop can be merged into the compute_bound part since models don't depend on sigma\n",
    "            print(\"    sigma:\"+str(sigma))\n",
    "            arg_list = get_job_args(TASK, bound='germain', alpha=alpha, sigma=sigma,\n",
    "                                    epsilon=epsilon, binary=binary, n_classifiers=n_classifiers)\n",
    "            # @TODO: Test\n",
    "            a = arg_list[0]            \n",
    "            compute_bound_parts(a['task'], a['posterior_path'], x_bound, y_bound, x_target, y_target_bin, \n",
    "                          prior_path=a['prior_path'], bound=a['bound'], binary=a['binary'], sigma=a['sigma'], \n",
    "                          epsilon=0.01, n_classifiers=a['n_classifiers'])\n",
    "            \n",
    "               "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
