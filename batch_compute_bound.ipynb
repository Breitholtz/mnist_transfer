{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function \n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import gc, re, copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.keras.layers import deserialize, serialize\n",
    "from tensorflow.python.keras.saving import saving_utils\n",
    "\n",
    "# Project imports \n",
    "from data import mnist_m as mnistm\n",
    "from data import mnist\n",
    "from data.label_shift import label_shift_linear, plot_labeldist, plot_splitbars\n",
    "from data.tasks import load_task\n",
    "from experiments.training import *\n",
    "from experiments.SL_bound import *\n",
    "from experiments.DA_bound import *\n",
    "from util.kl import *\n",
    "from util.misc import *\n",
    "from results.plotting import *\n",
    "\n",
    "# Hyper-parameters\n",
    "seed = 69105\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 10\n",
    "make_plots = False\n",
    "delta=0.05 ## what would this be?   \n",
    "binary=True\n",
    "epsilons=[0.01]\n",
    "alphas=[0.1]#0,0.3]\n",
    "sigmas=[[3,2],[3,3]]\n",
    "\n",
    "TASK = 2\n",
    "\n",
    "project_folder = \"/cephyr/users/frejohk/Alvis/projects/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and structure data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean, variance 0.36348352 70.18035\n",
      "---------------Load MNIST----------------\n",
      "Training set (60000, 32, 32, 3) (60000, 10)\n",
      "Test set (10000, 32, 32, 3) (10000, 10)\n",
      "\n",
      "\n",
      "mean, variance 1.1809415 74.36859\n",
      "---------------Load MNIST-M----------------\n",
      "Training set (60000, 32, 32, 3) (60000, 10)\n",
      "Test set (10000, 32, 32, 3) (10000, 10)\n",
      "[[10.986111, 5.0, 2.999428, 1.99958, 1.399789, 1.0, 0.71435696, 0.5, 0.33346358, 0.20003448], [0.09088036, 0.19987813, 0.33326975, 0.5, 0.7143216, 1.0, 1.4001397, 2.0, 3.0, 5.0025883]]\n"
     ]
    }
   ],
   "source": [
    "x_source, y_source, x_target, y_target = load_task(TASK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_args(task, bound='germain', alpha=0.1, sigma=[3,2], epsilon=[0.01], binary=False):\n",
    " \n",
    "    if binary:\n",
    "        with open('posteriors/'+\"task\"+str(task)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+'/params.txt', 'rb+') as f:\n",
    "            params=f.readlines()\n",
    "        f.close()\n",
    "        prior_path=\"priors/\"+\"task\"+str(task)+\"/Binary/\"+str(int(100*alpha))+\"/prior.ckpt\"\n",
    "        result_path=\"results/\"+\"task\"+str(task)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"\n",
    "    else:\n",
    "        with open('posteriors/'+\"task\"+str(task)+\"/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+'/params.txt', 'rb+') as f:\n",
    "            params=f.readlines()\n",
    "        f.close()\n",
    "        prior_path=\"priors/\"+\"task\"+str(task)+\"/\"+str(int(100*alpha))+\"/prior.ckpt\"\n",
    "        result_path=\"results/\"+\"task\"+str(task)+\"/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"\n",
    "\n",
    "    epsilon=float(params[1]) # @TODO: Superfluous? Isn't this submitted?\n",
    "    epochs_trained=int(params[2]) # @TODO: Unused as far as I can see\n",
    "    print(epochs_trained)\n",
    "     \n",
    "    posterior_paths = posterior_checkpoints(task, epsilon, alpha, binary=binary)\n",
    "    \n",
    "    arg_list = []\n",
    "    for post in posterior_paths: \n",
    "        args = {\n",
    "            'task': task, \n",
    "            'prior_path': prior_path, \n",
    "            'posterior_path': post,\n",
    "            'bound': bound, \n",
    "            'alpha': alpha,\n",
    "            'sigma': sigma, \n",
    "            'epsilon': epsilon, \n",
    "            'binary': binary\n",
    "        }\n",
    "        arg_list.append(args)\n",
    "    return arg_list\n",
    "\n",
    "    \n",
    "def compute_bound_parts(task, posterior_path, x_bound, y_bound, x_target, y_target, alpha=0.1, delta=0.05, \n",
    "                  prior_path=None, bound='germain', binary=False, n_classifiers=4, sigma=[3,3]):\n",
    "\n",
    "    print('Clearing session...')\n",
    "    K.clear_session()\n",
    "    \n",
    "    print('Initializing models...')\n",
    "    if binary:\n",
    "        M_prior = init_MNIST_model_binary()\n",
    "        M_posterior = init_MNIST_model_binary()\n",
    "    else:\n",
    "        M_prior = init_MNIST_model()\n",
    "        M_posterior = init_MNIST_model()                \n",
    "\n",
    "    # @TODO: Are the parameters for optimizer etc necessary when just loading the model?\n",
    "    M_prior.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "                   optimizer=tf.keras.optimizers.SGD(learning_rate=0.003, momentum=0.95),\n",
    "                      metrics=['accuracy'],)\n",
    "    \n",
    "    M_posterior.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "                   optimizer=tf.keras.optimizers.SGD(learning_rate=0.003, momentum=0.95),\n",
    "                      metrics=['accuracy'],)\n",
    "    \n",
    "    ### load the prior weights if there are any\n",
    "    if(binary and alpha != 0):\n",
    "        prior_path=\"priors/\"+\"task\"+str(task)+\"/Binary/\"+str(int(100*alpha))+\"/prior.ckpt\"\n",
    "    elif(alpha != 0):\n",
    "        prior_path=\"priors/\"+\"task\"+str(task)+\"/\"+str(int(100*alpha))+\"/prior.ckpt\"\n",
    "        \n",
    "    print('Loading weights...')\n",
    "    if alpha==0:\n",
    "        ### do nothing, just take the random initialisation\n",
    "        w_a=M_prior.get_weights()\n",
    "    else:\n",
    "        M_prior.load_weights(prior_path)\n",
    "        w_a=M_prior.get_weights()\n",
    "        \n",
    "    # Load posterior weights\n",
    "    M_posterior.load_weights(posterior_path)\n",
    "    w_s=M_posterior.get_weights()\n",
    "    \n",
    "    t = time.time()\n",
    "    \n",
    "    ## do X draws of the posterior, for two separate classifiers\n",
    "    sigma_tmp=sigma\n",
    "    sigma=sigma[0]*10**(-1*sigma[1])\n",
    "    \n",
    "    print('Drawing classifiers...')\n",
    "    w_s_draws = draw_classifier(w_s, sigma, num_classifiers=n_classifiers)\n",
    "    w_s_draws2 = draw_classifier(w_s, sigma, num_classifiers=n_classifiers)\n",
    "    \n",
    "    elapsed = time.time() - t\n",
    "    print('Time spent drawing the classifiers: %.4fs' % elapsed)\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculate train and target errors\n",
    "    \"\"\"\n",
    "    \n",
    "    errorsum=[]\n",
    "    target_errorsum=[]\n",
    "\n",
    "    y_bound = np.array(y_bound)\n",
    "    y_target = np.array(y_target)\n",
    "\n",
    "    ######## in here we should make the results save in a vector for each part to be able to calculate\n",
    "    ######## the standard deviation and be able to get error bars on things.\n",
    "    print('Calculating errors...')\n",
    "    t = time.time()\n",
    "    for h in w_s_draws:\n",
    "        M_posterior.set_weights(h)\n",
    "        errorsum.append((1-M_posterior.evaluate(x_bound,y_bound,verbose=0)[1]))\n",
    "        target_errorsum.append((1-M_posterior.evaluate(x_target,y_target,verbose=0)[1]))\n",
    "\n",
    "    for hprime in w_s_draws2:\n",
    "        M_posterior.set_weights(hprime)\n",
    "        errorsum.append((1-M_posterior.evaluate(x_bound,y_bound,verbose=0)[1]))\n",
    "        target_errorsum.append((1-M_posterior.evaluate(x_target,y_target,verbose=0)[1]))\n",
    "\n",
    "    train_germain = np.mean(errorsum) \n",
    "    target_germain = np.mean(target_errorsum)  \n",
    "    error_std = np.std(errorsum)\n",
    "    target_error_std = np.std(target_errorsum)\n",
    "    elapsed = time.time() - t\n",
    "    print('Time spent calculating errors: %.4fs' % elapsed)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Calculate joint errors\n",
    "    @TODO: This part should be merged with the above. Errors can be readibly computed from the predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    e_ssum=[]\n",
    "    e_tsum=[]\n",
    "    d_txsum=[]\n",
    "    d_sxsum=[]\n",
    "    d_tx_h=0\n",
    "    d_sx_h=0\n",
    "    d_tx_hprime=0\n",
    "    d_sx_hprime=0\n",
    "\n",
    "    t = time.time()\n",
    "\n",
    "    #### Here we just do the four pairs so there is no cross-usage\n",
    "    #### this can be not good for the independence of the values which makes the CI useless\n",
    "\n",
    "    for i, h in enumerate(w_s_draws):\n",
    "        M_posterior.set_weights(h)\n",
    "        d_tx_h=M_posterior.predict(x_target,verbose=0)\n",
    "        d_sx_h=M_posterior.predict(x_bound,verbose=0)\n",
    "        d_sx_h=make_01(d_sx_h)\n",
    "        d_tx_h=make_01(d_tx_h)\n",
    "\n",
    "        hprime=w_s_draws2[i]\n",
    "        M_posterior.set_weights(hprime)\n",
    "        d_tx_hprime=M_posterior.predict(x_target,verbose=0)\n",
    "        d_sx_hprime=M_posterior.predict(x_bound,verbose=0)\n",
    "        d_sx_hprime=make_01(d_sx_hprime)\n",
    "        d_tx_hprime=make_01(d_tx_hprime)\n",
    "\n",
    "        e_ssum.append(joint_error(d_sx_h,d_sx_hprime,y_bound))\n",
    "        d_sxsum=(classifier_disagreement(d_sx_h,d_sx_hprime))\n",
    "        e_tsum=(joint_error(d_tx_h,d_tx_hprime,y_target))\n",
    "        d_txsum=(classifier_disagreement(d_tx_h,d_tx_hprime))\n",
    "\n",
    "    # Means\n",
    "    e_s = np.mean(e_ssum)\n",
    "    d_sx = np.mean(d_sxsum)\n",
    "    e_t = np.mean(e_tsum)\n",
    "    d_tx = np.mean(d_txsum)\n",
    "    \n",
    "    # Stds\n",
    "    e_s_std = np.std(e_ssum)\n",
    "    d_sx_std = np.std(d_sxsum)\n",
    "    e_t_std = np.std(e_tsum)\n",
    "    d_tx_std = np.std(d_txsum)\n",
    "    \n",
    "    elapsed = time.time() - t\n",
    "    print(\"Time spent calculating joint errors and disagreements: \"+str(elapsed)+\"\\n\")    \n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Compute the KL divergence\n",
    "    \"\"\"\n",
    "    t = time.time()\n",
    "    KL = estimate_KL(w_a, w_s, sigma) ## compute the KL\n",
    "\n",
    "    elapsed = time.time() - t\n",
    "    print(\"Time spent calculating KL: \"+str(elapsed)+\"\\n\") \n",
    "    \n",
    "\n",
    "    print(\"Finished calculation of bound parts\")\n",
    "    \n",
    "    \"\"\"\n",
    "    Finish up and store results\n",
    "    \"\"\"\n",
    "    \n",
    "    # Checkpoint corresponds to either update or epoch depending on first part 1_ or 2_ \"\"\"\n",
    "    checkpoint = os.path.splitext(os.path.basename(posterior_path))[0]\n",
    "    \n",
    "    updates = []\n",
    "    if checkpoint[0:2]==\"1_\":\n",
    "            updates = int(checkpoint[2:])\n",
    "    else: \n",
    "        updates = (int(checkpoint[2:])+1)*547 # @TODO: Constant hack\n",
    "       \n",
    "    if Binary:\n",
    "        result_path=\"results/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"+str(sigma_tmp[0])+str(sigma_tmp[1])+'_'+checkpoint\n",
    "    else:\n",
    "        result_path=\"results/\"+\"task\"+str(TASK)+\"/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"+str(sigma_tmp[0])+str(sigma_tmp[1])+'_'+checkpoint\n",
    "        \n",
    "    # Create dir\n",
    "    os.makedirs(result_path, exist_ok=True)\n",
    "        \n",
    "    results=pd.DataFrame({\n",
    "        'Weightupdates': [updates],\n",
    "        'train_germain': [train_germain],\n",
    "        'target_germain': [target_germain],\n",
    "        'KL': [KL],\n",
    "        'e_s': [e_s],\n",
    "        'e_t': [e_t],\n",
    "        'd_tx': [d_tx], \n",
    "        'd_sx': [d_sx],\n",
    "        'error_std': [error_std],\n",
    "        'target_error_std': [target_error_std],\n",
    "        'e_s_std': [e_s_std],\n",
    "        'e_t_std': [e_t_std],\n",
    "        'd_tx_std': [d_tx_std],\n",
    "        'd_sx_std': [d_sx_std]\n",
    "    })\n",
    "   \n",
    "    print('Saving results...')\n",
    "    results.to_pickle(result_path)\n",
    "    print('Done.')\n",
    "   \n",
    "    \"\"\"\n",
    "    The reimaining part only makes sense in the context of a set of snapshots\n",
    "    \n",
    "    # Number of samples \n",
    "    m=len(y_bound)\n",
    "    \n",
    "     # calculate disrho bound\n",
    "    [res,bestparam, boundparts]=grid_search(train_germain,e_s,e_t,d_tx,d_sx,KL,delta,m)\n",
    "    \n",
    "    # calculate beta bound\n",
    "    [res2,bestparam2, boundparts2]=grid_search(train_germain,e_s,e_t,d_tx,d_sx,KL,delta,m,beta_bound=True)            \n",
    "                \n",
    "    results['germain_bound']=res\n",
    "    print(\"Germain bound\"+str(res))\n",
    "    print(\"[a, omega]= \"+str(bestparam))\n",
    "    \n",
    "    Best=np.zeros([len(res),1])\n",
    "    Best[0]=bestparam[0]\n",
    "    Best[1]=bestparam[1]\n",
    "    Best[2]=CLASSIFIERS\n",
    "    #print(Best)\n",
    "    results['bestparam']=Best\n",
    "    results['boundpart1_germain']=boundparts[0]\n",
    "    results['boundpart2_germain']=boundparts[1]\n",
    "    results['boundpart3_germain']=boundparts[2]\n",
    "    results['boundpart4_germain']=boundparts[3]\n",
    "    results['boundpart5_germain']=boundparts[4]\n",
    "    ## beta bound\n",
    "    results['beta_bound']=res2\n",
    "    results['beta_boundpart1']=boundparts2[0]\n",
    "    results['beta_boundpart2']=boundparts2[1]\n",
    "    results['beta_boundpart3']=boundparts2[2]\n",
    "    \n",
    "    fpath = project_folder+'mnist_transfer/'+result_path+\"_results.pkl\"\n",
    "    print('Saving into: %s' % fpath)\n",
    "    \n",
    "    with open(fpath,'wb') as f:\n",
    "        pickle.dump(results,f)\n",
    "    f.close()\n",
    "    return results\n",
    "    \"\"\"\n",
    "    \n",
    "def posterior_checkpoints(task, epsilon, alpha, binary=False):\n",
    "    ### Here we do something more intelligent to not have to hardcode the epoch amounts. \n",
    "    ### we parse the filenames and sort them in numerical order and then load the weights\n",
    "    if binary:\n",
    "        base_path=\"posteriors/\"+\"task\"+str(task)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))\n",
    "    else:\n",
    "        base_path=\"posteriors/\"+\"task\"+str(task)+\"/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))\n",
    "        \n",
    "    list1=[]\n",
    "    list2=[]\n",
    "    dirFiles = os.listdir(base_path) #list of directory files\n",
    "    ## remove the ckpt.index and sort so that we get the epochs that are in the directory\n",
    "    for files in dirFiles: #filter out all non jpgs\n",
    "        if '.ckpt.index' in files:\n",
    "            name = re.sub('\\.ckpt.index$', '', files)\n",
    "            ### if it has a one it goes in one list and if it starts with a two it goes in the other\n",
    "            if (name[0]==\"1\"):\n",
    "                list1.append(name)\n",
    "            elif (name[0]==\"2\"):\n",
    "                list2.append(name)\n",
    "                \n",
    "    list1.sort(key=lambda f: int(re.sub('\\D', '', f)))\n",
    "    num_batchweights=len(list1)\n",
    "    list2.sort(key=lambda f: int(re.sub('\\D', '', f)))\n",
    "    list1.extend(list2)\n",
    "    Ws=list1\n",
    "        \n",
    "    path=\"posteriors/\"+\"task\"+str(TASK)+\"/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"/\"#\"{epoch:0d}.ckpt\"\n",
    "    if Binary:\n",
    "        path=\"posteriors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"/\"#\"{epoch:0d}.ckpt\"\n",
    "    \n",
    "    posterior_paths = [os.path.join(path, str(checkpoint)+\".ckpt\") for checkpoint in Ws]\n",
    "    \n",
    "    return posterior_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating over experiments...\n",
      "\n",
      "alpha:0.1\n",
      "  epsilon:0.01\n",
      "    sigma:[3, 2]\n",
      "14\n",
      "Clearing session...\n",
      "Initializing models...\n",
      "Loading weights...\n",
      "Drawing classifiers...\n",
      "Time spent drawing the classifiers: 0.0367s\n",
      "Calculating errors...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-138-2c442bf4d6ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             compute_bound_parts(a['task'], a['posterior_path'], x_bound, y_bound, x_target, y_target_bin, \n\u001b[0;32m---> 25\u001b[0;31m                           prior_path=a['prior_path'], bound=a['bound'], binary=a['binary'], sigma=a['sigma'])\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-137-e308ffa46aaf>\u001b[0m in \u001b[0;36mcompute_bound_parts\u001b[0;34m(task, posterior_path, x_bound, y_bound, x_target, y_target, alpha, delta, prior_path, bound, binary, n_classifiers, sigma)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mM_posterior\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0merrorsum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mM_posterior\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_bound\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_bound\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mtarget_errorsum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mM_posterior\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_target\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_target\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mhprime\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mw_s_draws2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/Alvis/software/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m   def predict(self,\n",
      "\u001b[0;32m/apps/Alvis/software/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, model, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m         use_multiprocessing=use_multiprocessing, **kwargs)\n\u001b[0m\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m   def predict(self, model, x, batch_size=None, verbose=0, steps=None,\n",
      "\u001b[0;32m/apps/Alvis/software/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[0;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m               \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m               \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m               total_epochs=1)\n\u001b[0m\u001b[1;32m    476\u001b[0m           \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/Alvis/software/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/Alvis/software/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/Alvis/software/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/Alvis/software/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    604\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m/apps/Alvis/software/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/Alvis/software/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/Alvis/software/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/Alvis/software/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/apps/Alvis/software/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/Alvis/software/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mTFE_Py_Execute\u001b[0;34m(ctx, device_name, op_name, inputs, attrs, outputs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mTFE_Py_Execute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pywrap_tensorflow_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFE_Py_Execute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mTFE_Py_ExecuteCancelable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "y_target_bin = make_mnist_binary(y_target)\n",
    "y_source_bin = make_mnist_binary(y_source)\n",
    "\n",
    "print('Iterating over experiments...\\n')\n",
    "\n",
    "np.random.seed(seed)\n",
    "for alpha in alphas:\n",
    "    \n",
    "    print(\"alpha:\"+str(alpha))\n",
    "    \n",
    "    if alpha==0:\n",
    "        x_bound=x_source\n",
    "        y_bound=y_source_bin\n",
    "    else:\n",
    "        x_bound, x_prior, y_bound , y_prior = train_test_split(x_source, y_source_bin, test_size=alpha)\n",
    "    for epsilon in epsilons:\n",
    "        print(\"  epsilon:\"+str(epsilon))\n",
    "        for sigma in sigmas:    # @TODO: This loop can be merged into the compute_bound part since models don't depend on sigma\n",
    "            print(\"    sigma:\"+str(sigma))\n",
    "            arg_list = get_job_args(TASK, bound='germain', alpha=alpha, sigma=sigma,\n",
    "                                    epsilon=epsilon, binary=binary)\n",
    "            # @TODO: Test\n",
    "            a = arg_list[0]            \n",
    "            compute_bound_parts(a['task'], a['posterior_path'], x_bound, y_bound, x_target, y_target_bin, \n",
    "                          prior_path=a['prior_path'], bound=a['bound'], binary=a['binary'], sigma=a['sigma'])\n",
    "            \n",
    "               "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
