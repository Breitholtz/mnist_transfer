{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Keras implementation of NN's which we will look at MNIST with\n",
    "\n",
    "from __future__ import print_function \n",
    "\n",
    "#import scipy\n",
    "#import h5py\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "#svhn_path=\"../Datasets/svhn\"#\"/Home/Adam/Research/Datasets/svhn\"\n",
    "\n",
    "# Hyper-parameters\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 10\n",
    "\n",
    "img_rows, img_cols = 32, 32\n",
    "## where did you put mnist_transfer\n",
    "path_to_root_file=\"/home/adam/Code/\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean, variance 0.36348352 70.18035\n",
      "---------------Load MNIST----------------\n",
      "Training set (60000, 32, 32, 3) (60000, 10)\n",
      "Test set (10000, 32, 32, 3) (10000, 10)\n",
      "\n",
      "\n",
      "mean, variance 1.1809415 74.36859\n",
      "---------------Load MNIST-M----------------\n",
      "Training set (60000, 32, 32, 3) (60000, 10)\n",
      "Test set (10000, 32, 32, 3) (10000, 10)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "-1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3449: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### load module, and also MNIST/MNIST-M\n",
    "from importlib.machinery import SourceFileLoader\n",
    "mymodule = SourceFileLoader('mnistm', path_to_root_file+'mnist_transfer/data/mnist_m.py').load_module()\n",
    "mymodule2 = SourceFileLoader('mnist', path_to_root_file+'mnist_transfer/data/mnist.py').load_module()\n",
    "import mnistm\n",
    "import mnist\n",
    "x_train, y_train, x_test, y_test = mnist.load_mnist()\n",
    "x_train_m, y_train_m, x_test_m, y_test_m = mnistm.load_mnistm(y_train,y_test)\n",
    "import sys\n",
    "sys.exit(-1)\n",
    "### load module\n",
    "mymodule = SourceFileLoader('label_shift', path_to_root_file+'mnist_transfer/data/label_shift.py').load_module()\n",
    "\n",
    "from label_shift import *\n",
    "    \n",
    "######### Here we use the functions from label_shift ################\n",
    "\n",
    "###### Add train and test together and shift the distributions to create source and target distributions\n",
    "### MNIST all data\n",
    "x_full=np.append(x_train,x_test, axis=0)\n",
    "y_full=np.append(y_train,y_test, axis=0)\n",
    "### MNIST-M all data\n",
    "x_full_m=np.append(x_train_m,x_test_m, axis=0)\n",
    "y_full_m=np.append(y_train_m,y_test_m, axis=0)\n",
    "#x_shift,y_shift,x_shift_target,y_shift_target =label_shift(x_train,y_train,1/2,7)\n",
    "x_shift, y_shift, x_shift_target, y_shift_target =label_shift_linear(x_full,y_full,1/12,[0,1,2,3,4,5,6,7,8,9])\n",
    "x_shift_m, y_shift_m,x_shift_target_m, y_shift_target_m=label_shift_linear(x_full_m,y_full_m,1/12,[0,1,2,3,4,5,6,7,8,9],decreasing=False)\n",
    "\n",
    "plot_labeldist([0,1,2,3,4,5,6,7,8,9],y_shift_target,\"shifted, target\")\n",
    "plot_labeldist([0,1,2,3,4,5,6,7,8,9],y_shift,\"shifted, source\")\n",
    "plot_splitbars([0,1,2,3,4,5,6,7,8,9],y_shift,y_shift_m,\"MNIST, source\",\"MNIST-M, source\")\n",
    "plot_splitbars([0,1,2,3,4,5,6,7,8,9],y_shift_target,y_shift_target_m,\"MNIST, target\",\"MNIST-M, target\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "def dumb_func(args):\n",
    "    import tensorflow as tf\n",
    "    sum=0\n",
    "    for i in range(10000000):\n",
    "        sum+=i\n",
    "    print(sum)\n",
    "    #print(len(args))\n",
    "def function():    \n",
    "    p=Pool(5)\n",
    "    r=[1,2,3,4,5]#range(5)\n",
    "    if __name__=='__main__':\n",
    "        p.map(dumb_func,r)\n",
    "        #p.map(dumb_func,1)\n",
    "        p.close()\n",
    "        p.join()\n",
    "function()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_resnet_model(Binary=True):\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense\n",
    "    from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "    model = Sequential()\n",
    "    ## no top as we want 10 classes for mnist etc\n",
    "    model.add(ResNet50(include_top = False, pooling = 'avg', weights = None))\n",
    "    if Binary:\n",
    "        model.add(Dense(2, activation = 'softmax'))\n",
    "    else:\n",
    "        model.add(Dense(10, activation = 'softmax'))\n",
    "    return model\n",
    "\n",
    "# model=init_resnet_model(Binary=False)\n",
    "# model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# model.fit(x_full, y_full,\n",
    "#            batch_size=batch_size,\n",
    "#            epochs=10,\n",
    "#            verbose=1,\n",
    "#            validation_data=(x_full_m, y_full_m),\n",
    "#           )\n",
    "\n",
    "\n",
    "\n",
    "def init_FC_model(Binary=True):\n",
    "    ### same as Dziugaite, to compare with rivasplata et al. in their case\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1024,input_shape=(32,32,3), activation = 'relu'))\n",
    "    model.add(Dense(600, activation = 'relu'))\n",
    "    model.add(Dense(600, activation = 'relu'))\n",
    "    if Binary:\n",
    "        model.add(Dense(2, activation = 'softmax'))\n",
    "    else:\n",
    "        model.add(Dense(10, activation = 'softmax'))\n",
    "    return model\n",
    "model=init_FC_model(Binary=False)\n",
    "model.compile(optimizer=keras.optimizers.SGD(learning_rate=0.003, momentum=0.95), loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])\n",
    "#print(x_full.shape)\n",
    "#keras.utils.plot_model(model)\n",
    "model.fit(x_full, y_full,batch_size=256,epochs=10,verbose=1,validation_data=(x_full_m, y_full_m))\n",
    "#model.score(x_full_m,y_full_m)\n",
    "def load_usps():\n",
    "    import gzip\n",
    "    import pickle\n",
    "    ## copied and changed from https://github.com/JingWang18/Discriminative-Feature-Alignment/\n",
    "    f = gzip.open('/home/adam/Code/Datasets/usps/usps_28x28.pkl', 'rb')\n",
    "    data_set = pickle.load(f, encoding=\"bytes\")\n",
    "    f.close()\n",
    "    img_train = data_set[0][0]\n",
    "    label_train = data_set[0][1]\n",
    "    img_test = data_set[1][0]\n",
    "    label_test = data_set[1][1]\n",
    "    ### do we need this??\n",
    "    #inds = np.random.permutation(img_train.shape[0])\n",
    "  \n",
    "    #img_train = img_train[inds][:6562]\n",
    "    #label_train = label_train[inds][:6562]\n",
    "    \n",
    "    img_train = img_train * 255\n",
    "    img_test = img_test * 255\n",
    "    img_train = img_train.reshape((img_train.shape[0], 28, 28, 1))\n",
    "    img_test = img_test.reshape((img_test.shape[0], 28, 28, 1))\n",
    "\n",
    "    #### test this part!!!\n",
    "    img_train = np.pad(img_train,((0,0),(2,2),(2,2),(0,0))) #padding to make images 32x32 and not 28x28\n",
    "    img_test = np.pad(img_test,((0,0),(2,2),(2,2),(0,0))) \n",
    "    \n",
    "    ## normalising to unit variance\n",
    "    sigma=np.std(img_train)\n",
    "    img_train /= sigma \n",
    "    img_test /= sigma\n",
    "\n",
    "    ## mean subtraction\n",
    "    mu=np.mean(img_train)\n",
    "    img_train -= mu\n",
    "    img_test -= mu\n",
    "    print('mean, variance', mu, sigma)\n",
    "    \n",
    "    ## expand to (N,32,32,3) so that we can compare the two datasets\n",
    " \n",
    "    img_train=np.concatenate((img_train,img_train,img_train),axis=3)\n",
    "    img_test=np.concatenate((img_test,img_test,img_test),axis=3) \n",
    "    print(\"---------------Load USPS----------------\")\n",
    "    print('Training set', img_train.shape, label_train.shape)\n",
    "    #print('Validation set', x_val.shape, y_val.shape)\n",
    "    print('Test set', img_test.shape, label_test.shape)\n",
    "    print(\"\\n\")\n",
    "    return img_train, label_train, img_test, label_test\n",
    "\n",
    "#img_train, label_train, img_test, label_test = load_usps()\n",
    "\n",
    "def train_and_eval_LR(x_train,y_train,x_test,y_test):\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    LR = LogisticRegression()\n",
    "    LR.fit(x_train,y_train)\n",
    "    score = LR.score(x_test, y_test)\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10.986111, 5.0, 2.999428, 1.99958, 1.399789, 1.0, 0.71435696, 0.5, 0.33346358, 0.20003448], [0.09088036, 0.19987813, 0.33326975, 0.5, 0.7143216, 1.0, 1.4001397, 2.0, 3.0, 5.0025883]]\n"
     ]
    }
   ],
   "source": [
    "##### Add the label shifted datasets to each other creating the source and target domain for task 2\n",
    "\n",
    "##### calculate the label densities here\n",
    "densities=[]\n",
    "densities.append(np.sum(y_shift,axis=0))\n",
    "densities.append(np.sum(y_shift_m,axis=0))\n",
    "densities.append(np.sum(y_shift_target,axis=0))\n",
    "densities.append(np.sum(y_shift_target_m,axis=0))\n",
    "# mnist source, mnist-m source, mnist target,  mnist-m target\n",
    "#print(densities)\n",
    "TASK=2\n",
    "if TASK==1:\n",
    "    ###### label density shifted mnist\n",
    "    x_source=x_shift\n",
    "    y_source=y_shift\n",
    "    x_target=x_shift_target\n",
    "    y_target=y_shift_target\n",
    "elif TASK==2:\n",
    "    #### MIXED MNIST and MNIST-m\n",
    "    L=len(densities[0])\n",
    "    interdomain_densities = [[] for x in range(2)]\n",
    "    for i in range(L):\n",
    "        ## all densities are # in mnist over # in mnist-m\n",
    "        interdomain_densities[0].append(densities[0][i]/densities[1][i])\n",
    "        interdomain_densities[1].append(densities[2][i]/densities[3][i])\n",
    "    print(interdomain_densities)\n",
    "    x_source=np.append(x_shift,x_shift_m, axis=0)\n",
    "    y_source=np.append(y_shift,y_shift_m, axis=0)\n",
    "    x_target=np.append(x_shift_target,x_shift_target_m, axis=0)\n",
    "    y_target=np.append(y_shift_target,y_shift_target_m, axis=0)\n",
    "elif TASK==3:\n",
    "    #### MNIST -> MNIST-m\n",
    "    x_source=x_full\n",
    "    y_source=y_full\n",
    "    x_target=x_full_m\n",
    "    y_target=y_full_m\n",
    "elif TASK==4:\n",
    "    #### MNIST->USPS\n",
    "    x_source=x_full\n",
    "    y_source=y_full\n",
    "    x_target=x_usps\n",
    "    y_target=y_usps\n",
    "elif TASK==5:\n",
    "    #### MNIST -> SVHN\n",
    "    x_source=x_full\n",
    "    y_source=y_full\n",
    "    x_target=x_svhn\n",
    "    y_target=y_svhn\n",
    "elif TASK==6:\n",
    "    x_source=x_chexpert\n",
    "    y_source=y_chexpert\n",
    "    x_source=x_chest14\n",
    "    y_source=y_chest14\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## testing images and such\n",
    "\n",
    "\n",
    "\n",
    "#print(X_test)\n",
    "#print(\"----------------------------------------------------\")\n",
    "#print(x_test)\n",
    "\n",
    "#print(make_mnist_binary(y_train))\n",
    "#plt.imshow(x_test_m[605]) \n",
    "#print(x_test_m[303])\n",
    "#print(y_test_m[605])\n",
    "#plt.imshow(x_test[605]) \n",
    "#print(x_test[303])\n",
    "#print(y_test[605])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "def mmd_rbf_linear(x,y,sigma_square):\n",
    "    from sklearn import metrics\n",
    "    \"\"\"\n",
    "    Here we want to compute the unbiased estimate of the MMD in linear time using the formula\n",
    "    MMD^2_k(p,q)=2/n_s for i=1 to n_s/2 g_k(z_i)\n",
    "    ,where z_i=(x^s_2i-1,x^s_2i,x^t_2i-1,x^t_2i) and\n",
    "    g_k(z_i)= k(x^s_2i-1,x^s_2i)+k(x^t_2i-1,x^t_2i)-k(x^s_2i-1,x^t_2i)-k(x^s_2i,x^t_2i-1)\n",
    "    \"\"\"\n",
    "    \n",
    "    n_s=len(x)\n",
    "    n=int(np.floor(n_s/2.0))\n",
    "    \n",
    "    ### make the data into a form which we can compute with, i.e on Nx1 vector form\n",
    "    if x.ndim==1 or y.ndim==1:\n",
    "        x=x.reshape(-1, 1)\n",
    "        y=y.reshape(-1, 1)\n",
    "    \n",
    "    ##take out odd and even entries for the two vectors\n",
    "    x_even=x[:n_s:2]\n",
    "    x_odd=x[1:n_s:2]\n",
    "    y_even=y[:n_s:2]\n",
    "    y_odd=y[1:n_s:2]\n",
    "    \n",
    "    A=[]\n",
    "    B=[]\n",
    "    C=[]\n",
    "    D=[]\n",
    "    \n",
    "    \n",
    "    #### here we should do pairwise computation of the one off diagonal\n",
    "    for i in range(n_s-1):\n",
    "            A.append(kernel_scalar(x[i+1],x[i],sigma_square)[0])\n",
    "            B.append(kernel_scalar(y[i+1],y[i],sigma_square)[0])\n",
    "            C.append(-1*kernel_scalar(x[i],y[i+1],sigma_square)[0])\n",
    "            D.append(-1*kernel_scalar(y[i+1],x[i],sigma_square)[0])\n",
    "    A=np.array(A)\n",
    "    B=np.array(B)\n",
    "    C=np.array(C)\n",
    "    D=np.array(D)\n",
    "    #*(1/n) not needed?\n",
    "\n",
    "    #print(\"A=\"+str(A))\n",
    "    #print(\"B=\"+str(B))\n",
    "    #print(\"C=\"+str(C))\n",
    "    #print(\"D=\"+str(D))\n",
    "    \n",
    "    return (A+B+C+D).mean()\n",
    "def mmd_rbf(x,y,sigma_square):\n",
    "    from sklearn import metrics\n",
    "    #### if needed for one dimensional data\n",
    "    if x.ndim==1 or y.ndim==1:\n",
    "        x=x.reshape(-1, 1)\n",
    "        y=y.reshape(-1, 1)\n",
    "    XX = metrics.pairwise.rbf_kernel(x, x, 1/sigma_square)\n",
    "    YY = metrics.pairwise.rbf_kernel(y, y, 1/sigma_square)\n",
    "    XY = metrics.pairwise.rbf_kernel(x, y, 1/sigma_square)\n",
    "    A=np.diag(XX,1)\n",
    "    B=np.diag(YY,1)\n",
    "    C=-np.diag(XY,1)\n",
    "    D=-np.diag(XY,1)\n",
    "    #print(\"A=\"+str(A))\n",
    "    #print(\"B=\"+str(B))\n",
    "    #print(\"C=\"+str(C))\n",
    "    #print(\"D=\"+str(D))\n",
    "    \n",
    "    return (np.diag(XX,1)+np.diag(YY,1)-np.diag(XY,1)-np.diag(XY,1)).mean()#XX.mean() + YY.mean() - 2 * XY.mean()\n",
    "\n",
    "  \n",
    "def kernel_scalar(x,y,sigma_square):\n",
    "    return np.exp(-(1/(sigma_square)) * ((x - y) ** 2))\n",
    "def kernel(x,y,sigma_square):\n",
    "    return np.exp(-(1/(sigma_square)) * ((x - y) ** 2).sum(axis=1))\n",
    "def kernel2(x,y,sigma_square):\n",
    "    return np.exp(-np.sum(np.square(x-y),axis=1)/(sigma_square))\n",
    "'''\n",
    "#Test cases\n",
    "A=np.ones(4)\n",
    "B=np.linspace(1,4,4)\n",
    "print(mmd_rbf_linear(A,B,1))\n",
    "print(mmd_rbf(A.reshape(-1, 1),B.reshape(-1, 1),1))\n",
    "print(\"------------------------------\")\n",
    "A=np.ones(9)\n",
    "B=np.ones(9)+ [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "print(mmd_rbf_linear(A,B,1))\n",
    "print(mmd_rbf(A.reshape(-1, 1),B.reshape(-1, 1),1))\n",
    "\n",
    "\n",
    "#### test with normal dist. data we should see that the \n",
    "#### normal dist should be centered at about 0.17-0.2 on the x-axis\n",
    "MMDsq=[]\n",
    "MMDsq2=[]\n",
    "for i in range(2000):\n",
    "    samples = np.random.normal(0, 1, 200);\n",
    "    samples2= np.random.normal(0,3*np.sqrt(2), 200);\n",
    "    #MMDsq.append(mmd_rbf(mnist_flat,mnistm_flat,0.5))\n",
    "    #mmds=[]\n",
    "    #for sigma in sigmas:\n",
    "            #mmds.append(mmd_rbf_linear(samples,samples2,sigma))\n",
    "    MMDsq.append(mmd_rbf_linear(samples,samples2,0.5))\n",
    "    MMDsq2.append(mmd_rbf(samples,samples2,0.5))\n",
    "    #MMDsq.append(np.sum(mmds))\n",
    "print(\"-------------------------------------------------------------\")\n",
    "\n",
    "print(np.mean(MMDsq))\n",
    "print(np.mean(MMDsq2))\n",
    "plt.hist(MMDsq,bins=20)\n",
    "\n",
    "plt.show()\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_images(x_train,x_train_m):\n",
    "    ### flatten images into vectors to be able to compute MMD\n",
    "    mnist_flat=[None]*len(x_train)\n",
    "    mnistm_flat=[None]*len(x_train)\n",
    "    for i in range(len(x_train)):\n",
    "        mnist_flat[i]=tf.reshape(x_train[i],[-1])\n",
    "        mnistm_flat[i]=tf.reshape(x_train_m[i],[-1])\n",
    "    #print(np.array(mnist_flat).shape)\n",
    "    #print(np.array(mnistm_flat).shape)\n",
    "    return np.array(mnist_flat), np.array(mnistm_flat)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## flatten the mnist and mnist-m images and use the MMD to compute a metric between them\n",
    "mnist_flat, mnistm_flat=flatten_images(x_train,x_train_m)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### do this for a range of sigma and sum the results for an estimation of the true MMD\n",
    "\n",
    "sigmas = [\n",
    "      1e-6, 1e-5, \n",
    "      1e-4, 1e-3, 1e-2, 1e-1, 1, 5, 10, 15, 20, 25, 30, 35, 100,\n",
    "      1e3, 1e4, 1e5, 1e6\n",
    "  ]\n",
    "\n",
    "mmdlist=[]\n",
    "\n",
    "for sigma in sigmas:\n",
    "    mmdlist.append(mmd_rbf(mnist_flat[:1000],mnistm_flat[:1000],sigma))\n",
    "print(mmdlist)\n",
    "finalmmd=np.sqrt(np.mean(mmdlist))\n",
    "print(finalmmd)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "### load module\n",
    "mymodule = SourceFileLoader('train', path_to_root_file+'mnist_transfer/experiments/training.py').load_module()\n",
    "mymodule2 = SourceFileLoader('kl', path_to_root_file+'mnist_transfer/util/kl.py').load_module()\n",
    "mymodule3 = SourceFileLoader('util', path_to_root_file+'mnist_transfer/util/misc.py').load_module()\n",
    "mymodule4 = SourceFileLoader('SL', path_to_root_file+'mnist_transfer/experiments/SL_bound.py').load_module()\n",
    "from kl import *\n",
    "from train import *\n",
    "from util import *\n",
    "from SL import *\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sys\n",
    "import gc\n",
    "import re\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "def unpack(model, training_config, weights):\n",
    "    from tensorflow.python.keras.layers import deserialize, serialize\n",
    "    from tensorflow.python.keras.saving import saving_utils\n",
    "    restored_model = deserialize(model)\n",
    "    if training_config is not None:\n",
    "        restored_model.compile(\n",
    "            **saving_utils.compile_args_from_training_config(\n",
    "                training_config\n",
    "            )\n",
    "        )\n",
    "    restored_model.set_weights(weights)\n",
    "    return restored_model\n",
    "\n",
    "# Hotfix function\n",
    "def make_keras_picklable():\n",
    "    from tensorflow.keras.models import Model\n",
    "    from tensorflow.python.keras.layers import deserialize, serialize\n",
    "    from tensorflow.python.keras.saving import saving_utils\n",
    "    def __reduce__(self):\n",
    "        model_metadata = saving_utils.model_metadata(self)\n",
    "        training_config = model_metadata.get(\"training_config\", None)\n",
    "        model = serialize(self)\n",
    "        weights = self.get_weights()\n",
    "        return (unpack, (model, training_config, weights))\n",
    "\n",
    "    cls = Model\n",
    "    cls.__reduce__ = __reduce__\n",
    "\n",
    "\n",
    "'''\n",
    "def imap_unordered_bar(func, args, n_processes = 2):\n",
    "    p = Pool(n_processes)\n",
    "    res_list = []\n",
    "    #with tqdm(total = len(args)) as pbar:\n",
    "        #for i, res in enumerate(p.imap_unordered(func, args)):\n",
    "            #pbar.update()\n",
    "            #res_list.append(res)\n",
    "    #pbar.close()\n",
    "    p.close()\n",
    "    p.join()\n",
    "    return #res_list\n",
    "'''\n",
    "def read_weights(model,w_a,x_bound,y_bound,x_target,y_target,sigma,epsilon,alpha,Binary=False,Task=TASK):\n",
    "    batch_size=128\n",
    "    batches_per_epoch=np.ceil(len(y_target)/batch_size) ## should be 547\n",
    "    epoch=1\n",
    "    \n",
    "    # Run the function to fix pickling issue\n",
    "    make_keras_picklable()\n",
    "    \n",
    "    \n",
    "    sigma=sigma[0]*10**(-1*sigma[1])    \n",
    "    \n",
    "    ### Here we do something more intelligent to not have to hardcode the epoch amounts. \n",
    "    ### we parse the filenames and sort them in numerical order and then load the weights\n",
    "    if Binary:\n",
    "        path=\"posteriors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))\n",
    "    else:\n",
    "        path=\"posteriors/\"+\"task\"+str(TASK)+\"/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))\n",
    "        #epochs_trained\n",
    "    #epochs = [] #list of \n",
    "    list1=[]\n",
    "    list2=[]\n",
    "    dirFiles = os.listdir(path) #list of directory files\n",
    "    ## remove the ckpt.index and sort so that we get the epochs that are in the directory\n",
    "    for files in dirFiles: #filter out all non checkpoints\n",
    "        if '.ckpt.index' in files:\n",
    "            name = re.sub('\\.ckpt.index$', '', files)\n",
    "            ### if it has a one it goes in one list and if it starts with a two it goes in the other\n",
    "            if (name[0]==\"1\"):\n",
    "                list1.append(name)\n",
    "            elif (name[0]==\"2\"):\n",
    "                list2.append(name)\n",
    "            #epochs.append(name)\n",
    "    #epochs.sort(key=lambda f: int(re.sub('\\D', '', f)))\n",
    "    list1.sort(key=lambda f: int(re.sub('\\D', '', f)))\n",
    "    num_batchweights=len(list1)\n",
    "    list2.sort(key=lambda f: int(re.sub('\\D', '', f)))\n",
    "    list1.extend(list2)\n",
    "    Ws=list1 ## vector of checkpoint filenames\n",
    "    \n",
    "    weight_updates=[]\n",
    "    for i in Ws:\n",
    "        #print(i)\n",
    "        if i[0]==\"1\":\n",
    "            if i[1]==\"_\":\n",
    "                weight_updates.append(int(i[2:]))\n",
    "    for i in list2:\n",
    "        weight_updates.append((int(i[2:])+1)*batches_per_epoch)\n",
    "   \n",
    "    ### load the model and the weights\n",
    "    N_checkpoints=len(Ws)\n",
    "    KLs=np.zeros(N_checkpoints)\n",
    "    errors=np.zeros(N_checkpoints)\n",
    "    targeterrors=np.zeros(N_checkpoints)\n",
    "    epochs=np.zeros(N_checkpoints)\n",
    "#     for checkpoint in Ws:\n",
    "    \n",
    "    #### here we should pass all the checkpoints to different processes and evaluate on the dataset\n",
    "    args=[]\n",
    "    #for i in range(N_checkpoints):\n",
    "    #for i in range(2):\n",
    "        #args.append(nnp.array(i,Ws[i],path,KLs, errors,targeterrors,epochs,model,w_a,copy.deepcopy(x_bound),copy.deepcopy(y_bound),copy.deepcopy(x_target),copy.deepcopy(y_target),sigma,epsilon,alpha,Binary,TASK))\n",
    "        #args.append(np.array([i,Ws[i],path,KLs, errors,targeterrors,epochs,model,w_a,x_bound,y_bound,x_target,y_target,sigma,epsilon,alpha,Binary,TASK]))\n",
    "    args.append([Ws[0],x_bound,y_bound])   \n",
    "    args.append([Ws[1],copy.deepcopy(x_bound),copy.deepcopy(y_bound)])\n",
    "    args.append([Ws[2],copy.deepcopy(x_bound),copy.deepcopy(y_bound)])   \n",
    "    args.append([Ws[3],copy.deepcopy(x_bound),copy.deepcopy(y_bound)])\n",
    "    p = Pool(processes = 3)\n",
    "    print(\"could deep copy\")\n",
    "    #for arg in args:\n",
    "    if __name__ == '__main__':\n",
    "        p.imap(dumb_func,args)\n",
    "        p.close()\n",
    "        p.join()\n",
    "\n",
    "    #for arg in args:  \n",
    "        #[KLs,errors,targeterrors] = p.imap(parallel_method, (arg,))\n",
    "    \n",
    "    #for i in range(len(args)):\n",
    "        \n",
    "    #p = Pool(processes = 4)\n",
    "    #p.map(parallel_method, (args))\n",
    "    #p.close()\n",
    "    #p.join() \n",
    "    #parallel_method(*args)#path,KLs, errors,targeterrors,epochs,model,w_a,x_bound,y_bound,x_target,y_target,sigma,epsilon,alpha,Binary=Binary,Task=TASK)\n",
    "    #imap_unordered_bar(parallel_method, args)\n",
    "\n",
    "    #print(\"we made it here!!!!\")\n",
    "    print(KLs)\n",
    "    print(errors)\n",
    "    print(targeterrors)\n",
    "    sys.exit(-1)\n",
    "    \n",
    "    return KLs,errors,targeterrors,Ws,Xvector\n",
    "def dumb_func(args):\n",
    "    #print(args)\n",
    "    init_tf()\n",
    "    \n",
    "    \n",
    "    print(\"!!!!\")\n",
    "    model=init_MNIST_model_binary()\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.003, momentum=0.95),metrics=['accuracy'])\n",
    "    print(\"Model compiled\")\n",
    "    path=\"posteriors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))\n",
    "    model.load_weights(path+\"/\"+str(checkpoint_name)+\".ckpt\").expect_partial()\n",
    "    print(w_s=model.get_weights())\n",
    "    #model.evaluate(args[1],args[2])\n",
    "    sum=0\n",
    "    for i in range(1000000):\n",
    "        sum+=i\n",
    "    print(sum)\n",
    "    print(len(args[0]))\n",
    "def init_tf():\n",
    "    ### making sure that we have the GPU to work on\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    #logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    if gpus:\n",
    "      # Restrict TensorFlow to only allocate 4GB of memory on the first GPU\n",
    "      # I do not know why I have to do this but gpu does not work otherwise.\n",
    "        try:\n",
    "            tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)])\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        except RuntimeError as e:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "            print(e)\n",
    "\n",
    "def parallel_method(index,checkpoint_name,path,KLs, errors,targeterrors,epochs,model,w_a,x_bound,y_bound,x_target,y_target,sigma,epsilon,alpha,Binary=False,Task=TASK):\n",
    "   \n",
    "    import tensorflow as tf\n",
    "    import keras as keras\n",
    "    import os\n",
    "    init_tf()\n",
    "            \n",
    "    if Binary:\n",
    "        model=init_MNIST_model_binary()\n",
    "        model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "        optimizer=keras.optimizers.SGD(learning_rate=0.003, momentum=0.95),\n",
    "                  metrics=['accuracy'],)\n",
    "    else:\n",
    "        model=init_MNIST_model()\n",
    "        model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "        optimizer=keras.optimizers.SGD(learning_rate=0.003, momentum=0.95),\n",
    "                  metrics=['accuracy'],)\n",
    "    model.load_weights(path+\"/\"+str(checkpoint_name)+\".ckpt\").expect_partial()\n",
    "    w_s=model.get_weights()\n",
    "\n",
    "    ##### draw classifiers that are samples from the gaussian dists which define our posterior\n",
    "\n",
    "    t = time.time()\n",
    "    ## do some draws of the posterior\n",
    "    w_s_draws=draw_classifier(w_s,sigma,num_classifiers=2)\n",
    "\n",
    "    elapsed = time.time() - t\n",
    "    print(\"Time spent drawing the classifiers: \"+str(elapsed)+\"\\n\")\n",
    "\n",
    "\n",
    "    ###### for each pair of drawn prior and posterior we calculate the necessary parts of the bound \n",
    "    ###### and then average the result and return that\n",
    "    k=0\n",
    "    errorsum=0\n",
    "    target_errorsum=0\n",
    "    KLsum=0\n",
    "    t = time.time()\n",
    "    for i in w_s_draws:\n",
    "        model.set_weights(i)\n",
    "        errorsum+=((1-model.evaluate(x_bound,y_bound,verbose=0,workers=1)[1]))\n",
    "        target_errorsum+=((1-model.evaluate(x_target,y_target,verbose=0,workers=1)[1]))\n",
    "        \n",
    "    errorsum/=len(w_s_draws)\n",
    "    target_errorsum/=len(w_s_draws)\n",
    "    KLsum=estimate_KL(w_a,w_s,sigma)\n",
    "    elapsed = time.time() - t\n",
    "    print(\"Time spent calculating the errors: \"+str(elapsed)+\"\\n\")\n",
    "    print(\"Iteration done with sigma:\"+str(sigma)+\" and epsilon:\"+str(epsilon))\n",
    "#     KLs.append(KLsum)\n",
    "#     targeterrors.append(target_errorsum)\n",
    "#     errors.append(errorsum)\n",
    "    KLs[index]=KLsum\n",
    "    targeterrors[index]=target_errorsum\n",
    "    errors[index]=errorsum\n",
    "    ## this is needed because keras is bad and leaks memory; Or I am dumb and cannot see what I am doing wrong\n",
    "    del model\n",
    "    _ = gc.collect()\n",
    "    return KLs,errors,targeterrors\n",
    "    \n",
    "## custom callback to terminate training at some specific value of a metric\n",
    "    \n",
    "class stop_callback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, monitor='accuracy', value=0.001, verbose=0):\n",
    "        super(tf.keras.callbacks.Callback, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.value = value\n",
    "        self.verbose = verbose\n",
    "    def on_epoch_end(self, epoch, logs={}): \n",
    "        if(logs.get('accuracy')> self.value): # select the accuracy\n",
    "            print(\"\\n !!! training error threshold reached, no further training !!!\")\n",
    "            self.model.stop_training = True\n",
    "            \n",
    "class fast_checkpoints(tf.keras.callbacks.Callback):\n",
    "    def __init__(self,checkpoint_path,save_freq):\n",
    "        super(tf.keras.callbacks.Callback, self).__init__()\n",
    "        self.save_freq=save_freq\n",
    "        self.filepath=checkpoint_path\n",
    "        self.verbose=1\n",
    "        self.save_best_only=False\n",
    "        self.save_weights_only=True\n",
    "    def on_train_batch_begin(self, batch, epoch, logs=None):\n",
    "         if batch%self.save_freq==0:\n",
    "            self.model.save_weights(self.filepath+\"/1_\"+str(batch)+\".ckpt\")\n",
    "            print(\"\\n Saved weights at the start of batch\"+str(batch)+\"\\n\")\n",
    "            self.model.save_weights(self.filepath+\"/1_\"+str(batch)+\".ckpt\")\n",
    "            \n",
    "def train_posterior(alpha,x_train,y_train,prior_weights=None,x_test=[],y_test=[],save=True,epsilon=0.01,Task=2,Binary=False):\n",
    "        \n",
    "        TASK=Task\n",
    "        batch_size=128\n",
    "        \n",
    "        ### x_test should be the whole of S for early stopping purposes\n",
    "    \n",
    "        checkpoint_path = \"posteriors/\"+\"task\"+str(TASK)+\"/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))\n",
    "        if Binary:\n",
    "            checkpoint_path = \"posteriors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))\n",
    "        \n",
    "        \n",
    "        # Create a callback that saves the model's weights every epoch\n",
    "        cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        save_freq=547,   ### 547 = ceiling(70000/128) i.e training set for MNIST/MNIST-M,\n",
    "        filepath=checkpoint_path+\"/2_{epoch:0d}.ckpt\", \n",
    "        verbose=1,\n",
    "        save_best_only=False,\n",
    "        save_weights_only=True,\n",
    "            ## tune when to save as needed for plots\n",
    "        )\n",
    "        fast_cp_callback =fast_checkpoints(checkpoint_path,45)\n",
    "        stopping_callback=stop_callback(monitor='val_acc',value=1-epsilon)\n",
    "    \n",
    "        if Binary:\n",
    "            M=init_MNIST_model_binary()\n",
    "        else:\n",
    "            M=init_MNIST_model()\n",
    "\n",
    "        \n",
    "            \n",
    "        ## choose loss function, optimiser etc. and train\n",
    "        \n",
    "        M.compile(loss=keras.losses.categorical_crossentropy,\n",
    "               optimizer=keras.optimizers.SGD(learning_rate=0.003, momentum=0.95),\n",
    "                      metrics=['accuracy'],)\n",
    "        ### load the prior weights\n",
    "        if prior_weights is not None:\n",
    "            M.set_weights(prior_weights)\n",
    "        elif(alpha==0):\n",
    "            ### save the rand. init as the prior\n",
    "            prior_path=\"priors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(100*alpha))+\"/prior.ckpt\"\n",
    "            M.save_weights(prior_path)\n",
    "        else:\n",
    "            if Binary:\n",
    "                prior_path=\"priors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(100*alpha))+\"/prior.ckpt\"\n",
    "                M.load_weights(prior_path).expect_partial()\n",
    "            else:\n",
    "                prior_path=\"priors/\"+\"task\"+str(TASK)+\"/\"+str(int(100*alpha))+\"/prior.ckpt\"\n",
    "                M.load_weights(prior_path).expect_partial()\n",
    "        \n",
    "    \n",
    "        if save:\n",
    "            CALLBACK=[fast_cp_callback,stopping_callback]\n",
    "        else:\n",
    "            CALLBACK=[stopping_callback]\n",
    "        ### train for one epoch with more checkpoints to be able to plot more there\n",
    "        fit_info = M.fit(x_train, y_train,\n",
    "           batch_size=batch_size,\n",
    "           epochs=1, \n",
    "           callbacks=CALLBACK,\n",
    "           validation_data=(x_test, y_test),\n",
    "           verbose=1,\n",
    "                        )\n",
    "        \n",
    "        \n",
    "        if save:\n",
    "            CALLBACK=[cp_callback,stopping_callback]\n",
    "        else:\n",
    "            CALLBACK=[stopping_callback]\n",
    "            \n",
    "        fit_info = M.fit(x_train, y_train,\n",
    "           batch_size=batch_size,\n",
    "           epochs=2000, # we should have done early stopping before this completes\n",
    "           callbacks=CALLBACK,\n",
    "           validation_data=(x_test, y_test),\n",
    "           verbose=1,\n",
    "                        )\n",
    "        \n",
    "        \n",
    "         #### save the last posterior weights to disk\n",
    "        epochs_trained=len(fit_info.history['loss'])\n",
    "        if save:\n",
    "            M.save_weights(checkpoint_path+\"/2_\"+str(epochs_trained)) ###### check if we need this; TODO!!!!!!\n",
    "            \n",
    "        #### save textfile with parameters, i.e. alpha ,epochs trained and epsilon\n",
    "        if Binary:\n",
    "            with open('posteriors/'+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+'/params.txt', 'w') as f:\n",
    "                f.write('\\n'.join([str(alpha), str(epsilon), str(epochs_trained)]))     \n",
    "            f.close()\n",
    "        else:\n",
    "            with open('posteriors/'+\"task\"+str(TASK)+\"/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+'/params.txt', 'w') as f:\n",
    "                f.write('\\n'.join([str(alpha), str(epsilon), str(epochs_trained)]))     \n",
    "            f.close()\n",
    "        W=M.get_weights()\n",
    "        return W\n",
    "    \n",
    "def train_prior(alpha,total_epochs,x_train=[],y_train=[],x_target=[],y_target=[],save=True,Task=2,Binary=False):\n",
    "    TASK=Task\n",
    "    checkpoint_path = \"priors/\"+\"task\"+str(TASK)+\"/\"+str(int(100*alpha))\n",
    "    \n",
    "    if Binary:\n",
    "        M=init_MNIST_model_binary()\n",
    "        checkpoint_path = \"priors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(100*alpha))#+\"/prior.ckpt\"\n",
    "    else:\n",
    "        M=init_MNIST_model()\n",
    "        \n",
    "    fast_cp_callback =fast_checkpoints(checkpoint_path,10)\n",
    "    if save:\n",
    "            CALLBACK=[fast_cp_callback]\n",
    "    else:\n",
    "            CALLBACK=[]\n",
    "            \n",
    "    # now to use it in multiprocessing, the following is necessary\n",
    "    M._make_predict_function()\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    default_graph = tf.get_default_graph()\n",
    "    default_graph.finalize()\n",
    "    \n",
    "    ## choose loss function, optimiser etc. and train\n",
    "    M.compile(loss=keras.losses.categorical_crossentropy,\n",
    "               optimizer=keras.optimizers.SGD(learning_rate=0.003, momentum=0.95),\n",
    "                      metrics=['accuracy'],)\n",
    "    fit_info = M.fit(x_train, y_train,\n",
    "           batch_size=batch_size,\n",
    "           callbacks=CALLBACK,\n",
    "           epochs=total_epochs,\n",
    "           verbose=1,\n",
    "                        )\n",
    "    #### save the final prior weights to disk\n",
    "    if save:\n",
    "        M.save_weights(checkpoint_path+\"/prior.ckpt\")\n",
    "    \n",
    " \n",
    "    \n",
    "    list1=[]\n",
    "    \n",
    "    dirFiles = os.listdir(checkpoint_path) #list of directory files\n",
    "    \n",
    "    ## remove the ckpt.index and sort so that we get the epochs that are in the directory\n",
    "    for files in dirFiles: #filter out all non weights\n",
    "        if '.ckpt.index' in files:\n",
    "            name = re.sub('\\.ckpt.index$', '', files)\n",
    "            if (name[0]==\"1\"):\n",
    "                list1.append(name)\n",
    "        \n",
    "    list1.sort(key=lambda f: int(re.sub('\\D', '', f)))\n",
    "    list1.append(\"prior\")    ## add the final weights which has no number\n",
    "    \n",
    "    Ws=list1\n",
    "    weight_updates=[]\n",
    "    for i in Ws:\n",
    "        if i[0]==\"1\":\n",
    "            if i[1]==\"_\":\n",
    "                weight_updates.append(int(i[2:]))\n",
    "    weight_updates.append(int(np.ceil(len(y_train)/batch_size)))\n",
    "    \n",
    "    error=[]\n",
    "    target_error=[]\n",
    "    for checkpoint in Ws:\n",
    "        if Binary:\n",
    "            model=init_MNIST_model_binary()\n",
    "            model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                   optimizer=keras.optimizers.SGD(learning_rate=0.003, momentum=0.95),\n",
    "                      metrics=['accuracy'],)\n",
    "        else:\n",
    "            model=init_MNIST_model()\n",
    "            model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                   optimizer=keras.optimizers.SGD(learning_rate=0.003, momentum=0.95),\n",
    "                      metrics=['accuracy'],)\n",
    "            \n",
    "        model.load_weights(checkpoint_path+\"/\"+str(checkpoint)+\".ckpt\").expect_partial()\n",
    "        target_error.append(1-model.evaluate(x_target,y_target,verbose=0)[1])\n",
    "        error.append(1-model.evaluate(x_train,y_train,verbose=0)[1])\n",
    "    \n",
    "    if save:\n",
    "        results=pd.DataFrame({'Weightupdates': weight_updates,\n",
    "            'Trainerror': error,\n",
    "            'targeterror':target_error,\n",
    "            })\n",
    "        with open(path_to_root_file+'mnist_transfer/'+checkpoint_path+\"/results.pkl\",'wb') as f:\n",
    "            pickle.dump(results,f)\n",
    "        f.close()\n",
    "    \n",
    "    return model.get_weights()\n",
    "    \n",
    "def read_and_prepare_results(alpha,x_bound,y_bound,x_target,y_target,sigma,delta,N,epsilon,Binary=False,Task=TASK):\n",
    "    \n",
    "    sigma_tmp=sigma\n",
    "    sigma=sigma[0]*10**(-1*sigma[1])\n",
    "    \n",
    "    ## read params.txt for the desired alpha and get the parameters\n",
    "    if Binary:\n",
    "        with open('posteriors/'+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+'/params.txt', 'rb+') as f:\n",
    "            params=f.readlines()\n",
    "        f.close()\n",
    "        prior_path=\"priors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(100*alpha))+\"/prior.ckpt\"\n",
    "        result_path=\"results/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"\n",
    "    else:\n",
    "        with open('posteriors/'+\"task\"+str(TASK)+\"/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+'/params.txt', 'rb+') as f:\n",
    "            params=f.readlines()\n",
    "        f.close()\n",
    "        prior_path=\"priors/\"+\"task\"+str(TASK)+\"/\"+str(int(100*alpha))+\"/prior.ckpt\"\n",
    "        result_path=\"results/\"+\"task\"+str(TASK)+\"/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"\n",
    "        \n",
    "    epsilon=float(params[1])\n",
    "    epochs_trained=int(params[2])\n",
    "    \n",
    "    # initialise model\n",
    "    if Binary:\n",
    "        M=init_MNIST_model_binary()\n",
    "    else:\n",
    "        M=init_MNIST_model()\n",
    "    M.compile(loss=keras.losses.categorical_crossentropy,\n",
    "               optimizer=keras.optimizers.SGD(learning_rate=0.003, momentum=0.95),\n",
    "                      metrics=['accuracy'],)\n",
    "    ### load the prior weights if there are any\n",
    "    if alpha==0:\n",
    "        ### do nothing, i.e take the random init\n",
    "        w_a=M.get_weights()\n",
    "    else:\n",
    "        M.load_weights(prior_path).expect_partial()\n",
    "        w_a=M.get_weights()\n",
    "    print(Binary)\n",
    "    # read the weights and calculate what is needed for the bound\n",
    "    [KLs,errors,targeterrors,Ws,weight_updates]=read_weights(M,w_a,x_bound,y_bound,x_target,y_target,sigma_tmp,epsilon,alpha,Binary=Binary,Task=TASK)    \n",
    "    \n",
    "    #print(KLs)\n",
    "    #print(errors)\n",
    "    #print(targeterrors)\n",
    "    #print(Ws)\n",
    "   \n",
    "    bound=[]\n",
    "    ### calculate the bound\n",
    "    for i in range(len(weight_updates)):\n",
    "        bound.append(calculate_bound(KLs[i],alpha,delta,N,errors[i]))\n",
    "    \n",
    "    \n",
    "    #save the results to a pickled dataframe in results\n",
    "    results=pd.DataFrame({'Weightupdates': weight_updates,\n",
    "        'Trainerror': errors,\n",
    "        'targeterror':targeterrors,\n",
    "        'KL': KLs,\n",
    "        'Bound': bound})\n",
    "    with open(path_to_root_file+'mnist_transfer/'+result_path+str(sigma_tmp[0])+str(sigma_tmp[1])+\"_results.pkl\",'wb') as f:#int(sigma*10**8)\n",
    "        pickle.dump(results,f)\n",
    "    f.close()\n",
    "    return results\n",
    "\n",
    "def plot_result_file(epsilon,alpha,sigma,Binary=False,Task=TASK):\n",
    "    import pandas as pd\n",
    "    sigma_tmp=sigma\n",
    "    sigma=sigma[0]*10**(-1*sigma[1])\n",
    "    \n",
    "    if Binary:\n",
    "        result_path=\"results/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"+str(sigma_tmp[0])+str(sigma_tmp[1])\n",
    "        plt.title(\"Binary: \"+r\"$\\alpha$=\"+str(alpha)+r\" $\\epsilon$=\"+str(epsilon)+r\" $\\sigma$=\"+str(sigma))\n",
    "    else:\n",
    "        result_path=\"results/\"+\"task\"+str(TASK)+\"/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"+str(sigma_tmp[0])+str(sigma_tmp[1])\n",
    "        plt.title(r\"$\\alpha$=\"+str(alpha)+r\" $\\epsilon$=\"+str(epsilon)+r\" $\\sigma$=\"+str(sigma))\n",
    "    results=pd.read_pickle(result_path+\"_results.pkl\")\n",
    "    \n",
    "    ### do the plots\n",
    "    plt.plot(results[\"Weightupdates\"],results[\"Bound\"],'r*-')\n",
    "    plt.plot(results[\"Weightupdates\"],results[\"Trainerror\"],'m^-')\n",
    "    \n",
    "    plt.xlabel(\"Weight updates\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    \n",
    "    plt.legend([\"Bound\",\"Empirical error\"])\n",
    "    plt.show()\n",
    "\n",
    "def find_optimal_sigma(sigmas,epsilon, alpha,Binary=False,Task=TASK):\n",
    "    #### to find the optimal sigma just do a search through all the results \n",
    "    #### and save the one for each parameter which has the minimal bound\n",
    "    #### Do we do this per epoch or for some other value? The sigma which yields the lowest bound overall for some epoch?\n",
    "    optimal=[0,1]\n",
    "    # search through all epochs and pick the sigma which yields the smallest bound during the whole training process\n",
    "    for sigma in sigmas:\n",
    "        sigma_tmp=sigma\n",
    "        sigma=sigma[0]*10**(-1*sigma[1])\n",
    "        if Binary:\n",
    "            result_path=\"results/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"+str(sigma_tmp[0])+str(sigma_tmp[1])\n",
    "        else:\n",
    "            result_path=\"results/\"+\"task\"+str(TASK)+\"/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"+str(sigma_tmp[0])+str(sigma_tmp[1])\n",
    "        results=pd.read_pickle(result_path+\"_results.pkl\")\n",
    "        MIN=np.min(results[\"Bound\"])\n",
    "        if (MIN<optimal[1]):\n",
    "            optimal[1]=MIN\n",
    "            optimal[0]=sigma\n",
    "    print(\"The optimal sigma is {} with bound value {}\".format(optimal[0],optimal[1]))\n",
    "\n",
    "   \n",
    "#### find the optimal sigma for every combination of parameters\n",
    "\n",
    "#### use the optimal sigmas to calculate the bound 50 times(with different data orders and initialisation)\n",
    "#### (Note: also delta=13*delta_0) for every combination and save the mean and std for plotting into a result file\n",
    "#for i in range(50):\n",
    "    ## take in the data and split with a new seed\n",
    " #   x_bound, x_prior, y_bound , y_prior = train_test_split(x_source,y_source,test_size=alpha,random_state=(69105+i))\n",
    "#### \n",
    "def read_prior(alpha,TASK=2,Binary=True):\n",
    "    checkpoint_path = \"priors/\"+\"task\"+str(TASK)+\"/\"+str(int(100*alpha))\n",
    "    if Binary:\n",
    "        checkpoint_path = \"priors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(100*alpha))\n",
    "    result_path=path_to_root_file+'mnist_transfer/'+checkpoint_path+\"/results.pkl\"\n",
    "    results=pd.read_pickle(result_path)\n",
    "    plt.title(r\"$\\alpha$=\"+str(alpha))\n",
    "    plt.plot(results[\"Weightupdates\"],results[\"Trainerror\"],'m^-')\n",
    "    plt.plot(results[\"Weightupdates\"],results[\"targeterror\"],'k^-')\n",
    "    plt.legend([\"Training error\",\"Target error\"])\n",
    "    \n",
    "def plot_prior_and_posterior(alpha,epsilon,sigma,TASK=2,Binary=True):\n",
    "    ### load in the prior data\n",
    "    sigma_tmp=sigma\n",
    "    sigma=sigma[0]*10**(-1*sigma[1])\n",
    "    checkpoint_path = \"priors/\"+\"task\"+str(TASK)+\"/\"+str(int(100*alpha))\n",
    "    if Binary:\n",
    "        checkpoint_path = \"priors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(100*alpha))\n",
    "    result_path=path_to_root_file+'mnist_transfer/'+checkpoint_path+\"/results.pkl\"\n",
    "    results=pd.read_pickle(result_path)\n",
    "    result_path_post=\"results/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"+str(sigma_tmp[0])+str(sigma_tmp[1])\n",
    "    results2=pd.read_pickle(result_path_post+\"_results.pkl\")\n",
    "    \n",
    "    \n",
    "    ### remove/ignore the last entry of the prior data \n",
    "    ### as it should be a duplication of the first one from the posterior results\n",
    "    \n",
    "    ### training error\n",
    "    A=list(results2[\"Weightupdates\"]+list(results[\"Weightupdates\"])[-1])\n",
    "    B=list(results[\"Weightupdates\"])[:-1]\n",
    "    B.extend(A)\n",
    "    C=list(results[\"Trainerror\"])[:-1]\n",
    "    C.extend(list(results2[\"train_germain\"]))\n",
    "    plt.plot(B,C,'-m^')\n",
    "    \n",
    "    ## target error\n",
    "    D=list(results[\"targeterror\"])[:-1]\n",
    "    D.extend(list(results2[\"target_germain\"]))\n",
    "    plt.plot(B,D,'-k*')\n",
    "    \n",
    "    ### bound\n",
    "    E=results2[\"germain_bound\"]\n",
    "    plt.plot(A,E,'-D')\n",
    "    F=results2['boundpart3_germain']\n",
    "    plt.plot(A,F,'-o')\n",
    "    print(results2[\"target_germain\"])\n",
    "    print(results2[\"germain_bound\"])\n",
    "    ### lines for uninformative region and worse than random guessing; also for end of prior training\n",
    "    plt.axvline(A[0],color=\"grey\")\n",
    "    plt.axhline(y=0.5, color=\"black\", linestyle=\"--\")\n",
    "    plt.axhline(y=1, color=\"red\", linestyle=\"--\")\n",
    "    plt.legend([\"Training error\",\"Target error\",\"Bound\",\"KL-part\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha is:0\n",
      "True\n",
      "could deep copy\n",
      "Virtual devices cannot be modified after being initialized\n",
      "!!!!\n",
      "Virtual devices cannot be modified after being initialized\n",
      "!!!!\n",
      "Virtual devices cannot be modified after being initialized\n",
      "!!!!\n",
      "Virtual devices cannot be modified after being initialized\n",
      "!!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-131:\n",
      "Process ForkPoolWorker-129:\n",
      "Process ForkPoolWorker-130:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-ec1c90cd6780>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m#w_s=train_posterior(alpha,x_source,y_source_bin,None,x_test=x_source,y_test=y_source_bin,epsilon=epsilon,Task=TASK,Binary=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msigma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msigmas\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mres\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_and_prepare_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_source\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_source_bin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_target\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_target_bin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_source\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mBinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#x_bound,y_bound,x_target,y_target_bin,sigma,delta,len(x_bound),epsilon,Binary=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0;31m#plot_result_file(epsilon,alpha,sigma,TASK,Binary=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-03313f668f9e>\u001b[0m in \u001b[0;36mread_and_prepare_results\u001b[0;34m(alpha, x_bound, y_bound, x_target, y_target, sigma, delta, N, epsilon, Binary, Task)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBinary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m     \u001b[0;31m# read the weights and calculate what is needed for the bound\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m     \u001b[0;34m[\u001b[0m\u001b[0mKLs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargeterrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mWs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweight_updates\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw_a\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_bound\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_bound\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_target\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_target\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msigma_tmp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mBinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBinary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTASK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[0;31m#print(KLs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-03313f668f9e>\u001b[0m in \u001b[0;36mread_weights\u001b[0;34m(model, w_a, x_bound, y_bound, x_target, y_target, sigma, epsilon, alpha, Binary, Task)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdumb_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;31m#for arg in args:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.9/multiprocessing/pool.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mCLOSE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTERMINATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"In unknown state\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_worker_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.9/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1053\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1054\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.9/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1067\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1069\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1070\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.9/multiprocessing/queues.py\", line 365, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.9/multiprocessing/queues.py\", line 365, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.9/multiprocessing/queues.py\", line 366, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.9/multiprocessing/connection.py\", line 221, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.9/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.9/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "module5 = SourceFileLoader('plot', path_to_root_file+'mnist_transfer/results/plotting.py').load_module()\n",
    "from plot import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "#alpha=0.1\n",
    "delta=0.05 ## what would this be?   \n",
    "#sigma=0.01  \n",
    "\n",
    "alphas=[]\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "length=10\n",
    "for i in range(length-1):\n",
    "    alphas.append((i+1)/length)\n",
    "\n",
    "epsilons=[0.03,0.01,0.001]\n",
    "#epsilons=[0.03]\n",
    "#epsilons=[0.01,0.001]\n",
    "### do hyperparameter sweep over sigma\n",
    "sigmas=[]\n",
    "#sigmas=[0.001,0.003,0.0001]\n",
    "#alphas=[0,0.1,0.5]#,0.6,0.7,0.8,0.9]\n",
    "#alphas=[0.3]\n",
    "\n",
    "for i in range(2,9):  \n",
    "    sigmas.append([3,i])#3*10**(-i))\n",
    "    if(i==8):\n",
    "        break\n",
    "    sigmas.append([1,i])#10**(-i))\n",
    "#alphas.append(0.3)\n",
    "#sigmas.append([3,2])\n",
    "alphas=[0.1]\n",
    "y_source_bin=make_mnist_binary(y_source)\n",
    "y_target_bin=make_mnist_binary(y_target)\n",
    "for alpha in alphas:\n",
    "    print(\"Alpha is:\"+str(alpha))\n",
    "    x_bound, x_prior, y_bound , y_prior = train_test_split(x_source,y_source_bin,test_size=alpha,random_state=69105)\n",
    "    w_a=train_prior(alpha,1,x_source,y_source_bin,x_target=x_target,y_target=y_target_bin,save=True,Task=2,Binary=True)\n",
    "    #w_a=train_prior(alpha,1,x_prior,y_prior,x_target=x_target,y_target=y_target_bin,save=True,Task=2,Binary=True)\n",
    "    #read_prior(0.3,TASK=TASK,Binary=True)\n",
    "    #plot_result_file(0.03,alpha,[3,2],TASK,Binary=True)\n",
    "    #plot_prior_and_posterior(alpha,0.03,[3,3],TASK=TASK,Binary=True)\n",
    "    \n",
    "    for epsilon in epsilons:\n",
    "        w_s=train_posterior(alpha,x_source,y_source_bin,None,x_test=x_source,y_test=y_source_bin,epsilon=epsilon,Task=TASK,Binary=True)\n",
    "        #for sigma in sigmas:\n",
    "            #res=read_and_prepare_results(alpha,x_source,y_source_bin,x_target,y_target_bin,sigma,delta,len(x_source),epsilon,Binary=True)#x_bound,y_bound,x_target,y_target_bin,sigma,delta,len(x_bound),epsilon,Binary=True)\n",
    "            #plot_result_file(epsilon,alpha,sigma,TASK,Binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import sys\n",
    "import os\n",
    "mymodule2 = SourceFileLoader('DA', path_to_root_file+'mnist_transfer/experiments/DA_bound.py').load_module()\n",
    "from DA import *\n",
    "##### we want to compute the bound from Germain thm 7\n",
    "\n",
    "#### R_T  \\leq  omega' \\hat{R}_S+ a'/2 * dis_\\rho (S,T) +(omega'/omega +a'/a)*(KL(\\rho\\| \\pi )+\\ln(\\frac{3}{\\delta}))/m \n",
    "##### + \\lambda_\\rho + 1/2*(a'-1)\n",
    "\n",
    "\n",
    "#### a > 0 omega> 0, this can and probably should be optimised for the bound\n",
    "\n",
    "#### \\hat{R}_S is the empirical error on source, so just draw classifiers and calculate\n",
    "\n",
    "\n",
    "#### beta(T \\| S), domain divergence, this is more or less equal to average label density ratio or worst case depending on parameter q\n",
    "\n",
    "\n",
    " #### lambda_\\rho, difference in expected joint error |e_T-e_S| where e_S= E_h,h' E_x,y L(h(x),y)L(h'(x),y) \n",
    "    #### so draw two classifiers and calculate the mean of the product of losses on the domain   \n",
    "def read_weights_germain(model,w_a,x_bound,y_bound,x_target,y_target,epochs_trained,sigma,epsilon,alpha,Binary=False,Task=TASK):\n",
    "    import re\n",
    "    KLs=[]\n",
    "    e_s=[]\n",
    "    e_t=[]\n",
    "    d_tx=[]\n",
    "    d_sx=[]\n",
    "    epochs=[]\n",
    "    train_germain=[] \n",
    "    target_germain=[]\n",
    "    dis_rho=[]\n",
    "    lambda_rho=[]\n",
    "    sigma_tmp=sigma\n",
    "    sigma=sigma[0]*10**(-1*sigma[1])\n",
    "    \n",
    "    ### Here we do something more intelligent to not have to hardcode the epoch amounts. \n",
    "    ### we parse the filenames and sort them in numerical order and then load the weights\n",
    "    if Binary:\n",
    "        path=\"posteriors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))\n",
    "    else:\n",
    "        path=\"posteriors/\"+\"task\"+str(TASK)+\"/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))\n",
    "    list1=[]\n",
    "    list2=[]\n",
    "    dirFiles = os.listdir(path) #list of directory files\n",
    "    ## remove the ckpt.index and sort so that we get the epochs that are in the directory\n",
    "    for files in dirFiles: #filter out all non jpgs\n",
    "        if '.ckpt.index' in files:\n",
    "            name = re.sub('\\.ckpt.index$', '', files)\n",
    "            ### if it has a one it goes in one list and if it starts with a two it goes in the other\n",
    "            if (name[0]==\"1\"):\n",
    "                list1.append(name)\n",
    "            elif (name[0]==\"2\"):\n",
    "                list2.append(name)\n",
    "            #epochs.append(name)\n",
    "    #epochs.sort(key=lambda f: int(re.sub('\\D', '', f)))\n",
    "    list1.sort(key=lambda f: int(re.sub('\\D', '', f)))\n",
    "    num_batchweights=len(list1)\n",
    "    list2.sort(key=lambda f: int(re.sub('\\D', '', f)))\n",
    "    list1.extend(list2)\n",
    "    Ws=list1 ## vector of checkpoint filenames\n",
    "    #print(Ws)\n",
    "    #sys.exit(-1)\n",
    "    #epochs=[int(i) for i in Ws]\n",
    "    Xvector=[]\n",
    "    for i in Ws:\n",
    "        #print(i)\n",
    "        if i[0]==\"1\":\n",
    "            if i[1]==\"_\":\n",
    "                Xvector.append(int(i[2:]))\n",
    "    for i in list2:\n",
    "        Xvector.append((int(i[2:])+1)*547) ## TODO contant hack\n",
    "    print(Xvector)\n",
    "    #sys.exit(-1)\n",
    "    \"\"\"   \n",
    "    epochs = [] #list of checkpoint filenames\n",
    "    dirFiles = os.listdir(path) #list of directory files\n",
    "    ## remove the ckpt.index and sort so that we get the epochs that are in the directory\n",
    "    for files in dirFiles: #filter out all non jpgs\n",
    "        if '.ckpt.index' in files:\n",
    "            name = re.sub('\\.ckpt.index$', '', files)\n",
    "            epochs.append(name)\n",
    "    epochs.sort(key=lambda f: int(re.sub('\\D', '', f)))\n",
    "    epochs=[int(i) for i in epochs]\n",
    "    \"\"\" \n",
    "    path=\"posteriors/\"+\"task\"+str(TASK)+\"/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"/\"#\"{epoch:0d}.ckpt\"\n",
    "    if Binary:\n",
    "        path=\"posteriors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"/\"#\"{epoch:0d}.ckpt\"\n",
    "    \n",
    "    \n",
    "    L=len(Xvector)\n",
    "     ### vectors for saving std of each point\n",
    "    error_std=[]\n",
    "    target_error_std=[]\n",
    "    e_s_std=[]\n",
    "    e_t_std=[]\n",
    "    d_tx_std=[]\n",
    "    d_sx_std=[]\n",
    "    for checkpoint in Ws:\n",
    "        if Binary:\n",
    "            model=init_MNIST_model_binary()\n",
    "            model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                   optimizer=keras.optimizers.SGD(learning_rate=0.003, momentum=0.95),\n",
    "                      metrics=['accuracy'],)\n",
    "        else:\n",
    "            model=init_MNIST_model()\n",
    "            model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                   optimizer=keras.optimizers.SGD(learning_rate=0.003, momentum=0.95),\n",
    "                      metrics=['accuracy'],)\n",
    "\n",
    "        model.load_weights(path+str(checkpoint)+\".ckpt\").expect_partial()\n",
    "        w_s=model.get_weights()\n",
    "        \"\"\"\n",
    "        ############################# test\n",
    "        #print(x_target[1])\n",
    "        pred = model.predict(x_target)\n",
    "        print(make_01(pred))\n",
    "        indices = [i for i,v in enumerate(pred) if np.sum(pred[i]-y_target[i])==0]\n",
    "        print(indices)\n",
    "        sys.exit(-1)\n",
    "        \"\"\"\n",
    "        ##### here we should draw classifiers and pass that on\n",
    "        CLASSIFIERS=4\n",
    "        t = time.time()\n",
    "        ## do X draws of the posterior, for two separate classifiers\n",
    "        w_s_draws=draw_classifier(w_s,sigma,num_classifiers=CLASSIFIERS)\n",
    "        w_s_draws2=draw_classifier(w_s,sigma,num_classifiers=CLASSIFIERS)\n",
    "        ## do X draws of the prior\n",
    "        #w_a_draws=draw_classifier(w_a,sigma,num_classifiers=2)\n",
    "          \n",
    "        elapsed = time.time() - t\n",
    "        print(\"Time spent drawing the classifiers: \"+str(elapsed)+\"\\n\")\n",
    "   \n",
    "        \n",
    "        ###### for each pair of drawn prior and posterior we calculate the necessary parts of the bound \n",
    "        ###### and then average the result and return that\n",
    "        \n",
    "        \n",
    "\n",
    "        errorsum=[]\n",
    "        target_errorsum=[]\n",
    "       \n",
    "        ######## in here we should make the results save in a vector for each part to be able to calculate\n",
    "        ######## the standard deviation and be able to get error bars on things.\n",
    "        t = time.time()\n",
    "        for h in w_s_draws:\n",
    "            model.set_weights(h)\n",
    "            errorsum.append((1-model.evaluate(x_bound,y_bound,verbose=0)[1]))\n",
    "            target_errorsum.append((1-model.evaluate(x_target,y_target,verbose=0)[1]))\n",
    "        \n",
    "        \n",
    "        for hprime in w_s_draws2:\n",
    "            model.set_weights(hprime)\n",
    "            errorsum.append((1-model.evaluate(x_bound,y_bound,verbose=0)[1]))\n",
    "            target_errorsum.append((1-model.evaluate(x_target,y_target,verbose=0)[1]))\n",
    "     \n",
    "        \n",
    "        train_germain.append((np.mean(errorsum))) #/(len(w_s_draws)+len(w_s_draws2))\n",
    "        target_germain.append(np.mean(target_errorsum))  #/(len(w_s_draws)+len(w_s_draws2))\n",
    "        error_std.append(np.std(errorsum))\n",
    "        target_error_std.append(np.std(target_errorsum))\n",
    "        elapsed = time.time() - t\n",
    "        print(\"Time spent calculating errors: \"+str(elapsed)+\"\\n\")\n",
    "        \n",
    "        ######## in here we should make the results save in a vector for each part to be able to calculate\n",
    "        ######## the standard deviation and be able to get error bars on things.\n",
    "        \n",
    "        #### loop over pairs of classifiers from posterior for the disagreement and joint error\n",
    "        #q=0\n",
    "        e_ssum=[]\n",
    "        e_tsum=[]\n",
    "        d_txsum=[]\n",
    "        d_sxsum=[]\n",
    "        d_tx_h=0\n",
    "        d_sx_h=0\n",
    "        d_tx_hprime=0\n",
    "        d_sx_hprime=0\n",
    "       \n",
    "        t = time.time()\n",
    "        \n",
    "        #### Here we should just do the four pairs so there is no cross-usage here\n",
    "        #### this can be not good for the independence of the values which makes the CI useless\n",
    "        \n",
    "        for i, h in enumerate(w_s_draws):\n",
    "            model.set_weights(h)\n",
    "            d_tx_h=model.predict(x_target,verbose=0)\n",
    "            d_sx_h=model.predict(x_bound,verbose=0)\n",
    "            d_sx_h=make_01(d_sx_h)\n",
    "            d_tx_h=make_01(d_tx_h)\n",
    "            #for hprime in w_s_draws2:\n",
    "            hprime=w_s_draws2[i]\n",
    "            model.set_weights(hprime)\n",
    "            d_tx_hprime=model.predict(x_target,verbose=0)\n",
    "            d_sx_hprime=model.predict(x_bound,verbose=0)\n",
    "            d_sx_hprime=make_01(d_sx_hprime)\n",
    "            d_tx_hprime=make_01(d_tx_hprime)\n",
    "                \n",
    "            e_ssum.append(joint_error(d_sx_h,d_sx_hprime,y_bound))\n",
    "            d_sxsum=(classifier_disagreement(d_sx_h,d_sx_hprime))\n",
    "            e_tsum=(joint_error(d_tx_h,d_tx_hprime,y_target))\n",
    "            d_txsum=(classifier_disagreement(d_tx_h,d_tx_hprime))\n",
    "        \n",
    "        e_s.append(np.mean(e_ssum))\n",
    "        d_sx.append(np.mean(d_sxsum))\n",
    "        e_t.append(np.mean(e_tsum))\n",
    "        d_tx.append(np.mean(d_txsum))\n",
    "        ### save the std\n",
    "        e_s_std.append(np.std(e_ssum))\n",
    "        d_sx_std.append(np.std(d_sxsum))\n",
    "        e_t_std.append(np.std(e_tsum))\n",
    "        d_tx_std.append(np.std(d_txsum))\n",
    "        elapsed = time.time() - t\n",
    "        print(\"Time spent calculating joint errors and disagreements: \"+str(elapsed)+\"\\n\")    \n",
    "        \n",
    "        KLsum=0\n",
    "        t = time.time()\n",
    "        \n",
    "        KLsum=estimate_KL(w_a,w_s,sigma)## compute the KL\n",
    "        ## only w_s, w_a here\n",
    "                \n",
    "      \n",
    "        KLs.append(KLsum)\n",
    "        elapsed = time.time() - t\n",
    "        print(\"Time spent calculating KL: \"+str(elapsed)+\"\\n\") \n",
    "        \n",
    "        ### memory leak city\n",
    "        del model\n",
    "        _=gc.collect()\n",
    "        \n",
    "        \n",
    "     \n",
    "    print(\"Finished calculation of bound parts\")\n",
    "   \n",
    "          #### load the result file if it exists otherwise make one\n",
    "    if Binary:\n",
    "        result_path=\"results/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"+str(sigma_tmp[0])+str(sigma_tmp[1])\n",
    "    else:\n",
    "        result_path=\"results/\"+\"task\"+str(TASK)+\"/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"+str(sigma_tmp[0])+str(sigma_tmp[1])\n",
    "    if(os.path.exists(result_path+\"_results.pkl\")):\n",
    "        results=pd.read_pickle(result_path+\"_results.pkl\")\n",
    "    else:\n",
    "        results=pd.DataFrame({'Weightupdates': Xvector,\n",
    "        'train_germain': train_germain,\n",
    "        'target_germain':target_germain,\n",
    "        'KL': KLs})\n",
    "        with open(path_to_root_file+'mnist_transfer/'+result_path+\"_results.pkl\",'wb') as f:\n",
    "            pickle.dump(results,f)\n",
    "        f.close()\n",
    "        results=pd.read_pickle(result_path+\"_results.pkl\")\n",
    "\n",
    "    train_germain=np.array(train_germain)\n",
    "    results['Weightupdates']=Xvector\n",
    "    results['train_germain']=train_germain\n",
    "    results['target_germain']=target_germain\n",
    "    results['e_s']=e_s\n",
    "    results['e_t']=e_t\n",
    "    results['d_tx']=d_tx\n",
    "    results['d_sx']=d_sx\n",
    "    results['KL']=KLs\n",
    "    ### save the std deviations as well in some form like a vector of deviations for each factor\n",
    "    results['error_std']=error_std\n",
    "    results['target_error_std']=target_error_std\n",
    "    results['e_s_std']=e_s_std\n",
    "    results['e_t_std']=e_t_std\n",
    "    results['d_tx_std']=d_tx_std\n",
    "    results['d_sx_std']=d_sx_std\n",
    "    KL=KLs\n",
    "    \n",
    "    m=len(y_bound)\n",
    "    delta=0.05 ## hardcoded value\n",
    "     # calculate disrho bound\n",
    "    [res,bestparam, boundparts]=grid_search(train_germain,e_s,e_t,d_tx,d_sx,KL,delta,m,L)\n",
    "    # calculate beta bound\n",
    "    [res2,bestparam2, boundparts2]=grid_search(train_germain,e_s,e_t,d_tx,d_sx,KL,delta,m,L,beta_bound=True)            \n",
    "                \n",
    "                \n",
    "                \n",
    "    results['germain_bound']=res\n",
    "    print(\"Germain bound\"+str(res))\n",
    "    print(\"[a, omega]= \"+str(bestparam))\n",
    "    Best=np.zeros([len(res),1])\n",
    "    Best[0]=bestparam[0]\n",
    "    Best[1]=bestparam[1]\n",
    "    Best[2]=CLASSIFIERS\n",
    "    #print(Best)\n",
    "    results['bestparam']=Best\n",
    "    results['boundpart1_germain']=boundparts[0]\n",
    "    results['boundpart2_germain']=boundparts[1]\n",
    "    results['boundpart3_germain']=boundparts[2]\n",
    "    results['boundpart4_germain']=boundparts[3]\n",
    "    results['boundpart5_germain']=boundparts[4]\n",
    "    ## beta bound\n",
    "    results['beta_bound']=res2\n",
    "    results['beta_boundpart1']=boundparts2[0]\n",
    "    results['beta_boundpart2']=boundparts2[1]\n",
    "    results['beta_boundpart3']=boundparts2[2]\n",
    "    with open(path_to_root_file+'mnist_transfer/'+result_path+\"_results.pkl\",'wb') as f:\n",
    "        pickle.dump(results,f)\n",
    "    f.close()\n",
    "    return results\n",
    "\n",
    "def grid_search(train_germain,e_s,e_t,d_tx,d_sx,KL,delta,m,L,beta_bound=False):\n",
    "    #### here we want to do a coarse grid search over a and omega to get the smallest bound \n",
    "    print(\"Starting gridsearch....\")\n",
    "    avec=[0.001,0.005,0.01,0.05,0.1,0.5,1,5,10,50,100,500,1000,5000,10000,50000,100000]\n",
    "    omegas=[0.001,0.005,0.01,0.05,0.1,0.5,1,5,10,50,100,500,1000,5000,10000,50000,100000]\n",
    "    tmp= sys.maxsize\n",
    "    res=[]\n",
    "    bestparam=[0,0]\n",
    "    for a in avec:\n",
    "        for omega in omegas:\n",
    "            if beta_bound:\n",
    "                germain_bound, boundparts=calculate_beta_bound(e_s,d_tx,KL,delta,a,omega,m,L)\n",
    "            else:\n",
    "                germain_bound,boundparts=calculate_germain_bound(train_germain,e_s,e_t,d_tx,d_sx,KL,delta,a,omega,m,L)\n",
    "            if min(germain_bound)<tmp:\n",
    "                tmp=min(germain_bound)\n",
    "                #print(\"Best bound thus far:\"+str(tmp))\n",
    "                res=germain_bound\n",
    "                bestparam=[a,omega]\n",
    "                \n",
    "    ### do a finer sweep around the best parameters\n",
    "    if bestparam[0]!=0:\n",
    "        avec=np.arange(bestparam[0]-bestparam[0]/2,bestparam[0]+bestparam[0]*4,0.1*bestparam[0])\n",
    "    else:## no bound better than the max int was found, if that is even possible\n",
    "        avec=np.arange(-1,1,0.1)\n",
    "    if bestparam[1]!=0:\n",
    "        omegas=np.arange(bestparam[1]-bestparam[1]/2,bestparam[1]+bestparam[1]*4,0.1*bestparam[1])\n",
    "    else:## no bound better than the max int was found\n",
    "        avec=np.arange(-1,1,0.1)\n",
    "    boundparts=[0, 0,0,0,0]\n",
    "    for a in avec:\n",
    "        for omega in omegas:\n",
    "            if beta_bound:\n",
    "                germain_bound, boundparts=calculate_beta_bound(e_s,d_tx,KL,delta,a,omega,m,L)\n",
    "            else:\n",
    "                germain_bound,boundparts=calculate_germain_bound(train_germain,e_s,e_t,d_tx,d_sx,KL,delta,a,omega,m,L)\n",
    "            if min(germain_bound)<tmp:\n",
    "                tmp=min(germain_bound)\n",
    "                #print(\"Best finer bound thus far:\"+str(tmp))\n",
    "                res=germain_bound\n",
    "                bestparam=[a,omega]\n",
    "                #boundparts=[a1,a2,a3,a4,a5]\n",
    "    return [res,bestparam, boundparts]\n",
    "def calculate_beta_bound(e_s,d_tx,KL,delta,b,c,m,L,BETA=0):\n",
    "    BETA=10.986111 ### hardcoded value for beta_infinity for TASK2 TODO!\n",
    "    m_s=m  ## temporary, we should pass these in\n",
    "    m_t=m  ## temporary, we should pass these in\n",
    "    bprime=b/(1-np.exp(-b))\n",
    "    cprime=c/(1-np.exp(-c))\n",
    "    \n",
    "    bound=[]\n",
    "    a1=np.zeros(L)\n",
    "    a2=np.zeros(L)\n",
    "    a3=np.zeros(L)\n",
    "    for i in range(L):\n",
    "        a1[i]=cprime/2*(d_tx[i])\n",
    "        a2[i]=bprime*e_s[i]\n",
    "        a3[i]=(cprime/(m_t*c)+bprime*BETA/(m_s*b))*(2*KL[i]+np.log(2/delta))\n",
    "    ## we cannot evaluate the eta term in the bound so this is it. For TASK 2 it is 0 anyway.\n",
    "    ## And for other tasks we will not evaluate this bound anyway as we probably have no way of doing so easily..\n",
    "        bound.append(a1[i]+a2[i]+a3[i])\n",
    "    boundparts=[a1,a2,a3]\n",
    "    return bound, boundparts\n",
    "def germain_bound(x_bound,y_bound,x_target,y_target,alpha,sigma,epsilon,task,Binary=False):\n",
    "    TASK=task\n",
    "    sigma_tmp=sigma\n",
    "    sigma=sigma[0]*10**(-1*sigma[1])\n",
    "        ## read params.txt for the desired alpha and get the parameters\n",
    " \n",
    "    if Binary:\n",
    "        with open('posteriors/'+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+'/params.txt', 'rb+') as f:\n",
    "            params=f.readlines()\n",
    "        f.close()\n",
    "        prior_path=\"priors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(100*alpha))+\"/prior.ckpt\"\n",
    "        result_path=\"results/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"\n",
    "    else:\n",
    "        with open('posteriors/'+\"task\"+str(TASK)+\"/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+'/params.txt', 'rb+') as f:\n",
    "            params=f.readlines()\n",
    "        f.close()\n",
    "        prior_path=\"priors/\"+\"task\"+str(TASK)+\"/\"+str(int(100*alpha))+\"/prior.ckpt\"\n",
    "        result_path=\"results/\"+\"task\"+str(TASK)+\"/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"\n",
    "\n",
    "    epsilon=float(params[1])\n",
    "    epochs_trained=int(params[2])\n",
    "    if Binary:\n",
    "        M=init_MNIST_model_binary()\n",
    "    else:\n",
    "        M=init_MNIST_model()\n",
    "                \n",
    "    M.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                   optimizer=keras.optimizers.SGD(learning_rate=0.003, momentum=0.95),\n",
    "                      metrics=['accuracy'],)\n",
    "      ### load the prior weights if there are any\n",
    "    if(Binary and alpha != 0):\n",
    "        prior_path=\"priors/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(100*alpha))+\"/prior.ckpt\"\n",
    "    elif(alpha != 0):\n",
    "        prior_path=\"priors/\"+\"task\"+str(TASK)+\"/\"+str(int(100*alpha))+\"/prior.ckpt\"\n",
    "    if alpha==0:\n",
    "        ### do nothing, just take the random initialisation\n",
    "        w_a=M.get_weights()\n",
    "    else:\n",
    "        M.load_weights(prior_path).expect_partial()\n",
    "        w_a=M.get_weights()\n",
    "    \n",
    " \n",
    "    ## get the prior weights for the KL and pass into read_weights\n",
    "    results=read_weights_germain(M,w_a,x_bound,y_bound,x_target,y_target,epochs_trained,sigma_tmp,epsilon,alpha,Binary=Binary,Task=TASK)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " from sklearn.model_selection import train_test_split\n",
    "\n",
    "delta=0.05\n",
    "Binary=True\n",
    "\n",
    "epsilons=[0.03]#,0.01]#,0.001]#,0.01,0.001]\n",
    "#epsilons=[0.01,0.001]\n",
    "#sigmas=[0.003]\n",
    "#sigmas=[0.00001,0.0003,0.000001,0.00003]\n",
    "alphas=[0,0.3]#,0.5]#,0.3]#,0.3]#0.3,0.4,0.6]\n",
    "#alphas=[0]\n",
    "sigmas=[[3,2],[3,3]]#,[3,3],[1,6]]\n",
    "'''\n",
    "for i in range(2,9):  \n",
    "    sigmas.append([3,i])#3*10**(-i))\n",
    "    if(i==8):\n",
    "        break\n",
    "    sigmas.append([1,i])#10**(-i))\n",
    "'''\n",
    "#errors=[428, 468, 1247, 1256, 2235, 2348, 2539, 2723, 2797, 3058, 3075, 3717, 3902, 3998, 4210, 4703, 4714, 4801, 5086, 5090, 5276, 5323, 5335, 5392, 5559, 5779, 6082, 6304, 6308, 6351, 6354, 6363, 6388, 6510, 6584, 6615, 6624, 6632, 6703, 6766, 6838, 6839, 6926, 7061, 7089, 7224, 7331, 7334, 7338, 7369, 7429, 7627, 7726, 7731, 7764, 7847, 7872, 7954, 8122, 8259, 8290, 8307, 8547, 8577, 8803, 8842, 8846, 8860, 8972, 9201, 9303, 9427, 9566, 9570, 9642, 9723, 9917, 10272, 10316, 10609, 10621, 10715, 10735, 10751, 10878, 10897, 11006, 11203, 11206, 11256, 11267, 11362, 11372, 11450, 11544, 11632, 11701, 12033, 12135, 12141, 12874, 13101, 13194, 13380, 14147, 14501, 14683, 15246, 15297, 15569, 15589, 15604, 15632, 15659, 15750, 15926, 16045, 16224, 16237, 16246, 16291, 16341, 16426, 16520, 16525, 16549, 16554, 16686, 16763, 16780, 16792, 16803, 16814, 16820, 16891, 16907, 16912, 17015, 17048, 17053, 17183, 17216, 17230, 17272, 17286, 17310, 17418, 17432, 17441, 17663, 17875, 17881, 17904, 17941, 17971, 18031, 18230, 18270, 18339, 18370, 18440, 18471, 18547, 18643, 18660, 18694, 18708, 18777, 18805, 18811, 18818, 18963, 19082, 19101, 19114, 19124, 19194, 19248, 19319, 19357, 19369, 19383, 19464, 19510, 19521, 19554, 19582, 19694, 19841, 19845, 19905, 19935, 19974, 19998, 20002, 20268, 20276, 20302, 20339, 20343, 20386, 20395, 20454, 20473, 20595, 20814, 20817, 20858, 21059, 21445, 21449, 21516, 21518, 21527, 21557, 21559, 21566, 21614, 21645, 21801, 21826, 21868, 21883, 21941, 21947, 22002, 22008, 22160, 22167, 22179, 22274, 22411, 22452, 22536, 22649, 22650, 22662, 22672, 22700, 22704, 22718, 22839, 22872, 22883, 22892, 23001, 23035, 23152, 23162, 23169, 23210, 23227, 23259, 23277, 23418, 23578, 23584, 23688, 23741, 23760, 23777, 23790, 23829, 23957, 24056, 24059, 24089, 24150, 24157, 24182, 24295, 24339, 24387, 24429, 24695, 24753, 24873, 24907, 24943, 25004, 25036, 25084, 25287, 25316, 25317, 25329, 25376, 25412, 25492, 25544, 25612, 25618, 25636, 25854, 26071, 26084, 26151, 26177, 26479, 26493, 26523, 26539, 26641, 26667, 26713, 26737, 27030, 27222, 27239, 27326, 27429, 27502, 27511, 27548, 27567, 27675, 27817, 27832, 27890, 27932, 27947, 28193, 28341, 28364, 28377, 28411, 28453, 28528, 28564, 28885, 28896, 28912, 28943, 28991, 29029, 29119, 29185, 29225, 29233, 29289, 29339, 29468, 29551, 29561, 29584, 29623, 29705, 29740, 29749, 29887, 29893, 29897, 29924, 30014, 30048, 30193, 30194, 30228, 30354, 30390, 30403, 30450, 30494, 30647, 30655, 30697, 30757, 31068, 31078, 31126, 31152, 31160, 31161, 31190, 31255, 31273, 31325, 31373, 31389, 31731, 31803, 31824, 31827, 31829, 31837, 31841, 31845, 31850, 31865, 31868, 31893, 31921, 31939, 31950, 31957, 31977, 31985, 32008, 32010, 32019, 32023, 32045, 32054, 32071, 32075, 32083, 32087, 32097, 32101, 32111, 32112, 32143, 32152, 32161, 32176, 32216, 32226, 32229, 32241, 32245, 32249, 32251, 32252, 32267, 32281, 32286, 32305, 32306, 32311, 32321, 32337, 32346, 32348, 32358, 32370, 32391, 32396, 32417, 32419, 32443, 32446, 32478, 32480, 32486, 32488, 32491, 32500, 32526, 32527, 32529, 32530, 32555, 32572, 32587, 32591, 32595, 32597, 32612, 32617, 32646, 32650, 32652, 32658, 32667, 32687, 32692, 32702, 32703, 32710, 32728, 32729, 32751, 32752, 32770, 32789, 32804, 32805, 32814, 32817, 32826, 32834, 32842, 32849, 32856, 32859, 32862, 32874, 32878, 32886, 32890, 32897, 32922, 32929, 32946, 32959, 32974, 32993, 33005, 33015, 33017, 33027, 33040, 33043, 33088, 33101, 33115, 33117, 33127, 33145, 33150, 33154, 33156, 33159, 33162, 33193, 33206, 33214, 33221, 33230, 33234, 33260, 33261, 33266, 33310, 33322, 33356, 33363, 33364, 33367, 33377, 33390, 33404, 33407, 33410, 33412, 33431, 33437, 33438, 33441, 33447, 33452, 33463, 33477, 33480, 33553, 33573, 33578, 33592, 33603, 33634, 33639, 33640, 33650, 33668, 33673, 33678, 33700, 33717, 33733, 33766, 33781, 33784, 33802, 33834, 33835, 33842, 33844, 33850, 33861, 33868, 33904, 33919, 33925, 33930, 33931, 33968, 33995, 34006, 34012, 34014, 34026, 34030, 34031, 34037, 34039, 34087, 34094, 34101, 34120, 34153, 34163, 34185, 34200, 34217, 34220, 34237, 34239, 34263, 34265, 34271, 34295, 34297, 34298, 34300, 34301, 34302, 34311, 34315, 34318, 34324, 34346, 34351, 34359, 34376, 34378, 34384, 34400, 34436, 34447, 34459, 34482, 34493, 34494, 34500, 34525, 34543, 34555, 34584, 34585, 34588, 34598, 34599, 34602, 34610, 34655, 34666, 34673, 34678, 34683, 34687, 34699, 34705, 34706, 34712, 34713, 34714, 34724, 34728, 34729, 34732, 34738, 34747, 34752, 34756, 34773, 34789, 34795, 34806, 34807, 34808, 34815, 34823, 34840, 34850, 34865, 34872, 34881, 34884, 34916, 34935, 34946, 34963, 35021, 35039, 35041, 35043, 35046, 35048, 35049, 35086, 35094, 35108, 35110, 35118, 35122, 35124, 35125, 35158, 35179, 35196, 35210, 35214, 35225, 35233, 35253, 35260, 35277, 35278, 35279, 35318, 35325, 35339, 35341, 35343, 35347, 35358, 35365, 35371, 35373, 35374, 35377, 35383, 35419, 35430, 35435, 35445, 35452, 35491, 35495, 35503, 35533, 35550, 35557, 35559, 35565, 35578, 35580, 35590, 35597, 35606, 35619, 35633, 35634, 35639, 35642, 35660, 35662, 35663, 35673, 35691, 35702, 35708, 35709, 35718, 35761, 35775, 35776, 35777, 35792, 35809, 35866, 35892, 35910, 35912, 35928, 35951, 35954, 35966, 35998, 36001, 36002, 36008, 36018, 36020, 36031, 36038, 36049, 36053, 36055, 36088, 36095, 36108, 36123, 36162, 36165, 36170, 36178, 36228, 36229, 36235, 36236, 36257, 36260, 36261, 36268, 36285, 36286, 36289, 36304, 36306, 36307, 36313, 36321, 36323, 36340, 36349, 36357, 36360, 36361, 36362, 36364, 36369, 36392, 36414, 36419, 36434, 36435, 36438, 36442, 36443, 36451, 36470, 36477, 36479, 36480, 36486, 36501, 36518, 36532, 36538, 36539, 36544, 36571, 36577, 36579, 36588, 36590, 36620, 36628, 36629, 36645, 36659, 36660, 36686, 36703, 36715, 36719, 36734, 36736, 36737, 36761, 36771, 36781, 36824, 36826, 36832, 36844, 36848, 36917, 36937, 36942, 36945, 36951, 36977, 37020, 37031, 37040, 37041, 37046, 37061, 37071, 37111, 37112, 37125, 37163, 37171, 37207, 37223, 37228, 37240, 37246, 37251, 37252, 37265, 37271, 37277, 37281, 37295, 37296, 37314, 37315, 37346, 37350, 37377, 37380, 37390, 37391, 37403, 37423, 37428, 37432, 37438, 37443, 37452, 37453, 37457, 37466, 37495, 37535, 37539, 37579, 37612, 37623, 37625, 37654, 37657, 37675, 37676, 37699, 37708, 37711, 37729, 37736, 37787, 37797, 37805, 37832, 37836, 37856, 37865, 37870, 37875, 37880, 37889, 37905, 37939, 37951, 37953, 37966, 37967, 37971, 37985, 38038, 38043, 38066, 38081, 38101, 38113, 38118, 38136, 38138, 38145, 38151, 38181, 38236, 38253, 38257, 38263, 38293, 38327, 38332, 38337, 38340, 38357, 38385, 38409, 38413, 38416, 38452, 38455, 38471, 38507, 38516, 38521, 38532, 38536, 38545, 38548, 38550, 38554, 38562, 38571, 38584, 38585, 38590, 38593, 38603, 38609, 38611, 38632, 38653, 38671, 38692, 38694, 38753, 38756, 38775, 38777, 38778, 38813, 38820, 38854, 38858, 38860, 38879, 38881, 38889, 38893, 38928, 38930, 38983, 38988, 39054, 39084, 39103, 39108, 39126, 39129, 39139, 39167, 39190, 39231, 39257, 39277, 39284, 39318, 39328, 39344, 39350, 39401, 39440, 39447, 39464, 39485, 39489, 39515, 39528, 39535, 39549, 39550, 39566, 39600, 39602, 39604, 39635, 39639, 39643, 39657, 39662, 39717, 39726, 39747, 39776, 39777, 39778, 39781, 39826, 39828, 39830, 39842, 39843, 39848, 39861, 39866, 39894, 39902, 39905, 39912, 39955, 39962, 39978, 39985, 39986, 39994, 40013, 40053, 40072, 40112, 40115, 40138, 40173, 40188, 40195, 40209, 40214, 40218, 40224, 40229, 40241, 40256, 40288, 40300, 40315, 40329, 40349, 40355, 40358, 40393, 40423, 40442, 40458, 40463, 40465, 40469, 40482, 40510, 40515, 40520, 40529, 40530, 40541, 40577, 40584, 40591, 40595, 40607, 40656, 40668, 40685, 40687, 40692, 40693, 40699, 40729, 40739, 40747, 40800, 40838, 40898, 40904, 40922, 40933, 40938, 40941, 40942, 40968, 40991, 41002, 41010, 41030, 41040, 41074, 41176, 41187, 41193, 41204, 41217, 41233, 41236, 41249, 41300, 41322, 41331, 41337, 41340, 41343, 41379, 41383, 41445, 41483, 41490, 41517, 41536, 41547, 41569, 41573, 41575, 41580, 41583, 41596, 41612, 41613, 41619, 41624, 41651, 41656, 41661, 41663, 41664, 41696, 41697, 41704, 41712, 41714, 41726, 41730, 41735, 41745, 41747, 41754, 41756, 41758, 41766, 41767, 41792, 41801, 41805, 41816, 41821, 41834, 41848, 41856, 41876, 41879, 41908, 41927, 41957, 41973, 42017, 42038, 42047, 42049, 42061, 42089, 42096, 42099, 42117, 42123, 42133, 42140, 42157, 42177, 42179, 42182, 42199, 42201, 42255, 42274, 42292, 42314, 42316, 42317, 42319, 42327, 42347, 42348, 42353, 42410, 42426, 42443, 42453, 42501, 42504, 42508, 42520, 42531, 42556, 42561, 42565, 42575, 42605, 42623, 42634, 42636, 42641, 42661, 42663, 42690, 42731, 42742, 42744, 42751, 42755, 42767, 42772, 42804, 42806, 42809, 42814, 42898, 42924, 42928, 42942, 42948, 42966, 43008, 43021, 43081, 43083, 43103, 43106, 43142, 43160, 43191, 43212, 43233, 43243, 43248, 43278, 43296, 43341, 43350, 43364, 43382, 43384, 43403, 43427, 43457, 43477, 43488, 43494, 43508, 43515, 43583, 43593, 43598, 43608, 43611, 43635, 43676, 43677, 43694, 43722, 43775, 43777, 43779, 43787, 43793, 43801, 43802, 43822, 43827, 43829, 43833, 43858, 43859, 43873, 43874, 43880, 43896, 43897, 43931, 43950, 43961, 43965, 43977, 43990, 44025, 44066, 44115, 44126, 44174, 44198, 44269, 44304, 44305, 44321, 44326, 44342, 44364, 44377, 44436, 44471, 44486, 44489, 44491, 44505, 44543, 44554, 44566, 44577, 44584, 44600, 44602, 44640, 44650, 44656, 44662, 44673, 44710, 44736, 44743, 44759, 44773, 44816, 44859, 44905, 44910, 44932, 44974, 44997, 45015, 45057, 45076, 45100, 45106, 45134, 45159, 45193, 45215, 45232, 45259, 45277, 45338, 45457, 45496, 45497, 45560, 45590, 45607, 45608, 45614, 45631, 45656, 45683, 45709, 45732, 45736, 45740, 45754, 45818, 45925, 45935, 45948, 45955, 45986, 45998, 46017, 46036, 46065, 46079, 46103, 46110, 46165, 46230, 46266, 46292, 46294, 46301, 46307, 46315, 46321, 46382, 46386, 46394, 46417, 46443, 46485, 46517, 46564, 46600, 46628, 46644, 46651, 46673, 46684, 46737, 46776, 46785, 46855, 46942, 47040, 47084, 47162, 47232, 47237, 47286, 47292, 47346, 47355, 47396, 47458, 47482, 47491, 47579, 47597, 47602, 47682, 47703, 47741, 47823, 47835, 47839, 47840, 47894, 47928, 47964, 47966, 47977, 48004, 48029, 48075, 48090, 48101, 48108, 48141, 48144, 48157, 48159, 48166, 48176, 48212, 48222, 48256, 48264, 48322, 48329, 48341, 48344, 48346, 48349, 48377, 48380, 48384, 48416, 48460, 48470, 48491, 48502, 48512, 48521, 48528, 48539, 48558, 48582, 48599, 48618, 48624, 48631, 48664, 48729, 48800, 48810, 48835, 48897, 48907, 48921, 48922, 48960, 48982, 49003, 49019, 49032, 49050, 49063, 49077, 49167, 49205, 49211, 49238, 49259, 49267, 49270, 49272, 49310, 49328, 49415, 49430, 49443, 49524, 49558, 49605, 49712, 49747, 49753, 49797, 49884, 49897, 49916, 49940, 49966, 49973, 49974, 49981, 50002, 50005, 50015, 50096, 50098, 50115, 50126, 50158, 50208, 50218, 50227, 50244, 50247, 50265, 50296, 50331, 50333, 50336, 50353, 50367, 50370, 50371, 50388, 50421, 50450, 50463, 50473, 50494, 50517, 50525, 50539, 50547, 50548, 50581, 50669, 50672, 50708, 50730, 50763, 50821, 50832, 50856, 50876, 50907, 50913, 50919, 50953, 50961, 50969, 51025, 51033, 51041, 51047, 51096, 51104, 51118, 51178, 51179, 51207, 51218, 51224, 51251, 51271, 51278, 51298, 51300, 51327, 51337, 51339, 51343, 51391, 51394, 51396, 51404, 51405, 51416, 51465, 51470, 51471, 51481, 51544, 51564, 51577, 51594, 51602, 51638, 51646, 51688, 51695, 51712, 51739, 51745, 51832, 51842, 51907, 51911, 51919, 51935, 51944, 51951, 51967, 51972, 51975, 52020, 52026, 52041, 52043, 52045, 52060, 52088, 52098, 52100, 52130, 52138, 52151, 52198, 52279, 52280, 52318, 52340, 52343, 52353, 52356, 52375, 52382, 52389, 52457, 52459, 52486, 52492, 52495, 52497, 52583, 52619, 52758, 52759, 52777, 52784, 52786, 52819, 52843, 52860, 52861, 52911, 52914, 52919, 52922, 52930, 52942, 52943, 52958, 52995, 53006, 53025, 53034, 53044, 53055, 53076, 53082, 53088, 53095, 53107, 53109, 53120, 53128, 53135, 53143, 53144, 53189, 53192, 53204, 53212, 53215, 53216, 53230, 53234, 53241, 53343, 53363, 53398, 53399, 53440, 53443, 53454, 53513, 53568, 53593, 53608, 53618, 53629, 53649, 53679, 53690, 53716, 53720, 53769, 53784, 53786, 53787, 53805, 53818, 53862, 53888, 53908, 53923, 53961, 54013, 54028, 54035, 54036, 54109, 54287, 54335, 54340, 54386, 54387, 54409, 54419, 54431, 54447, 54469, 54581, 54586, 54611, 54677, 54682, 54727, 54756, 54782, 54809, 54818, 54820, 54890, 54903, 54904, 54907, 54914, 54920, 54930, 54960, 54977, 54991, 54992, 54994, 54997, 55026, 55048, 55060, 55087, 55099, 55101, 55102, 55117, 55137, 55146, 55164, 55170, 55175, 55181, 55204, 55207, 55209, 55222, 55238, 55256, 55268, 55278, 55283, 55302, 55303, 55328, 55337, 55360, 55366, 55381, 55385, 55387, 55408, 55433, 55435, 55441, 55465, 55466, 55480, 55486, 55491, 55496, 55521, 55526, 55535, 55537, 55538, 55560, 55570, 55574, 55619, 55622, 55625, 55654, 55657, 55670, 55691, 55693, 55702, 55721, 55722, 55736, 55743, 55762, 55788, 55821, 55851, 55854, 55865, 55869, 55887, 55895, 55921, 55953, 55964, 55968, 55976, 55977, 55981, 55985, 55986, 56005, 56016, 56023, 56040, 56042, 56055, 56072, 56081, 56086, 56100, 56110, 56140, 56150, 56201, 56215, 56221, 56234, 56241, 56249, 56262, 56266, 56267, 56272, 56293, 56312, 56313, 56353, 56361, 56382, 56396, 56398, 56403, 56433, 56436, 56465, 56468, 56491, 56503, 56522, 56543, 56548, 56549, 56555, 56559, 56571, 56584, 56590, 56618, 56627, 56649, 56661, 56692, 56740, 56746, 56747, 56749, 56762, 56776, 56797, 56809, 56810, 56823, 56827, 56833, 56834, 56840, 56849, 56851, 56858, 56861, 56864, 56894, 56905, 56907, 56918, 56929, 56931, 56961, 56974, 56977, 56985, 56996, 57015, 57028, 57125, 57167, 57179, 57191, 57211, 57214, 57253, 57255, 57259, 57281, 57296, 57303, 57314, 57330, 57351, 57384, 57390, 57402, 57418, 57432, 57479, 57494, 57508, 57536, 57554, 57585, 57587, 57594, 57623, 57629, 57632, 57652, 57661, 57663, 57673, 57675, 57680, 57692, 57704, 57742, 57745, 57759, 57760, 57764, 57765, 57774, 57801, 57821, 57827, 57860, 57872, 57885, 57895, 57927, 57943, 57964, 57965, 57977, 57984, 57997, 58038, 58100, 58117, 58156, 58196, 58203, 58208, 58219, 58233, 58236, 58237, 58240, 58246, 58248, 58253, 58296, 58331, 58345, 58354, 58380, 58390, 58401, 58402, 58405, 58407, 58466, 58576, 58588, 58603, 58656, 58705, 58717, 58725, 58758, 58789, 58800, 58809, 58832, 58833, 58845, 58990, 59009, 59017, 59018, 59044, 59094, 59142, 59203, 59212, 59244, 59256, 59383, 59400, 59540, 59544, 59720, 59733, 59734, 59771, 59793, 59809, 59882, 59906, 59928, 60008, 60081, 60142, 60151, 60183, 60194, 60223, 60262, 60316, 60400, 60474, 60475, 60484, 60532, 60536, 60537, 60550, 60553, 60598, 60619, 60630, 60645, 60675, 60695, 60713, 60714, 60767, 60780, 60847, 60893, 60962, 61053, 61061, 61084, 61094, 61152, 61158, 61189, 61209, 61277, 61331, 61333, 61343, 61347, 61351, 61366, 61378, 61382, 61384, 61480, 61501, 61505, 61506, 61518, 61536, 61546, 61561, 61568, 61575, 61647, 61654, 61657, 61701, 61715, 61726, 61805, 61806, 61815, 61851, 61864, 61865, 62120, 62204, 62247, 62432, 62451, 62509, 62614, 62664, 62843, 62925, 62950, 63113, 63117, 63119, 63162, 63218, 63227, 63351, 63819, 63834, 63948, 63984, 64062, 64185, 64188, 64214, 64387, 64414, 64476, 64478, 64503, 64508, 64620, 64687, 64741, 64761, 64786, 64791, 64835, 64839, 64869, 64888, 64913, 64959, 65020, 65038, 65045, 65051, 65103, 65201, 65204, 65209, 65295, 65326, 65371, 65400, 65429, 65536, 65565, 65635, 65682, 65780, 65795, 65844, 65884, 65928, 65934, 65963, 65997, 66002, 66007, 66123, 66128, 66129, 66184, 66201, 66218, 66232, 66234, 66262, 66277, 66282, 66335, 66349, 66422, 66424, 66509, 66512, 66536, 66543, 66551, 66572, 66600, 66611, 66617, 66633, 66638, 66652, 66745, 66828, 66838, 66845, 66853, 66862, 66869, 66880, 66902, 66915, 66938, 66945, 66962, 67040, 67154, 67162, 67173, 67183, 67189, 67192, 67205, 67215, 67222, 67230, 67247, 67264, 67268, 67329, 67331, 67341, 67346, 67360, 67386, 67396, 67418, 67436, 67442, 67446, 67450, 67472, 67491, 67503, 67526, 67567, 67658, 67661, 67668, 67670, 67678, 67690, 67707, 67709, 67713, 67728, 67742, 67766, 67782, 67809, 67821, 67826, 67848, 67867, 67909, 67942, 67949, 67952, 67992, 67999, 68021, 68035, 68061, 68091, 68096, 68107, 68126, 68130, 68137, 68146, 68150, 68195, 68213, 68242, 68245, 68248, 68272, 68349, 68375, 68382, 68412, 68422, 68423, 68508, 68519, 68525, 68574, 68610, 68627, 68629, 68682, 68695, 68737, 68743, 68776, 68797, 68810, 68820, 68837, 68844, 68848, 68878, 68888, 68916, 68935, 68966, 68970, 69008, 69045, 69046, 69079, 69130, 69200, 69243, 69267, 69326, 69353, 69471, 69495, 69535, 69547, 69555, 69565, 69575, 69592, 69632, 69669, 69766, 69772, 69851, 69852, 69861, 69862, 69869, 69888, 69928, 69953]\n",
    "#args=0\n",
    "#for i in errors:\n",
    " #   args+=y_target[i]\n",
    "#print(args)\n",
    "#sys.exit(-1)\n",
    "y_target_bin=make_mnist_binary(y_target)\n",
    "y_source_bin=make_mnist_binary(y_source)\n",
    "for alpha in alphas:\n",
    "    '''\n",
    "    if alpha==0:\n",
    "        x_bound=x_source\n",
    "        y_bound=y_source\n",
    "        if Binary:\n",
    "            y_bound=make_mnist_binary(y_bound)\n",
    "            x_bound, x_prior, y_bound , y_prior = train_test_split(x_source,y_source_bin,test_size=alpha,random_state=69105)\n",
    "        else:\n",
    "            y_source_bin=make_mnist_binary(y_source)\n",
    "            x_bound, x_prior, y_bound , y_prior = train_test_split(x_source,y_source_bin,test_size=alpha,random_state=69105)\n",
    "    else:\n",
    "        x_bound, x_prior, y_bound , y_prior = train_test_split(x_source,y_source,test_size=alpha,random_state=69105)\n",
    "    '''\n",
    "    print(\"alpha:\"+str(alpha))\n",
    "    if alpha==0:\n",
    "        x_bound=x_source\n",
    "        y_bound=y_source_bin\n",
    "    else:\n",
    "        x_bound, x_prior, y_bound , y_prior = train_test_split(x_source,y_source_bin,test_size=alpha,random_state=69105)\n",
    "    for epsilon in epsilons:\n",
    "        print(\"epsilon:\"+str(epsilon))\n",
    "        for sigma in sigmas:    \n",
    "            print(\"sigma:\"+str(sigma))\n",
    "            results=germain_bound(x_bound,y_bound,x_target,y_target_bin,alpha,sigma,epsilon,TASK,Binary=Binary)\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_germain_res(alpha,epsilon,sigma,Binary=False,do_errorbars=True,plot_parts=False):\n",
    "    sigma_tmp=sigma\n",
    "    sigma=sigma[0]*10**(-1*sigma[1])\n",
    "    import pandas as pd\n",
    "    if Binary:\n",
    "        result_path=\"results/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"+str(sigma_tmp[0])+str(sigma_tmp[1])\n",
    "        plt.title(\"Binary: \"+r\"$\\alpha$=\"+str(alpha)+r\" $\\epsilon$=\"+str(epsilon)+r\" $\\sigma$=\"+str(sigma))\n",
    "    else:\n",
    "        result_path=\"results/\"+\"task\"+str(TASK)+\"/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"+str(sigma_tmp[0])+str(sigma_tmp[1])\n",
    "        plt.title(r\"$\\alpha$=\"+str(alpha)+r\" $\\epsilon$=\"+str(epsilon)+r\" $\\sigma$=\"+str(sigma))\n",
    "    results=pd.read_pickle(result_path+\"_results.pkl\") #\n",
    "    num_classifiers=4#results['bestparam']\n",
    "    #print(num_classifiers)\n",
    "    \n",
    "    N=num_classifiers*2 # how many values we got the estimate of the standard deviation from\n",
    "    N2=num_classifiers\n",
    "    z_alpha=1.96 ## in actuality we could probably use some degree of freedom thing here to make it more correct as\n",
    "                 ## the standard deviation is unknown, but we do this for now and pretend like we know the standard deviation\n",
    "        \n",
    "    if do_errorbars==False:\n",
    "        plt.plot(results[\"Weightupdates\"],results[\"train_germain\"],'k^-')\n",
    "        plt.plot(results[\"Weightupdates\"],results[\"target_germain\"],'m^-')\n",
    "    else:\n",
    "        plt.errorbar(results[\"Weightupdates\"],results[\"train_germain\"],yerr=z_alpha*results['error_std']/N,fmt='k^-')\n",
    "        plt.errorbar(results[\"Weightupdates\"],results[\"target_germain\"],yerr=z_alpha*results['target_error_std']/N,fmt='m^-')\n",
    "    \n",
    "    plt.xlabel(\"Weight updates\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    if plot_parts==True:\n",
    "        if do_errorbars==False:\n",
    "            plt.plot(results[\"Weightupdates\"],results['boundpart1_germain'],'*')\n",
    "            plt.plot(results[\"Weightupdates\"],results['boundpart2_germain'],'+-')\n",
    "            plt.plot(results[\"Weightupdates\"],results['boundpart3_germain'],'X')\n",
    "            plt.plot(results[\"Weightupdates\"],results['boundpart4_germain'],'D')\n",
    "            plt.plot(results[\"Weightupdates\"],results['boundpart5_germain'],'o-')\n",
    "            plt.plot(results[\"Weightupdates\"],results[\"germain_bound\"],'r*-')\n",
    "            plt.plot(results[\"Weightupdates\"],results[\"beta_bound\"])\n",
    "            plt.legend([\"Sample error\",\"Target error\",\"sample error part\",\"dis_rho part\",\"KL part\",\"lambda_rho\",\"extra constant\",\"Dis-rho Bound\",\"beta bound\"], loc = 0)#,\"e_s\",\"e_t\",\"d_tx\",\"d_sx\"])#,\"Bound\"\n",
    "            plt.title(r\"$\\alpha$=\"+str(alpha)+r\" $\\epsilon$=\"+str(epsilon)+r\" $\\sigma$=\"+str(sigma))\n",
    "            plt.show()\n",
    "        else:\n",
    "            #### \n",
    "            #### Errors here are calculated with the amount of classifiers drawn as the n that divides\n",
    "            #### with n=2*(drawn classifiers) for errors and n=(drawn classifiers)^2 for other boundparts\n",
    "            #### \n",
    "            ####\n",
    "            plt.errorbar(results[\"Weightupdates\"],results['boundpart1_germain'],yerr=z_alpha*results['error_std']/N,fmt='*')\n",
    "            plt.errorbar(results[\"Weightupdates\"],results['boundpart2_germain'],yerr=z_alpha*(results['d_tx_std']+results['d_sx_std'])/N2,fmt='+-')\n",
    "            #plt.errorbar(results[\"Weightupdates\"],results['boundpart3_germain'],yerr=z_alpha*results['_std']/N,fmt='X')\n",
    "            plt.plot(results[\"Weightupdates\"],results['boundpart3_germain'],'X')\n",
    "            plt.errorbar(results[\"Weightupdates\"],results['boundpart4_germain'],yerr=z_alpha*(results['e_t_std']+results['e_s_std'])/N2,fmt='D')\n",
    "            plt.plot(results[\"Weightupdates\"],results['boundpart5_germain'],'o-')\n",
    "            #plt.errorbar(results[\"Weightupdates\"],results['boundpart5_germain'],yerr=z_alpha*results['_std']/N,fmt='o-')\n",
    "            plt.errorbar(results[\"Weightupdates\"],results[\"germain_bound\"],\n",
    "                         yerr=z_alpha*(results['error_std']/N+(results['e_t_std']+results['e_s_std']+results['d_tx_std']+results['d_sx_std'])/N2),fmt='r*-')\n",
    "            plt.errorbar(results[\"Weightupdates\"],results[\"beta_bound\"],\n",
    "                         yerr=(results['e_s_std']+results['d_tx_std'])/N2)\n",
    "            plt.legend([\"Sample error\",\"Target error\",\"sample error part\",\"dis_rho part\",\"KL part\",\"lambda_rho\",\"extra constant\",\"Dis-rho Bound\",\"beta bound\"], loc = 0)#,\"e_s\",\"e_t\",\"d_tx\",\"d_sx\"])#,\"Bound\"\n",
    "            plt.title(r\"$\\alpha$=\"+str(alpha)+r\" $\\epsilon$=\"+str(epsilon)+r\" $\\sigma$=\"+str(sigma))\n",
    "            plt.show()\n",
    "        \n",
    "    else:\n",
    "        if do_errorbars==False:\n",
    "            plt.plot(results[\"Weightupdates\"],results[\"germain_bound\"],'r*-')\n",
    "            plt.plot(results[\"Weightupdates\"],results[\"beta_bound\"])\n",
    "            plt.legend([\"Sample error\",\"Target error\",\"Dis-rho Bound\",\"beta bound\"], loc = 0)#,\"e_s\",\"e_t\",\"d_tx\",\"d_sx\"])#,\"Bound\"\n",
    "            plt.title(r\"$\\alpha$=\"+str(alpha)+r\" $\\epsilon$=\"+str(epsilon)+r\" $\\sigma$=\"+str(sigma))\n",
    "            plt.show()\n",
    "        else:\n",
    "            disrho_error=(results['error_std']/N+(results['e_t_std']+results['e_s_std']+results['d_tx_std']+results['d_sx_std'])/N2)\n",
    "            print(disrho_error)\n",
    "            plt.errorbar(results[\"Weightupdates\"],results[\"germain_bound\"],\n",
    "                         yerr=z_alpha*disrho_error,fmt='r*-',capsize=8)\n",
    "            plt.errorbar(results[\"Weightupdates\"],results[\"beta_bound\"],\n",
    "                         yerr=(results['e_s_std']+results['d_tx_std'])/N2,capsize=8)\n",
    "            plt.legend([\"Sample error\",\"Target error\",\"Dis-rho Bound\",\"beta bound\"], loc = 0)#,\"e_s\",\"e_t\",\"d_tx\",\"d_sx\"])#,\"Bound\"\n",
    "            plt.title(r\"$\\alpha$=\"+str(alpha)+r\" $\\epsilon$=\"+str(epsilon)+r\" $\\sigma$=\"+str(sigma))\n",
    "            plt.savefig(\"Plot\"+str(10*alpha)+\".png\",dpi=300)\n",
    "            plt.show()\n",
    "            #plt.savefig(\"Plot\"+str(10*alpha)+\".png\",dpi=300)\n",
    "sigma=[3,2]\n",
    "#plot_germain_res(0,0.03,sigma,True)\n",
    "plot_germain_res(0,0.03,sigma,True,True)\n",
    "plot_germain_res(0.3,0.03,sigma,True,True)\n",
    "#plot_germain_res(0.3,0.001,sigma,True)\n",
    "#plot_germain_res(0.3,0.01,sigma,True)\n",
    "#plot_germain_res(0.1,0.001,sigma,True)\n",
    "#plot_germain_res(0.1,0.01,sigma,True)\n",
    "#plot_germain_res(0.3,0.03,sigma,True)\n",
    "#plot_germain_res(0.1,0.03,sigma,True)\n",
    "#plot_germain_res(0.3,0.03,sigma,True)\n",
    "#plot_germain_res(0.3,0.01,sigma,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load module\n",
    "#mymodule = SourceFileLoader('train', path_to_root_file+'mnist_transfer/experiments/training.py').load_module()\n",
    "#mymodule2 = SourceFileLoader('plotting', path_to_root_file+'mnist_transfer/results/plotting.py').load_module()\n",
    "#from plotting import * \n",
    "#from train import *\n",
    "\n",
    "def joint_error(a,b,true_label):\n",
    "    ## expected joint error\n",
    "        shapes=a.shape\n",
    "        e_s=0\n",
    "        # e_S= E_h,h' E_x,y L(h(x),y)L(h'(x),y)  \n",
    "        arr1=np.abs(a-true_label)\n",
    "        arr1=np.sum(arr1,axis=1)\n",
    "        arr1=np.divide(arr1, 2)\n",
    "        arr2=np.abs(b-true_label)\n",
    "        arr2=np.sum(arr2,axis=1)\n",
    "        arr2=np.divide(arr2, 2)\n",
    "        #print(arr1)\n",
    "        #print(arr2)\n",
    "        e_s=arr1.T@arr2\n",
    "        e_s/=(shapes[0])\n",
    "        return e_s\n",
    "\n",
    "\n",
    "       \n",
    "    \n",
    "\n",
    "def classifier_disagreement(a,b):\n",
    "    ### classifier disagreement, i.e. R(h,h')= 1/n sum(L( h(x),h'(x) ))\n",
    "    shapes=a.shape\n",
    "    d=0\n",
    "    arr=np.abs(a-b)\n",
    "    arr=np.sum(arr,axis=1)\n",
    "    d=np.count_nonzero(arr)\n",
    "    d/=(shapes[0])\n",
    "    return d\n",
    "#a=np.array([[1,0],[1,0],[1,0]])\n",
    "#b=np.array([[1,0],[0,1],[1,0]])\n",
    "#d=np.array([[1,0],[1,0],[0,1]])\n",
    "#c=classifier_disagreement(a,b)\n",
    "#print(c)\n",
    "#e=joint_error(a,b,d)\n",
    "#print(e)\n",
    "def compute_beta(alpha,epsilon,sigma,Binary=False):\n",
    "    ### computes the beta bound from results and saves it to the result file along with its parts\n",
    "    sigma_tmp=sigma\n",
    "    sigma=sigma[0]*10**(-1*sigma[1])\n",
    "    import pandas as pd\n",
    "    if Binary:\n",
    "        result_path=\"results/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"+str(sigma_tmp[0])+str(sigma_tmp[1])\n",
    "        plt.title(\"Binary: \"+r\"$\\alpha$=\"+str(alpha)+r\" $\\epsilon$=\"+str(epsilon)+r\" $\\sigma$=\"+str(sigma))\n",
    "    else:\n",
    "        result_path=\"results/\"+\"task\"+str(TASK)+\"/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"+str(sigma_tmp[0])+str(sigma_tmp[1])\n",
    "        plt.title(r\"$\\alpha$=\"+str(alpha)+r\" $\\epsilon$=\"+str(epsilon)+r\" $\\sigma$=\"+str(sigma))\n",
    "    results=pd.read_pickle(result_path+\"_results.pkl\") \n",
    "    \n",
    "   \n",
    "    \n",
    "    bound, params, parts = grid_search(results['train_germain'],results['e_s'],results['e_t'],results['d_tx'],results['d_sx'],results['KL'],0.05,70000,len(results['KL']),beta_bound=True)\n",
    "    results[\"beta_bound\"]=bound\n",
    "    results['beta_boundpart1']=parts[0]\n",
    "    results['beta_boundpart2']=parts[1]\n",
    "    results['beta_boundpart3']=parts[2]\n",
    "    if Binary:\n",
    "        result_path=\"results/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"+str(sigma_tmp[0])+str(sigma_tmp[1])\n",
    "    else:\n",
    "        result_path=\"results/\"+\"task\"+str(TASK)+\"/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"+str(sigma_tmp[0])+str(sigma_tmp[1])\n",
    "    with open(path_to_root_file+'mnist_transfer/'+result_path+\"_results.pkl\",'wb') as f:\n",
    "        pickle.dump(results,f)\n",
    "    f.close()\n",
    "#compute_beta(0.3,0.03,[3,3],True)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "def make_sigma_plot(alpha,epsilon,Binary=True):\n",
    "    low_bound=[]\n",
    "    x_sigmas=[]\n",
    "    sigmas=[]\n",
    "    \n",
    "    for i in range(2,9):  \n",
    "        sigmas.append([3,i])#3*10**(-i))\n",
    "        if(i==8):\n",
    "            break\n",
    "        sigmas.append([1,i])#10**(-i))\n",
    "        \n",
    "    #sigmas=[[3,3]]\n",
    "    for sigma in sigmas:\n",
    "        sigma_tmp=sigma\n",
    "        sigma=sigma[0]*10**(-1*sigma[1])\n",
    "        x_sigmas.append(sigma)\n",
    "        \n",
    "        if Binary:\n",
    "            result_path=\"results/\"+\"task\"+str(TASK)+\"/Binary/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"+str(sigma_tmp[0])+str(sigma_tmp[1])\n",
    "        else:\n",
    "            result_path=\"results/\"+\"task\"+str(TASK)+\"/\"+str(int(1000*epsilon))+\"_\"+str(int(100*alpha))+\"_\"+str(sigma_tmp[0])+str(sigma_tmp[1])\n",
    "\n",
    "        results=pd.read_pickle(result_path+\"_results.pkl\")\n",
    "        low_bound.append(results[\"germain_bound\"].min())\n",
    "    print(low_bound)\n",
    "    print(x_sigmas)\n",
    "    plt.title(\"Plot of minimum bound over sigma for \"+r\"$\\alpha$=\"+str(alpha)+r\" $\\epsilon$=\"+str(epsilon))\n",
    "    plt.semilogx(x_sigmas,low_bound,'k*')\n",
    "    plt.show()\n",
    "make_sigma_plot(0.3,0.03)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
