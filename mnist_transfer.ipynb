{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "#### Keras implementation of NN's which we will look at MNIST with\n",
    "\n",
    "from __future__ import print_function\n",
    "import keras as keras \n",
    "from keras import backend as K\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.constraints import max_norm\n",
    "from keras.regularizers import L2\n",
    "import tensorflow as tf\n",
    "#import tensorflow_datasets as tfds\n",
    "import scipy\n",
    "import h5py\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "svhn_path=\"../Datasets/svhn\"#\"/Home/Adam/Research/Datasets/svhn\"\n",
    "# Hyper-parameters\n",
    "K.clear_session() ## needed???\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 10\n",
    "\n",
    "img_rows, img_cols = 32, 32\n",
    "\n",
    "### ???\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only allocate 2GB of memory on the first GPU\n",
    "  # I do not know why I have to do this but gpu does not work otherwise.\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)])\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "####### label shifting and plotting\n",
    "\n",
    "#class_label=1\n",
    "#summand=1\n",
    "#delta=0.7\n",
    "\n",
    "## Note: this assumes that you have a one hot encoding and that class_label is within the range of that vector length\n",
    "def label_shift(X,y,delta,class_label):\n",
    "    ## takes a dataset and shifts the class label distribution by some margin delta \n",
    "    ## we will just give the ``removed'' entries back as the target distribution\n",
    "    N=len(y)\n",
    "    idx=[]\n",
    "    y_2=[]\n",
    "    X_2=[]\n",
    "    x_target=[]\n",
    "    y_target=[]\n",
    "    for i in range(N):\n",
    "        if(y[i][class_label]==1):\n",
    "            idx.append(i)\n",
    "    ## what proportion of the label to remove\n",
    "    M=int(len(idx)*delta)\n",
    "    ## choose randomly delta amount of sample to include in target\n",
    "    chosen=random.sample(idx,M)\n",
    "    for i in range(N):\n",
    "        if(i in chosen):\n",
    "            x_target.append(X[i])\n",
    "            y_target.append(y[i])\n",
    "        else:\n",
    "            y_2.append(y[i])\n",
    "            X_2.append(X[i])\n",
    "    x_target=np.array(x_target)\n",
    "    y_target=np.array(y_target)\n",
    "    X_2=np.array(X_2)\n",
    "    y_2=np.array(y_2)\n",
    "    return([X_2, y_2, x_target, y_target])\n",
    "\n",
    "## Note: this assumes that you have a one hot encoding and that class_label is within the range of that vector length\n",
    "def label_shift_linear(X,y,delta,labels,decreasing=True):\n",
    "    \"\"\"\n",
    "    X: data points\n",
    "    y: labels\n",
    "    delta: percentage amount which you want to decrease for each label, i.e. slope for the shifting; delta in [0,1)\n",
    "    labels: a vector of the possible labels i.e. for MNIST we have labels=[0,1,2,3,4,5,6,7,8,9]\n",
    "    decreasing: bool to see if you want to make the shift increasing or decreasing\n",
    "    \"\"\"\n",
    "    ## takes a dataset and shifts the class label distribution for all labels by\n",
    "    ## a linearly increasing or decreasing amount\n",
    "    ## we will just remove entries of the class for now\n",
    "    \n",
    "    L=len(labels)\n",
    "    y_2=y\n",
    "    X_2=X\n",
    "    x_target=[]\n",
    "    y_target=[]\n",
    "    ## for every label go through and remove delta*label(or delta*(L-label)) amount of them (+1 to ensure overlap)\n",
    "    for label in labels:\n",
    "        if decreasing:\n",
    "            delta2=delta*(label+1)\n",
    "        else:\n",
    "            delta2=delta*(L-label+1)\n",
    "        assert(delta2<1)\n",
    "        X_2, y_2, x_target2, y_target2=label_shift(X_2,y_2,delta2,label)\n",
    "        if (label==0):\n",
    "            x_target=x_target2\n",
    "            y_target=y_target2\n",
    "        else:   \n",
    "            x_target=np.concatenate((x_target,x_target2))\n",
    "            y_target=np.concatenate((y_target,y_target2))\n",
    "        #print(\"--------------\")\n",
    "        #T=np.array(x_target)\n",
    "        #print(T.shape)\n",
    "    return([X_2, y_2, x_target, y_target])\n",
    "\n",
    "def plot_labeldist(labels,y_1,label_1):\n",
    "    \"\"\"\n",
    "    labels: a vector of the possible labels i.e. for MNIST we have labels=[0,1,2,3,4,5,6,7,8,9]\n",
    "    y_1: labels of dataset\n",
    "    label_1: name of dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "    # calculate the amount of label j in both datasets\n",
    "    N=len(y_1)\n",
    "   # M=len(labels)\n",
    "    densities_1=[]\n",
    "    sum=0\n",
    "    for j in labels:\n",
    "        sum=0\n",
    "        for i in range(len(y_1)):\n",
    "            if y_1[i][j]==1:\n",
    "                sum+=1\n",
    "        densities_1.append(sum)\n",
    "       \n",
    "    densities_1_rel=[]\n",
    "   \n",
    "    ## calculate relative density\n",
    "    for i in range(len(densities_1)):\n",
    "        densities_1_rel.append(densities_1[i]/(N))\n",
    "        \n",
    "    #print(densities_1_rel)\n",
    "\n",
    "\n",
    "    width = 0.35       # the width of the bars: can also be len(x) sequence\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.bar(labels, densities_1_rel, width, label=label_1)\n",
    "    #ax.set_ylim([0,1.25])\n",
    "    ax.set_xlabel('Labels')\n",
    "    ax.set_title('Label distribution')\n",
    "    ax.legend()\n",
    "    plt.rc('font', size=12, family='serif')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_splitbars(labels,y_1,y_2,label_1,label_2):\n",
    "    \"\"\"\n",
    "    labels: a vector of the possible labels i.e. for MNIST we have labels=[0,1,2,3,4,5,6,7,8,9]\n",
    "    y_1: labels of dataset1\n",
    "    y_2: labels of dataset2\n",
    "    label_1: name of dataset1\n",
    "    label_2: name of dataset2\n",
    "    \"\"\"\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "    # calculate the amount of label j in both datasets\n",
    "    densities_1=[]\n",
    "    densities_2=[]\n",
    "    sum=0\n",
    "    for j in labels:\n",
    "        sum=0\n",
    "        for i in range(len(y_1)):\n",
    "            if y_1[i][j]==1:\n",
    "                sum+=1\n",
    "        densities_1.append(sum)\n",
    "        sum=0\n",
    "        for i in range(len(y_2)):\n",
    "            if y_2[i][j]==1:\n",
    "                sum+=1\n",
    "        densities_2.append(sum)\n",
    "    densities_1_rel=[]\n",
    "    densities_2_rel=[]\n",
    "    ## calculate relative densities #### TODO: is this normalisation really what we want?\n",
    "    for i in range(len(densities_1)):\n",
    "        densities_1_rel.append(densities_1[i]/(densities_1[i]+densities_2[i]))\n",
    "        densities_2_rel.append(densities_2[i]/(densities_1[i]+densities_2[i]))\n",
    "    #print(densities_1_rel)\n",
    "    #print(densities_2_rel)\n",
    "\n",
    "\n",
    "    width = 0.35       # the width of the bars: can also be len(x) sequence\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.bar(labels, densities_1_rel, width, label=label_1)\n",
    "    ax.bar(labels, densities_2_rel, width , bottom=densities_1_rel,\n",
    "           label=label_2)\n",
    "    ax.set_ylim([0,1.25])\n",
    "    ax.set_xlabel('Labels')\n",
    "    ax.set_title('Label distribution')\n",
    "    ax.legend()\n",
    "    plt.rc('font', size=12, family='serif')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    \n",
    "######### Here we use the functions above ##############################################################\n",
    "\n",
    "###### Add train and test together and shift the distributions to create source and target distributions\n",
    "### MNIST all data\n",
    "x_full=np.append(x_train,x_test, axis=0)\n",
    "y_full=np.append(y_train,y_test, axis=0)\n",
    "### MNIST-M all data\n",
    "x_full_m=np.append(x_train_m,x_test_m, axis=0)\n",
    "y_full_m=np.append(y_train_m,y_test_m, axis=0)\n",
    "#x_shift,y_shift,x_shift_target,y_shift_target =label_shift(x_train,y_train,1/2,7)\n",
    "x_shift, y_shift, x_shift_target, y_shift_target =label_shift_linear(x_full,y_full,1/12,[0,1,2,3,4,5,6,7,8,9])\n",
    "x_shift_m, y_shift_m,x_shift_target_m, y_shift_target_m=label_shift_linear(x_full_m,y_full_m,1/12,[0,1,2,3,4,5,6,7,8,9],decreasing=False)\n",
    "#rint(\"test of shift\")\n",
    "#print(x_shift_target.shape)\n",
    "#print(x_shift.shape)\n",
    "#print(y_shift_target.shape)\n",
    "#print(y_shift.shape)\n",
    "plot_labeldist([0,1,2,3,4,5,6,7,8,9],y_shift_target,\"shifted, target\")\n",
    "plot_labeldist([0,1,2,3,4,5,6,7,8,9],y_shift,\"shifted, source\")\n",
    "plot_splitbars([0,1,2,3,4,5,6,7,8,9],y_shift,y_shift_m,\"MNIST, source\",\"MNIST-M, source\")\n",
    "plot_splitbars([0,1,2,3,4,5,6,7,8,9],y_shift_target,y_shift_target_m,\"MNIST, target\",\"MNIST-M, target\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### TODO: make source and target be the same amount and the proportions more symmetrical across labels if possible \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Add the label shifted datasets to each other creating the source and target domain for task 2\n",
    "x_source=np.append(x_shift,x_shift_m, axis=0)\n",
    "y_source=np.append(y_shift,y_shift_m, axis=0)\n",
    "x_target=np.append(x_shift_target,x_shift_target_m, axis=0)\n",
    "y_target=np.append(y_shift_target,y_shift_target_m, axis=0)\n",
    "#print(x_shift.shape)\n",
    "#print(x_shift_target.shape)\n",
    "## sanity check for label distribution, seems ok to me\n",
    "#plot_labeldist([0,1,2,3,4,5,6,7,8,9],y_full,\"MNIST\")\n",
    "#plot_labeldist([0,1,2,3,4,5,6,7,8,9],y_full_m,\"MNIST-M\")\n",
    "#plot_labeldist([0,1,2,3,4,5,6,7,8,9],y_source,\"MNIST+MNIST-M, source\")\n",
    "#plot_labeldist([0,1,2,3,4,5,6,7,8,9],y_target,\"MNIST+MNIST-M, target\")\n",
    "\n",
    "\n",
    "##### make index lists for train and test splits for source\n",
    "\n",
    "#N=70000 ideally, however, how can we ensure this when the amount differs between labels?\n",
    "## maybe we do not care too much about it exactly\n",
    "N=len(y_source)\n",
    "M=len(x_shift)\n",
    "train2=[]\n",
    "train1=[]\n",
    "test1=[]\n",
    "test2=[]\n",
    "ntr1=round(0.8*M)\n",
    "ntr2=round(0.8*N)\n",
    "### sample n_tr datapoints (w/o replacement) to be the training set at random\n",
    "## do this 10 times and save the indices into a file along with the training ones\n",
    "for i in range(10):\n",
    "    T=random.sample(range(M),M)\n",
    "    T2=random.sample(range(N),N)\n",
    "    train1.append(T[:ntr1])\n",
    "    train2.append(T2[:ntr2])\n",
    "    test1.append(T[ntr1:])\n",
    "    test2.append(T2[ntr2:])\n",
    "\n",
    "task1=[train1,test1]\n",
    "task2=[train2,test2]\n",
    "import pickle\n",
    "pkl_file=open('splits_task1.pkl','wb')\n",
    "listoflist=task1\n",
    "pickle.dump(listoflist,pkl_file)\n",
    "\n",
    "\n",
    "if True:\n",
    "    import pickle\n",
    "    pkl_file=open('splits_task2.pkl','wb')\n",
    "    listoflist=task2\n",
    "    pickle.dump(listoflist,pkl_file)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### load the splits from the file and make the split on the data\n",
    "import sys\n",
    "pkl_file=open('splits_task1.pkl','rb')\n",
    "split1=pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "traindata=[]\n",
    "#print(split1[0][0])\n",
    "for i in split1[0]:\n",
    "    trainidx=i\n",
    "    for j in trainidx:\n",
    "        traindata.append(x_shift[j])\n",
    "    print(traindata)\n",
    "    sys.exit(-1)\n",
    "for j in split1[1]:\n",
    "    testdata=j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean, variance 2.1985865 50.549427\n",
      "---------------Load SVHN----------------\n",
      "Training set (604388, 32, 32, 3) (604388, 10)\n",
      "Test set (26032, 32, 32, 3) (26032, 10)\n"
     ]
    }
   ],
   "source": [
    "### import svhn_cropped without grayscale\n",
    "\n",
    "# Open the file as readonly\n",
    "h5f = h5py.File('SVHN_cropped.h5', 'r')\n",
    "\n",
    "# Load the training, test and validation set\n",
    "X_train = h5f['X_train'][:]\n",
    "Y_train = h5f['y_train'][:]\n",
    "X_test = h5f['X_test'][:]\n",
    "Y_test = h5f['y_test'][:]\n",
    "X_extra = h5f['X_extra'][:]\n",
    "Y_extra = h5f['y_extra'][:]\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_extra = X_extra.astype('float32')\n",
    "\n",
    "#### make validation set from train and extra (and make extra part of train(?))\n",
    "\n",
    "X_train=np.append(X_train,X_extra, axis=0)\n",
    "Y_train=np.append(Y_train,Y_extra, axis=0)\n",
    "## normalising\n",
    "sigma=np.std(X_train)\n",
    "X_train /= sigma \n",
    "X_test /= sigma\n",
    "\n",
    "\n",
    "mu=np.mean(X_train)\n",
    "X_train -= mu\n",
    "X_test -= mu\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('mean, variance', mu, sigma)\n",
    "print(\"---------------Load SVHN----------------\")\n",
    "print('Training set', X_train.shape, Y_train.shape)\n",
    "#print('Extra set', X_extra.shape, Y_extra.shape)\n",
    "print('Test set', X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "## import svhn_cropped which is svhn in 32x32 size\n",
    "####### do not load this at the moment!\n",
    "####### do not load this at the moment!\n",
    "####### do not load this at the moment!\n",
    "####### do not load this at the moment!\n",
    "####### do not load this at the moment!\n",
    "####### do not load this at the moment!\n",
    "####### do not load this at the moment!\n",
    "# Open the file as readonly\n",
    "h5f = h5py.File('SVHN_gray.h5', 'r')\n",
    "\n",
    "# Load the training, test and validation set\n",
    "X_train = h5f['X_train'][:]\n",
    "Y_train = h5f['y_train'][:]\n",
    "X_test = h5f['X_test'][:]\n",
    "Y_test = h5f['y_test'][:]\n",
    "X_val = h5f['X_val'][:]\n",
    "Y_val = h5f['y_val'][:]\n",
    "\n",
    "# Close this file\n",
    "h5f.close()\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "## normalising\n",
    "#_train[:,axis=3] /= 255.0 \n",
    "#X_test /= 255.0\n",
    "#X_val /= 255.0\n",
    "\n",
    "print('Training set', X_train.shape, Y_train.shape)\n",
    "print('Validation set', X_val.shape, Y_val.shape)\n",
    "print('Test set', X_test.shape, Y_test.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean, variance 0.36348352 70.18035\n",
      "---------------Load MNIST----------------\n",
      "Training set (60000, 32, 32, 3) (60000, 10)\n",
      "Test set (10000, 32, 32, 3) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "## import mnist\n",
    "(x_train, lbl_train), (x_test, lbl_test) = mnist.load_data()\n",
    "x_train = np.pad(x_train,((0,0),(2,2),(2,2))) #padding to make images 32x32 and not 28x28\n",
    "x_test = np.pad(x_test,((0,0),(2,2),(2,2))) \n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "## normalising\n",
    "#x_train /= 255.0 \n",
    "#x_test /= 255.0\n",
    "\n",
    "## normalising to unit variance\n",
    "sigma=np.std(x_train)\n",
    "x_train /= sigma \n",
    "x_test /= sigma\n",
    "\n",
    "## mean subtraction\n",
    "mu=np.mean(x_train)\n",
    "x_train -= mu\n",
    "x_test -= mu\n",
    "## make labels into categorical classes\n",
    "y_train = keras.utils.to_categorical(lbl_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(lbl_test, num_classes)\n",
    "\n",
    "\n",
    "x_train=np.expand_dims(x_train,3)\n",
    "x_test=np.expand_dims(x_test,3)\n",
    "\n",
    "\n",
    "### make mnist into 3 channels\n",
    "x_train=np.concatenate((x_train,)*3, axis=-1)\n",
    "x_test=np.concatenate((x_test,)*3, axis=-1)\n",
    "print('mean, variance', mu, sigma)\n",
    "print(\"---------------Load MNIST----------------\")\n",
    "print('Training set', x_train.shape, y_train.shape)\n",
    "#print('Validation set', x_val.shape, y_val.shape)\n",
    "print('Test set', x_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean, variance 1.1809415 74.36859\n",
      "---------------Load MNIST-M----------------\n",
      "Training set (60000, 32, 32, 3) (60000, 10)\n",
      "Test set (10000, 32, 32, 3) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### load MNIST-M\n",
    "\n",
    "#h5f = h5py.File('MNIST-M.h5', 'r')\n",
    "\n",
    "# Load the training, test and \n",
    "#x_train_m = h5f['x_train'][:]\n",
    "#y_train_m = h5f['y_train'][:]\n",
    "#x_test_m = h5f['x_test'][:]\n",
    "#y_test_m = h5f['y_test'][:]\n",
    "import pandas as pd\n",
    "\n",
    "# Close this file\n",
    "#h5f.close()\n",
    "\n",
    "M = pd.read_pickle('mnistm_data.pkl')\n",
    "x_train_m =M['train']\n",
    "x_test_m =M['test']\n",
    "y_train_m=y_train\n",
    "y_test_m =y_test\n",
    "x_train_m = np.pad(x_train_m,((0,0),(2,2),(2,2),(0,0))) #padding to make images 32x32 and not 28x28\n",
    "x_test_m = np.pad(x_test_m,((0,0),(2,2),(2,2),(0,0)))\n",
    "\n",
    "x_train_m = x_train_m.astype('float32')\n",
    "x_test_m = x_test_m.astype('float32')\n",
    "## normalising to unit variance\n",
    "sigma=np.std(x_train_m)\n",
    "x_train_m /= sigma \n",
    "x_test_m /= sigma\n",
    "\n",
    "## mean subtraction\n",
    "mu=np.mean(x_train_m)\n",
    "x_train_m -= mu\n",
    "x_test_m -= mu\n",
    "\n",
    "print('mean, variance', mu, sigma)\n",
    "print(\"---------------Load MNIST-M----------------\")\n",
    "print('Training set', x_train_m.shape, y_train_m.shape)\n",
    "#print('Validation set', x_val.shape, y_val.shape)\n",
    "print('Test set', x_test_m.shape, y_test_m.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## preprocessing for mnist and svhn\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(X_test)\n",
    "#print(\"----------------------------------------------------\")\n",
    "#print(x_test)\n",
    "\n",
    "\n",
    "plt.imshow(x_test_m[605]) \n",
    "#print(x_test_m[303])\n",
    "print(y_test_m[605])\n",
    "#plt.imshow(x_test[605]) \n",
    "#print(x_test[303])\n",
    "print(y_test[605])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 3072)\n",
      "(60000, 3072)\n"
     ]
    }
   ],
   "source": [
    "def nppdist2sq(X,Y):\n",
    "    \"\"\" Computes the squared Euclidean distance between all pairs x in X, y in Y \"\"\"\n",
    "    C = -2*np.dot(X,Y.T)\n",
    "    nx = np.sum(np.square(X),1,keepdims=True)\n",
    "    ny = np.sum(np.square(Y),1,keepdims=True)\n",
    "    D = (C + ny.T) + nx\n",
    "    return D\n",
    "\n",
    "#print(nppdist2sq(x_train[0],x_train_m[0]))\n",
    "def compute_kernel(x, y):\n",
    "    x_size = tf.shape(x)[0]\n",
    "    y_size = tf.shape(y)[0]\n",
    "    dim = tf.shape(x)[1]\n",
    "    tiled_x = tf.tile(tf.reshape(x, tf.stack([x_size, 1, dim])), tf.stack([1, y_size, 1]))\n",
    "    tiled_y = tf.tile(tf.reshape(y, tf.stack([1, y_size, dim])), tf.stack([x_size, 1, 1]))\n",
    "    return tf.exp(-tf.reduce_mean(tf.square(tiled_x - tiled_y), axis=2) / tf.cast(dim, tf.float32))\n",
    "\n",
    "def compute_mmd(x, y, sigma_sqr=1.0):\n",
    "    x_kernel = compute_kernel(x, x)\n",
    "    y_kernel = compute_kernel(y, y)\n",
    "    xy_kernel = compute_kernel(x, y)\n",
    "    return tf.reduce_mean(x_kernel) + tf.reduce_mean(y_kernel) - 2 * tf.reduce_mean(xy_kernel)\n",
    "\n",
    "### flatten images into vectors to be able to compute MMD\n",
    "mnist_flat=[None]*len(x_train)\n",
    "mnistm_flat=[None]*len(x_train)\n",
    "for i in range(len(x_train)):\n",
    "    mnist_flat[i]=tf.reshape(x_train[i],[-1])\n",
    "    mnistm_flat[i]=tf.reshape(x_train_m[i],[-1])\n",
    "print(np.array(mnist_flat).shape)\n",
    "print(np.array(mnistm_flat).shape)\n",
    "#print(mnistm_flat.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.017748214888393697\n"
     ]
    }
   ],
   "source": [
    "#print(nppdist2sq(np.array(mnist_flat[:800]),np.array(mnistm_flat[:800])))\n",
    "\n",
    "#print(compute_mmd(np.array(mnist_flat[:200]),np.array(mnistm_flat[:200])))\n",
    "#print(np.array(mnistm_flat[1:][::2]).shape)\n",
    "#print(np.array(mnistm_flat[::2]))\n",
    "#print(np.array(mnistm_flat[::2]))\n",
    "def mmd_linear(x,y,sigma_sqr=1.0):\n",
    "    \"\"\"\n",
    "    Here we want to compute the unbiased estimate of the MMD using the formula\n",
    "    \n",
    "    MMD^2_k(p,q)=2/n_s for i=1 to n_s/2 g_k(z_i)\n",
    "    \n",
    "    ,where z_i=(x^s_2i-1,x^s_2i,x^t_2i-1,x^t_2i) and \n",
    "    g_k(z_i)= k(x^s_2i-1,x^s_2i)+k(x^t_2i-1,x^t_2i)-k(x^s_2i-1,x^t_2i)-k(x^s_2i,x^t_2i-1)\n",
    "    \"\"\"\n",
    "    x_even=x[::2]\n",
    "    x_odd=x[1:][::2]\n",
    "    y_even=y[::2]\n",
    "    y_odd=y[1:][::2]\n",
    "    n_s=len(x)\n",
    "    ### create kernel matrix for the chosen points?\n",
    "    mmd=0\n",
    "    for i in range(round(n_s/2.0)):\n",
    "        ### sum the kernels for the chosen pairs\n",
    "        mmd+=kernel(x_even[i],x_odd[i])\n",
    "        mmd+=kernel(x_even[i],x_odd[i])\n",
    "        mmd-=kernel(x_even[i],y_odd[i])\n",
    "        mmd-=kernel(y_even[i],x_odd[i])\n",
    "    mmd=mmd*(2/n_s)\n",
    "    return mmd\n",
    "\n",
    "def kernel(x,y,sigma_sqr=1):\n",
    "    return np.exp(-np.square(np.sum(x-y))/sigma_sqr)\n",
    "#print(kernel(np.array(mnist_flat[0]),np.array(mnistm_flat[0])))\n",
    "#mmd_linear(mnist_flat,mnistm_flat)\n",
    "##### do this for a range of sigma and sum the results for an estimation of the true MMD\n",
    "sigmas = [\n",
    "      1e-6, 1e-5, \n",
    "      1e-4, 1e-3, 1e-2, 1e-1, 1, 5, 10, 15, 20, 25, 30, 35, 100,\n",
    "      1e3, 1e4, 1e5, 1e6\n",
    "  ]\n",
    "mmdlist=[]\n",
    "for sigma in sigmas:\n",
    "    mmdlist.append(mmd_linear(mnist_flat,mnistm_flat,sigma))\n",
    "print(np.sum(mmdlist))\n",
    "### 0.017748214888393697 - seems a bit small\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## implement LeNet-5 architecture\n",
    "def init_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(6,(5,5),strides=(1,1), activation='tanh',input_shape=(32,32,1))) ## 6 5x5 conv kernels\n",
    "    model.add(AveragePooling2D(pool_size=(2, 2),strides=(2, 2)))\n",
    "    model.add(Conv2D(16,(5,5),strides=(1,1), activation='tanh')) ## 16 5x5 conv kernels\n",
    "    model.add(AveragePooling2D(pool_size=(2, 2),strides=(2, 2)))\n",
    "    model.add(Conv2D(120, kernel_size=(5, 5), strides=(1, 1), activation='tanh'))\n",
    "    model.add(Flatten())\n",
    "    #model.add(Dense(120, activation='tanh'))  #equivalent to the last conv2d above?\n",
    "    model.add(Dense(84, activation='tanh'))\n",
    "    model.add(Dense(10, activation='softmax')) # output layer\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## implement LeNet-5-like architecture\n",
    "def init_SVHN_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64,(5,5),strides=(1,1), activation='relu',input_shape=(32,32,3),kernel_constraint=max_norm(4), bias_constraint=max_norm(4),kernel_regularizer=L2(0.0005), bias_regularizer=L2(0.0005))) ## 6 5x5 conv kernels\n",
    "    model.add(Dropout(0.9))\n",
    "    model.add(AveragePooling2D(pool_size=(2, 2),strides=(2, 2)))\n",
    "    model.add(Conv2D(64,(5,5),strides=(1,1), activation='relu',kernel_constraint=max_norm(4), bias_constraint=max_norm(4),kernel_regularizer=L2(0.0005), bias_regularizer=L2(0.0005))) ## 16 5x5 conv kernels\n",
    "    model.add(Dropout(0.75))\n",
    "    model.add(AveragePooling2D(pool_size=(2, 2),strides=(2, 2)))\n",
    "    model.add(Conv2D(128,(5,5),strides=(1,1), activation='relu',kernel_constraint=max_norm(4), bias_constraint=max_norm(4),kernel_regularizer=L2(0.0005), bias_regularizer=L2(0.0005)))\n",
    "    model.add(Dropout(0.75))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(3072, activation='relu',kernel_constraint=max_norm(4), bias_constraint=max_norm(4),kernel_regularizer=L2(0.0005), bias_regularizer=L2(0.0005)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2048, activation='relu',kernel_constraint=max_norm(4), bias_constraint=max_norm(4),kernel_regularizer=L2(0.0005), bias_regularizer=L2(0.0005)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='softmax',kernel_constraint=max_norm(4), bias_constraint=max_norm(4),kernel_regularizer=L2(0.0005), bias_regularizer=L2(0.0005))) # output layer\n",
    "    \n",
    "    return model\n",
    "\n",
    "'''\n",
    "We use stochastic gradient descent with\n",
    "momentum: 0.9\n",
    "weight_decay: 0.0005\n",
    "------ this was not used on mnist<->svhn tests\n",
    "and the learning rate annealing described by the following formula:\n",
    "µp =µ0/(1 + α · p)^β,\n",
    "where p is the training progress linearly changing from 0\n",
    "to 1, µ0 = 0.01, α = 10 and β = 0.75 (the schedule\n",
    "was optimized to promote convergence and low error on\n",
    "the source domain).\n",
    "------\n",
    "\n",
    "Following (Srivastava et al., 2014) we also use dropout and\n",
    "l_2-norm restriction when we train the SVHN architecture.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## implement LeNet-5-like architecture\n",
    "def init_MNIST_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32,(5,5),strides=(1,1), activation='relu',input_shape=(32,32,3))) ## 6 5x5 conv kernels\n",
    "    model.add(AveragePooling2D(pool_size=(3, 3),strides=(2, 2)))\n",
    "    model.add(Conv2D(48,(5,5),strides=(1,1), activation='relu')) ## 16 5x5 conv kernels\n",
    "    model.add(AveragePooling2D(pool_size=(3, 3),strides=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax')) # output layer\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## shamelessly taken from : https://stackoverflow.com/questions/47731935/using-multiple-validation-sets-with-keras\n",
    "\n",
    "## Custom callback to be able to evaluate and save the results from several validation sets during training\n",
    "class AdditionalValidationSets(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, validation_sets, verbose=0, batch_size=None):\n",
    "        \"\"\"\n",
    "        :param validation_sets:\n",
    "        a list of 3-tuples (validation_data, validation_targets, validation_set_name)\n",
    "        or 4-tuples (validation_data, validation_targets, sample_weights, validation_set_name)\n",
    "        :param verbose:\n",
    "        verbosity mode, 1 or 0\n",
    "        :param batch_size:\n",
    "        batch size to be used when evaluating on the additional datasets\n",
    "        \"\"\"\n",
    "        super(AdditionalValidationSets, self).__init__()\n",
    "        self.validation_sets = validation_sets\n",
    "        for validation_set in self.validation_sets:\n",
    "            if len(validation_set) not in [2, 3]:\n",
    "                raise ValueError()\n",
    "        self.epoch = []\n",
    "        self.history = {}\n",
    "        self.verbose = verbose\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.epoch = []\n",
    "        self.history = {}\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        self.epoch.append(epoch)\n",
    "\n",
    "        # record the same values as History() as well\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "\n",
    "        # evaluate on the additional validation sets\n",
    "        for validation_set in self.validation_sets:\n",
    "            if len(validation_set) == 3:\n",
    "                validation_data, validation_targets, validation_set_name = validation_set\n",
    "                sample_weights = None\n",
    "            elif len(validation_set) == 4:\n",
    "                validation_data, validation_targets, sample_weights, validation_set_name = validation_set\n",
    "            else:\n",
    "                raise ValueError()\n",
    "\n",
    "            results = self.model.evaluate(x=validation_data,\n",
    "                                          y=validation_targets,\n",
    "                                          verbose=self.verbose,\n",
    "                                          sample_weight=sample_weights,\n",
    "                                          batch_size=self.batch_size)\n",
    "\n",
    "            for i, result in enumerate(results):\n",
    "                \n",
    "                if i == 0:\n",
    "                    valuename = validation_set_name + '_loss'\n",
    "                else:\n",
    "                    valuename = validation_set_name + '_' + self.model.metrics[i].name\n",
    "                self.history.setdefault(valuename, []).append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model=\"SVHN\" ,batch_size=128 ,total_epochs=25 ,iterations=1 ,x_train=[] ,y_train=[] ,x_test=[] ,y_test=[] ,x_target=[] ,y_target=[]):\n",
    "    history = AdditionalValidationSets([(x_target, y_target, 'target_val')])\n",
    "    \n",
    "    # Include the epoch in the file name (uses `str.format`)\n",
    "    checkpoint_path = \"checkpoints/\"+model+\"-cp-{epoch:04d}.ckpt\"\n",
    "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "    # Create a callback that saves the model's weights every 5 epochs\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path, \n",
    "    verbose=1,\n",
    "    save_best_only=False,\n",
    "    save_weights_only=True,\n",
    "        ## tune when to save as needed for plots\n",
    "    save_freq=50*469 ### 469 = ceiling(60000/128) i.e training set for MNIST/MNIST-M\n",
    "    )\n",
    "    \n",
    "    histories=[]\n",
    "    M=init_SVHN_model()\n",
    "    for i in range(iterations):\n",
    "        if model==\"SVHN\":\n",
    "            M=init_SVHN_model()\n",
    "        elif model==\"MNIST\":\n",
    "            M=init_MNIST_model()\n",
    "        elif model==\"MNIST-M\":\n",
    "            M=init_MNIST_model()\n",
    "        elif model==\"2MNIST-M\":\n",
    "            M=init_MNIST_model()\n",
    "        # Save the weights using the `checkpoint_path` format\n",
    "        M.save_weights(checkpoint_path.format(epoch=0))\n",
    "        ## choose loss function, optimiser etc. and train\n",
    "        M.compile(loss=keras.losses.categorical_crossentropy,\n",
    "               optimizer=keras.optimizers.SGD(learning_rate=0.01, momentum=0.9),\n",
    "                      metrics=['accuracy'],)\n",
    "\n",
    "        fit_info = M.fit(x_train, y_train,\n",
    "           batch_size=batch_size,\n",
    "           epochs=total_epochs,\n",
    "           verbose=1,\n",
    "           validation_data=(x_test, y_test),\n",
    "           callbacks=[history,cp_callback])\n",
    "        histories.append(history.history)\n",
    "    return histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(total_epochs,model=\"SVHN\",result=[], xlabel=\"Epoch\",save=True):#,ylabel=\"\", title=\"\"): \n",
    "    ### TODO: do one for each type of plot?\n",
    "    \n",
    "    from datetime import datetime\n",
    "\n",
    "    # datetime object containing current date and time\n",
    "    now = datetime.now()\n",
    "\n",
    "    # dd/mm/YY_H:M\n",
    "    dt_string = now.strftime(\"%d%m%Y_%H:%M\")\n",
    "    #print(\"date and time =\", dt_string)\n",
    "    \n",
    "    ## plotting and saving to disk\n",
    "    x=[i+1 for i in range(total_epochs)]\n",
    "\n",
    "    if model==\"SVHN\":\n",
    "        prefix=[\"S2MTGTACC\",\"S2MVAL\",\"S2MLOSS\",\"S2MVALLOSS\",\"S2MTGTLOSS\"]\n",
    "    elif model==\"MNIST\":\n",
    "        prefix=[\"M2STGTACC\",\"M2SVAL\",\"M2SLOSS\",\"M2SVALLOSS\",\"M2STGTLOSS\"]\n",
    "    elif model==\"MNIST-M\":\n",
    "         prefix=[\"MM2MTGTACC\",\"MM2MVAL\",\"MM2MLOSS\",\"MM2MVALLOSS\",\"MM2MTGTLOSS\"]\n",
    "    elif model==\"2MNIST-M\":\n",
    "        prefix=[\"M2MMTGTACC\",\"M2MMVAL\",\"M2MMLOSS\",\"M2MMVALLOSS\",\"M2MMTGTLOSS\"]\n",
    "\n",
    "    f, ax = plt.subplots()\n",
    "    ax.plot(x,result['target_val_accuracy'], '*-')\n",
    "    # Plot legend and use the best location automatically: loc = 0.\n",
    "    ax.legend(['Target acc'], loc = 0)\n",
    "    ax.set_title('Target acc per epoch')\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    if(save):\n",
    "        f.savefig(\"./images/\"+prefix[0]+dt_string+\".pdf\")\n",
    "   \n",
    "\n",
    "    f, ax = plt.subplots()\n",
    "    ax.plot(x,result['val_accuracy'], 'x-')\n",
    "    # Plot legend and use the best location automatically: loc = 0.\n",
    "    ax.legend(['Validation acc'], loc = 0)\n",
    "    ax.set_title('Validation acc per epoch')\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    if(save):\n",
    "        f.savefig(\"./images/\"+prefix[1]+dt_string+\".pdf\")\n",
    "\n",
    "    f, ax = plt.subplots()\n",
    "    \n",
    "    ax.plot(x,result['loss'], 'x-')\n",
    "    # Plot legend and use the best location automatically: loc = 0.\n",
    "    ax.legend(['Training loss'], loc = 0)\n",
    "    ax.set_title('Training loss per epoch')\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel('Loss')\n",
    "    if(save):\n",
    "        f.savefig(\"./images/\"+prefix[2]+dt_string+\".pdf\")\n",
    "\n",
    "    f, ax = plt.subplots()\n",
    "    ax.plot(x,result['val_loss'], 'x-')\n",
    "    # Plot legend and use the best location automatically: loc = 0.\n",
    "    ax.legend(['Validation loss'], loc = 0)\n",
    "    ax.set_title('Validation loss per epoch')\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel('Loss')\n",
    "    if(save):\n",
    "        f.savefig(\"./images/\"+prefix[3]+dt_string+\".pdf\")\n",
    "    \n",
    "    f, ax = plt.subplots()\n",
    "    ax.plot(x,result['target_val_loss'], 'x-')\n",
    "    # Plot legend and use the best location automatically: loc = 0.\n",
    "    ax.legend(['Target acc'], loc = 0)\n",
    "    ax.set_title('Target loss per epoch')\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel('Loss')\n",
    "    if(save):\n",
    "        f.savefig(\"./images/\"+prefix[4]+dt_string+\".pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results_data(sizes=[],model=\"SVHN\",result=[],save=True):# plot for the increasing amount of data\n",
    "    ### TODO: do one for each type of plot?\n",
    "    \n",
    "    from datetime import datetime\n",
    "\n",
    "    # datetime object containing current date and time\n",
    "    now = datetime.now()\n",
    "\n",
    "    # dd/mm/YY_H:M\n",
    "    dt_string = now.strftime(\"%d%m%Y_%H:%M\")\n",
    "    #print(\"date and time =\", dt_string)\n",
    "    \n",
    "    ## plotting and saving to disk\n",
    "    x=[i+1 for i in range(total_epochs)]\n",
    "\n",
    "    if model==\"SVHN\":\n",
    "        prefix=[\"S2MTGTACC\",\"S2MVAL\",\"S2MVALLOSS\",\"S2MTGTLOSS\"]\n",
    "        target=\"MNIST\"\n",
    "    elif model==\"MNIST\":\n",
    "        prefix=[\"M2STGTACC\",\"M2SVAL\",\"M2SVALLOSS\",\"M2STGTLOSS\"]\n",
    "        target=\"SVHN\"\n",
    "    elif model==\"MNIST-M\":\n",
    "         prefix=[\"MM2MTGTACC\",\"MM2MSRCACC\",\"MM2MSRCLOSS\",\"MM2MTGTLOSS\"]\n",
    "    elif model==\"2MNIST-M\":\n",
    "        prefix=[\"M2MMTGTACC\",\"M2MMVAL\",\"M2MMLOSS\",\"M2MMVALLOSS\",\"M2MMTGTLOSS\"]\n",
    "    \n",
    "    f, ax = plt.subplots()\n",
    "    ax.plot(sizes,aggregate_acc_t, '*-')\n",
    "    # Plot legend and use the best location automatically: loc = 0.\n",
    "    #ax.legend(['Train acc', 'Validation acc','Target acc'], loc = 0)\n",
    "    ax.legend(['Target acc'], loc = 0)\n",
    "    #ax.legend(['Validation acc'], loc = 0)\n",
    "    ax.set_title('Target acc per amount of training data ('+model+'->'+target+')')\n",
    "    #ax.set_title('Validation acc per epoch')\n",
    "    ax.set_xlabel('Data amount')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    if(save):\n",
    "        f.savefig(\"./images/data/\"+prefix[0]+dt_string+\".pdf\")\n",
    "\n",
    "    f, ax = plt.subplots()\n",
    "    #ax.plot(x,result['accuracy'], 'o-')\n",
    "    #ax.plot(x,result['val_accuracy'], 'x-')\n",
    "    ax.plot(sizes,aggregate_acc_s, '*-')\n",
    "    # Plot legend and use the best location automatically: loc = 0.\n",
    "    #ax.legend(['Train acc', 'Validation acc','Target acc'], loc = 0)\n",
    "    ax.legend(['Source acc'], loc = 0)\n",
    "    #ax.legend(['Validation acc'], loc = 0)\n",
    "    ax.set_title('Source acc per amount of training data ('+model+'->'+target+')')\n",
    "    #ax.set_title('Validation acc per epoch')\n",
    "    ax.set_xlabel('Data amount')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    if(save):\n",
    "        f.savefig(\"./images/data/\"+prefix[1]+dt_string+\".pdf\")\n",
    "\n",
    "    f, ax = plt.subplots()\n",
    "    #ax.plot(x,result['accuracy'], 'o-')\n",
    "    #ax.plot(x,result['val_accuracy'], 'x-')\n",
    "    ax.plot(sizes,aggregate_loss_s, '*-')\n",
    "    # Plot legend and use the best location automatically: loc = 0.\n",
    "    #ax.legend(['Train acc', 'Validation acc','Target acc'], loc = 0)\n",
    "    ax.legend(['Source loss'], loc = 0)\n",
    "    #ax.legend(['Validation acc'], loc = 0)\n",
    "    ax.set_title('Source loss per amount of training data ('+model+'->'+target+')')\n",
    "    #ax.set_title('Validation acc per epoch')\n",
    "    ax.set_xlabel('Data amount')\n",
    "    ax.set_ylabel('Loss')\n",
    "    if(save):\n",
    "        f.savefig(\"./images/data/\"+prefix[2]+dt_string+\".pdf\")\n",
    "    \n",
    "    f, ax = plt.subplots()\n",
    "    #ax.plot(x,result['accuracy'], 'o-')\n",
    "    #ax.plot(x,result['val_accuracy'], 'x-')\n",
    "    ax.plot(sizes,aggregate_loss_t, '*-')\n",
    "    # Plot legend and use the best location automatically: loc = 0.\n",
    "    #ax.legend(['Train acc', 'Validation acc','Target acc'], loc = 0)\n",
    "    ax.legend(['Target loss'], loc = 0)\n",
    "    #ax.legend(['Validation acc'], loc = 0)\n",
    "    ax.set_title('Target loss per amount of training data ('+model+'->'+target+')')\n",
    "    #ax.set_title('Validation acc per epoch')\n",
    "    ax.set_xlabel('Data amount')\n",
    "    ax.set_ylabel('Loss')\n",
    "    if(save):\n",
    "        f.savefig(\"./images/data/\"+prefix[3]+dt_string+\".pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''### test for SVHN-> MNIST, \n",
    "total_epochs=220 #\n",
    "\n",
    "\n",
    "iterations=10\n",
    "histories=[]\n",
    "\n",
    "#histories=train_model(model=\"SVHN\",total_epochs=total_epochs,iterations=iterations,x_train=X_train,y_train=Y_train,x_test=X_test,y_test=Y_test,x_target=x_test,y_target=y_test)\n",
    "    \n",
    "histories=train_model(total_epochs=total_epochs,x_train=X_train,y_train=Y_train,x_test=X_test,y_test=Y_test,x_target=x_test,y_target=y_test)\n",
    "   \n",
    "                      \n",
    "##### Aggregating over all the training runs                    \n",
    "K=histories[0].keys()\n",
    "                \n",
    "result={}\n",
    "for key in K:\n",
    "    tmp=[]\n",
    "    for epoch in range(len(histories)):\n",
    "        tmp.append(histories[epoch][key])\n",
    "    result[key]=tmp\n",
    "\n",
    "for key in K:\n",
    "    result[key]=np.mean(result[key],axis=0)\n",
    "\n",
    "    \n",
    "## plotting and saving to disk\n",
    "plot_results(model=\"SVHN\",result=result,xlabel=\"Epoch\",total_epochs=total_epochs)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''### test for MNIST->SVHN\n",
    "total_epochs=2000 #\n",
    "\n",
    "\n",
    "iterations=10\n",
    "histories=[]\n",
    "\n",
    "\n",
    "    \n",
    "histories=train_model(model=\"MNIST\",total_epochs=total_epochs,x_train=x_train,y_train=y_train,x_test=x_test,y_test=y_test,x_target=X_test,y_target=Y_test)\n",
    "   \n",
    "                      \n",
    "##### Aggregating over all the training runs                    \n",
    "K=histories[0].keys()\n",
    "                \n",
    "result={}\n",
    "for key in K:\n",
    "    tmp=[]\n",
    "    for epoch in range(len(histories)):\n",
    "        tmp.append(histories[epoch][key])\n",
    "    result[key]=tmp\n",
    "\n",
    "for key in K:\n",
    "    result[key]=np.mean(result[key],axis=0)\n",
    "\n",
    "    \n",
    "## plotting and saving to disk\n",
    "plot_results(model=\"MNIST\",result=result,xlabel=\"Epoch\",total_epochs=total_epochs)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''### test for MNIST-M-> MNIST\n",
    "total_epochs=2000 #\n",
    "\n",
    "\n",
    "iterations=10\n",
    "histories=[]\n",
    "\n",
    "\n",
    "    \n",
    "histories=train_model(model=\"MNIST-M\",total_epochs=total_epochs,x_train=x_train_m,y_train=y_train_m,x_test=x_test_m,y_test=y_test_m,x_target=x_test,y_target=y_test)\n",
    "   \n",
    "                      \n",
    "##### Aggregating over all the training runs                    \n",
    "K=histories[0].keys()\n",
    "                \n",
    "result={}\n",
    "for key in K:\n",
    "    tmp=[]\n",
    "    for epoch in range(len(histories)):\n",
    "        tmp.append(histories[epoch][key])\n",
    "    result[key]=tmp\n",
    "\n",
    "for key in K:\n",
    "    result[key]=np.mean(result[key],axis=0)\n",
    "\n",
    "    \n",
    "## plotting and saving to disk\n",
    "plot_results(model=\"MNIST-M\",result=result,xlabel=\"Epoch\",total_epochs=total_epochs)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test for MNIST-> MNIST-M\n",
    "total_epochs=200 #\n",
    "\n",
    "\n",
    "iterations=1\n",
    "histories=[]\n",
    "\n",
    "\n",
    "\n",
    "histories=train_model(model=\"2MNIST-M\",total_epochs=total_epochs,x_train=x_train,y_train=y_train,x_test=x_test,y_test=y_test,x_target=x_test_m,y_target=y_test_m)\n",
    "\n",
    "\n",
    "                      \n",
    "##### Aggregating over all the training runs                    \n",
    "K=histories[0].keys()\n",
    "                \n",
    "result={}\n",
    "for key in K:\n",
    "    tmp=[]\n",
    "    for epoch in range(len(histories)):\n",
    "        tmp.append(histories[epoch][key])\n",
    "    result[key]=tmp\n",
    "\n",
    "for key in K:\n",
    "    result[key]=np.mean(result[key],axis=0)\n",
    "\n",
    "    \n",
    "## plotting and saving to disk\n",
    "plot_results(model=\"2MNIST-M\",result=result,xlabel=\"Epoch\",total_epochs=total_epochs,save=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "## taken from springerNLP github\n",
    "# https://github.com/SpringerNLP/Chapter11/blob/master/maximum_mean_discrepancy.py\n",
    "\n",
    "def expand_multiple_dims(x, axes, name=\"expand_multiple_dims\"):\n",
    "  \"\"\"\n",
    "  :param tf.Tensor x:\n",
    "  :param list[int]|tuple[int] axes: after completion, tf.shape(y)[axis] == 1 for axis in axes\n",
    "  :param str name: scope name\n",
    "  :return: y where we have a new broadcast axis for each axis in axes\n",
    "  :rtype: tf.Tensor\n",
    "  \"\"\"\n",
    "  with tf.name_scope(name):\n",
    "    for i in sorted(axes):\n",
    "      x = tf.expand_dims(x, axis=i, name=\"expand_axis_%i\" % i)\n",
    "    return x\n",
    "\n",
    "\n",
    "def dimshuffle(x, axes, name=\"dimshuffle\"):\n",
    "  \"\"\"\n",
    "  Like Theanos dimshuffle.\n",
    "  Combines tf.transpose, tf.expand_dims and tf.squeeze.\n",
    "\n",
    "  :param tf.Tensor x:\n",
    "  :param list[int|str]|tuple[int|str] axes:\n",
    "  :param str name: scope name\n",
    "  :rtype: tf.Tensor\n",
    "  \"\"\"\n",
    "  with tf.name_scope(name):\n",
    "    assert all([i == \"x\" or isinstance(i, int) for i in axes])\n",
    "    real_axes = [i for i in axes if isinstance(i, int)]\n",
    "    bc_axes = [i for (i, j) in enumerate(axes) if j == \"x\"]\n",
    "    if x.get_shape().ndims is None:\n",
    "      x_shape = tf.shape(x)\n",
    "      x = tf.reshape(x, [x_shape[i] for i in range(max(real_axes) + 1)])  # will have static ndims\n",
    "    assert x.get_shape().ndims is not None\n",
    "\n",
    "    # First squeeze missing axes.\n",
    "    i = 0\n",
    "    while i < x.get_shape().ndims:\n",
    "      if i not in real_axes:\n",
    "        x = tf.squeeze(x, axis=i)\n",
    "        real_axes = [(j if (j < i) else (j - 1)) for j in real_axes]\n",
    "      else:\n",
    "        i += 1\n",
    "\n",
    "    # Now permute.\n",
    "    assert list(sorted(real_axes)) == list(range(x.get_shape().ndims))\n",
    "    if real_axes != list(range(x.get_shape().ndims)):\n",
    "      x = tf.transpose(x, real_axes)\n",
    "\n",
    "    # Now add broadcast dimensions.\n",
    "    if bc_axes:\n",
    "      x = expand_multiple_dims(x, bc_axes)\n",
    "    assert len(axes) == x.get_shape().ndims\n",
    "    return x\n",
    "\n",
    "def mmd(x1, x2, beta):\n",
    "    \"\"\"\n",
    "    maximum mean discrepancy (MMD) based on Gaussian kernel\n",
    "    function for keras models (theano or tensorflow backend)\n",
    "    \n",
    "    - Gretton, Arthur, et al. \"A kernel method for the two-sample-problem.\"\n",
    "    Advances in neural information processing systems. 2007.\n",
    "    \"\"\"\n",
    "    x1x1 = gaussian_kernel(x1, x1, beta)\n",
    "    x1x2 = gaussian_kernel(x1, x2, beta)\n",
    "    x2x2 = gaussian_kernel(x2, x2, beta)\n",
    "    diff = x1x1.mean() - 2 * x1x2.mean() + x2x2.mean()\n",
    "    return diff\n",
    "\n",
    "def gaussian_kernel(x1, x2, beta = 1.0):\n",
    "    #r = tf.transpose(x1)\n",
    "    #r = x.dimshuffle(0,'x',1)\n",
    "    return K.exp( -beta * K.square(x1 - x2).sum(axis=-1))\n",
    "\n",
    "print(mmd(x_train,x_test,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "### test for SVHN-> MNIST,  USING only a part of the training set\n",
    "total_epochs=200 #\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "num_runs=1\n",
    "iterations=1\n",
    "histories=[]\n",
    "prev_loss_t=[]\n",
    "prev_loss_s=[]\n",
    "prev_acc_t=[]\n",
    "prev_acc_s=[]\n",
    "N=len(X_train)\n",
    "\n",
    "for runs in range(num_runs):\n",
    "    target_acc=[]\n",
    "    target_loss=[]\n",
    "    source_acc=[]\n",
    "    source_loss=[]\n",
    "    sizes=[]\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        \n",
    "        size=round(0.001*N)*(10*i+1)\n",
    "        \n",
    "        sizes.append(size)\n",
    "         ## take out 1*i percent of the data and train\n",
    "        X_train2, X_test2, Y_train2, Y_test2 = train_test_split(X_train,Y_train,test_size=size)\n",
    "        histories=train_model(total_epochs=total_epochs,x_train=X_test2,y_train=Y_test2,x_test=X_test,y_test=Y_test,x_target=x_test,y_target=y_test)\n",
    "        \n",
    "        # get last epochs accuracy for target\n",
    "        target_acc.append(histories[0]['target_val_accuracy'][-1])\n",
    "        target_loss.append(histories[0]['target_val_loss'][-1])\n",
    "        source_acc.append(histories[0]['val_accuracy'][-1])\n",
    "        source_loss.append(histories[0]['val_loss'][-1])\n",
    "    \n",
    "    ##### append the list of last epoch accuracies for each amount of data                    \n",
    "    ##### to the prev_runs list\n",
    "    print(target_acc)\n",
    "    prev_loss_t.append(target_loss)\n",
    "    prev_loss_s.append(source_loss)\n",
    "    prev_acc_t.append(target_acc)\n",
    "    prev_acc_s.append(source_acc)\n",
    "    \n",
    "\n",
    "### take mean over all runs\n",
    "\n",
    "aggregate_loss_t=np.mean(prev_loss_t,axis=0)\n",
    "aggregate_loss_s=np.mean(prev_loss_s,axis=0)\n",
    "aggregate_acc_t=np.mean(prev_acc_t,axis=0)\n",
    "aggregate_acc_s=np.mean(prev_acc_s,axis=0)\n",
    "\n",
    "result=[]\n",
    "result.append(aggregate_acc_t)\n",
    "result.append(aggregate_acc_s)\n",
    "result.append(aggregate_loss_s)\n",
    "result.append(aggregate_loss_t)\n",
    "\n",
    "    \n",
    "## plotting and saving to disk\n",
    "plot_results_data(sizes=sizes,result=result,save=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " '''\n",
    "    ### test for MNIST->SVHN,  USING only a part of the training set\n",
    "total_epochs=200 #\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "num_runs=5\n",
    "iterations=15\n",
    "histories=[]\n",
    "prev_loss_t=[]\n",
    "prev_loss_s=[]\n",
    "prev_acc_t=[]\n",
    "prev_acc_s=[]\n",
    "N=len(X_train)\n",
    "\n",
    "for runs in range(num_runs):\n",
    "    target_acc=[]\n",
    "    target_loss=[]\n",
    "    source_acc=[]\n",
    "    source_loss=[]\n",
    "    sizes=[]\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        \n",
    "        size=round(0.001*N)*(10*i+1)\n",
    "        \n",
    "        sizes.append(size)\n",
    "         ## take out 1*i percent of the data and train\n",
    "        x_train2, x_test2, y_train2, y_test2 = train_test_split(x_train,y_train,test_size=size)\n",
    "        histories=train_model(model='MNIST',total_epochs=total_epochs,x_train=x_test2,y_train=y_test2,x_test=x_test,y_test=y_test,x_target=X_test,y_target=Y_test)\n",
    "        \n",
    "        # get last epochs accuracy for target\n",
    "        target_acc.append(histories[0]['target_val_accuracy'][-1])\n",
    "        target_loss.append(histories[0]['target_val_loss'][-1])\n",
    "        source_acc.append(histories[0]['val_accuracy'][-1])\n",
    "        source_loss.append(histories[0]['val_loss'][-1])\n",
    "    \n",
    "    ##### append the list of last epoch accuracies for each amount of data                    \n",
    "    ##### to the prev_runs list\n",
    "    print(target_acc)\n",
    "    prev_loss_t.append(target_loss)\n",
    "    prev_loss_s.append(source_loss)\n",
    "    prev_acc_t.append(target_acc)\n",
    "    prev_acc_s.append(source_acc)\n",
    "    \n",
    "\n",
    "### take mean over all runs\n",
    "\n",
    "aggregate_loss_t=np.mean(prev_loss_t,axis=0)\n",
    "aggregate_loss_s=np.mean(prev_loss_s,axis=0)\n",
    "aggregate_acc_t=np.mean(prev_acc_t,axis=0)\n",
    "aggregate_acc_s=np.mean(prev_acc_s,axis=0)\n",
    "\n",
    "result=[]\n",
    "result.append(aggregate_acc_t)\n",
    "result.append(aggregate_acc_s)\n",
    "result.append(aggregate_loss_s)\n",
    "result.append(aggregate_loss_t)\n",
    "\n",
    "    \n",
    "## plotting and saving to disk\n",
    "plot_results_data(model='MNIST',sizes=sizes,result=result)\n",
    "    \n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    '''\n",
    "    ######## PLOTS for MNIST ############\n",
    "f, ax = plt.subplots()\n",
    "#ax.plot(x,result['accuracy'], 'o-')\n",
    "#ax.plot(x,result['val_accuracy'], 'x-')\n",
    "ax.plot(x,result['target_val_accuracy'], '*-')\n",
    "# Plot legend and use the best location automatically: loc = 0.\n",
    "#ax.legend(['Train acc', 'Validation acc','Target acc'], loc = 0)\n",
    "ax.legend(['Target acc'], loc = 0)\n",
    "#ax.legend(['Validation acc'], loc = 0)\n",
    "ax.set_title('Target acc per Epoch (MNIST->SVHN)')\n",
    "#ax.set_title('Validation acc per epoch')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Accuracy')\n",
    "#f.savefig(\"MNIST2SVHN.pdf\")\n",
    "#f.savefig(\"MNISTval.pdf\")\n",
    "\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "#ax.plot(x,result['accuracy'], 'o-')\n",
    "ax.plot(x,result['val_accuracy'], 'x-')\n",
    "#ax.plot(x,result['target_val_accuracy'], '*-')\n",
    "# Plot legend and use the best location automatically: loc = 0.\n",
    "#ax.legend(['Train acc', 'Validation acc','Target acc'], loc = 0)\n",
    "#ax.legend(['Target acc'], loc = 0)\n",
    "ax.legend(['Validation acc'], loc = 0)\n",
    "#ax.set_title('Target acc per Epoch (MNIST->SVHN)')\n",
    "ax.set_title('Validation acc per epoch')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Accuracy')\n",
    "#f.savefig(\"MNIST2SVHN.pdf\")\n",
    "#f.savefig(\"MNIST2SVHNval.pdf\")\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "#ax.plot(x,result2['accuracy'], 'o-')\n",
    "ax.plot(x,result2['val_loss'], 'x-')\n",
    "#ax.plot(x,result2['target_val_accuracy'], '*-')\n",
    "# Plot legend and use the best location automatically: loc = 0.\n",
    "#ax.legend(['Train acc', 'Validation acc','Target acc'], loc = 0)\n",
    "#ax.legend(['Target acc'], loc = 0)\n",
    "ax.legend(['Validation acc'], loc = 0)\n",
    "#ax.set_title('Target acc per Epoch (SVHN->MNIST)')\n",
    "ax.set_title('Training loss per epoch')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "#f.savefig(\"SVHN2MNIST.pdf\")\n",
    "#f.savefig(\"MNIST2SVHNloss.pdf\")\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "#ax.plot(x,result2['accuracy'], 'o-')\n",
    "ax.plot(x,result2['val_loss'], 'x-')\n",
    "#ax.plot(x,result2['target_val_accuracy'], '*-')\n",
    "# Plot legend and use the best location automatically: loc = 0.\n",
    "#ax.legend(['Train acc', 'Validation acc','Target acc'], loc = 0)\n",
    "#ax.legend(['Target acc'], loc = 0)\n",
    "ax.legend(['Validation acc'], loc = 0)\n",
    "#ax.set_title('Target acc per Epoch (SVHN->MNIST)')\n",
    "ax.set_title('Validation loss per epoch')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "#f.savefig(\"SVHN2MNIST.pdf\")\n",
    "#f.savefig(\"MNIST2SVHNvalloss.pdf\")\n",
    "\n",
    "\n",
    "############SVHN PLOTS ############ \n",
    "\n",
    "f, ax = plt.subplots()\n",
    "#ax.plot(x,result2['accuracy'], 'o-')\n",
    "#ax.plot(x,result2['val_accuracy'], 'x-')\n",
    "ax.plot(x,result2['target_val_accuracy'], '*-')\n",
    "# Plot legend and use the best location automatically: loc = 0.\n",
    "#ax.legend(['Train acc', 'Validation acc','Target acc'], loc = 0)\n",
    "ax.legend(['Target acc'], loc = 0)\n",
    "#ax.legend(['Validation acc'], loc = 0)\n",
    "ax.set_title('Target acc per Epoch (SVHN->MNIST)')\n",
    "#ax.set_title('Validation acc per epoch')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Accuracy')\n",
    "#f.savefig(\"SVHN2MNIST.pdf\")\n",
    "#f.savefig(\"SVHN2MNISTval.pdf\")\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "#ax.plot(x,result2['accuracy'], 'o-')\n",
    "ax.plot(x,result2['val_accuracy'], 'x-')\n",
    "#ax.plot(x,result2['target_val_accuracy'], '*-')\n",
    "# Plot legend and use the best location automatically: loc = 0.\n",
    "#ax.legend(['Train acc', 'Validation acc','Target acc'], loc = 0)\n",
    "#ax.legend(['Target acc'], loc = 0)\n",
    "ax.legend(['Validation acc'], loc = 0)\n",
    "#ax.set_title('Target acc per Epoch (SVHN->MNIST)')\n",
    "ax.set_title('Validation acc per epoch')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Accuracy')\n",
    "#f.savefig(\"SVHN2MNIST.pdf\")\n",
    "#f.savefig(\"SVHN2MNISTval.pdf\")\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "#ax.plot(x,result2['accuracy'], 'o-')\n",
    "ax.plot(x,result2['val_loss'], 'x-')\n",
    "#ax.plot(x,result2['target_val_accuracy'], '*-')\n",
    "# Plot legend and use the best location automatically: loc = 0.\n",
    "#ax.legend(['Train acc', 'Validation acc','Target acc'], loc = 0)\n",
    "#ax.legend(['Target acc'], loc = 0)\n",
    "ax.legend(['Validation acc'], loc = 0)\n",
    "#ax.set_title('Target acc per Epoch (SVHN->MNIST)')\n",
    "ax.set_title('Training loss per epoch')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "#f.savefig(\"SVHN2MNIST.pdf\")\n",
    "#f.savefig(\"SVHN2MNISTloss.pdf\")\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "#ax.plot(x,result2['accuracy'], 'o-')\n",
    "ax.plot(x,result2['val_loss'], 'x-')\n",
    "#ax.plot(x,result2['target_val_accuracy'], '*-')\n",
    "# Plot legend and use the best location automatically: loc = 0.\n",
    "#ax.legend(['Train acc', 'Validation acc','Target acc'], loc = 0)\n",
    "#ax.legend(['Target acc'], loc = 0)\n",
    "ax.legend(['Validation acc'], loc = 0)\n",
    "#ax.set_title('Target acc per Epoch (SVHN->MNIST)')\n",
    "ax.set_title('Validation loss per epoch')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "#f.savefig(\"SVHN2MNIST.pdf\")\n",
    "#f.savefig(\"SVHN2MNISTvalloss.pdf\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
